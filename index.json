[{"categories":null,"contents":" For SaTML 2025, 53 papers were accepted out of 180 submissions, resulting in an acceptance rate of 29.4%. Below is the list of the accepted papers, organized by category. To learn more about the three categories of papers, please visit the CFP.\nResearch Papers Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual Metrics Francesco\u0026nbsp;Croce (EPFL), Christian\u0026nbsp;Schlarmann, Naman Deep\u0026nbsp;Singh, Matthias\u0026nbsp;Hein (University\u0026nbsp;of\u0026nbsp;Tuebingen) 📃 Abstract 📚 Arxiv Measuring perceptual similarity is a key tool in computer vision. In recent years perceptual metrics based on features extracted from neural networks with large and diverse training sets, e.g. CLIP, have become popular. At the same time, the metrics extracted from features of neural networks are not adversarially robust. In this paper we show that adversarially robust CLIP models, called \\rclipf, obtained by unsupervised adversarial finetuning induce a better and adversarially robust perceptual metric that outperforms existing metrics in a zero-shot setting, and further matches the performance of state-of-the-art metrics while being robust after fine-tuning. Moreover, our perceptual metric achieves strong performance on related task such as robust image-to-image retrieval, which becomes especially relevant when applied to ``Not Safe for Work'' (NSFW) content detection and dataset filtering. While standard perceptual metrics can be easily attacked by a small perturbation completely degrading NSFW detection, our robust perceptual metric maintains high accuracy under an attack while having similar performance for unperturbed images. Finally, perceptual metrics induced by robust CLIP models have higher interpretability: feature inversion can show which images are considered similar, while text inversion can find what images are associated to a given prompt. This also allows us to visualize the very rich visual concepts learned by a CLIP model, including memorized persons, paintings and complex queries. Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks Daryna\u0026nbsp;Oliynyk, Rudolf\u0026nbsp;Mayer (SBA\u0026nbsp;Research), Andreas\u0026nbsp;Rauber (TU\u0026nbsp;Wien) 📃 Abstract 📚 Arxiv Machine learning (ML) models were shown to be vulnerable to different security attacks \u0026ndash; including model stealing attacks, which lead to intellectual property infringement. Among other attack types, substitute model training is an all-encompassing attack applicable to any machine learning model whose behaviour can be approximated from input-output queries. Whereas previous works mainly focused on improving the performance of substitute models by, e.g. developing a new substitute training method, there have been only limited comprehensive ablation studies that try to understand the impact the strength of an attacker has on the substitute model's performance. As a result, different authors came to diverse, sometimes contradicting conclusions. In this work, we therefore exhaustively examine the influence of different factors, primarily forming the attacker's capabilities and knowledge, on a substitute training attack. We investigate how the quality of the substitute training data, the training strategy, and discrepancies between the characteristics of the target and substitute models impact the performance of the attack. Our findings suggest that some of the factors that have been considered important in the past are, in fact, not that influential; instead, we discover new correlations between the attack conditions and success rate. Moreover, our results often exceed or match the performance of attacks that assume a stronger attacker, suggesting that these stronger attacks are likely endangering a model owner's intellectual property to a significantly higher degree than shown until now. Auditing Differential Privacy Guarantees Using Density Estimation Antti\u0026nbsp;Koskela, Jafar\u0026nbsp;Mohammadi (Nokia\u0026nbsp;Bell\u0026nbsp;Labs) 📃 Abstract We present a novel method for accurately auditing the differential privacy (DP) guarantees of DP mechanisms. In particular, our solution is applicable to auditing DP guarantees of machine learning (ML) models. Previous auditing methods tightly capture the privacy guarantees of DP-SGD trained models in the white-box setting where the auditor has access to all intermediate models; however, the success of these methods depends on a priori information about the parametric form of the noise and the subsampling ratio used for sampling the gradients. We present a method that does not require such information and is agnostic to the randomization used for the underlying mechanism. Similarly to several previous DP auditing methods, we assume that the auditor has access to a set of independent observations from two one-dimensional distributions corresponding to outputs from two neighbouring datasets. Furthermore, our solution is based on a simple histogram-based density estimation technique to find lower bounds for the statistical distance between these distributions when measured using the hockey-stick divergence. We show that our approach also naturally generalizes the previously considered class of threshold membership inference auditing methods. We improve upon accurate auditing methods such as the f-DP auditing. Moreover, we address an open problem posed by Nasr et al. (2023) on how to accurately audit the subsampled Gaussian mechanism without any knowledge of the parameters of the underlying mechanism. Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition Christian Janos\u0026nbsp;Lebeda (Inria), Matthew\u0026nbsp;Regehr, Gautam\u0026nbsp;Kamath (University\u0026nbsp;of\u0026nbsp;Waterloo), Thomas\u0026nbsp;Steinke (Google\u0026nbsp;DeepMind) 📃 Abstract 📚 Arxiv We consider the problem of computing tight privacy guarantees for the composition of subsampled differentially private mechanisms. Recent algorithms can numerically compute the privacy parameters to arbitrary precision but must be carefully applied. Our main contribution is to address two common points of confusion. First, some privacy accountants assume that the privacy guarantees for the composition of a subsampled mechanism are determined by self-composing the worst-case datasets for the uncomposed mechanism. We show that this is not true in general. Second, Poisson subsampling is sometimes assumed to have similar privacy guarantees compared to sampling without replacement. We show that the privacy guarantees may in fact differ significantly between the two sampling schemes. In particular, we give an example of hyperparameters that result in ε ≈ 1 for Poisson subsampling and ε \u003e 10 for sampling without replacement. This occurs for some parameters that could realistically be chosen for DP-SGD. Backdoor Detection through Replicated Execution of Outsourced Training Hengrui\u0026nbsp;Jia, Sierra\u0026nbsp;Wyllie (University\u0026nbsp;of\u0026nbsp;Toronto\u0026nbsp;and\u0026nbsp;Vector\u0026nbsp;Institute), Akram Bin\u0026nbsp;Sediq, Ahmed A.\u0026nbsp;Ibrahim (Ericsson\u0026nbsp;Canada), Nicolas\u0026nbsp;Papernot (University\u0026nbsp;of\u0026nbsp;Toronto\u0026nbsp;and\u0026nbsp;Vector\u0026nbsp;Institute) 📃 Abstract 📚 Arxiv It is common practice to outsource the training of machine learning models to cloud providers. Clients who do so gain from the cloud's economies of scale, but implicitly assume trust: the server should not deviate from the client's training procedure. A malicious server may, for instance, seek to insert backdoors in the model. Detecting a backdoored model without prior knowledge of both the backdoor attack and its accompanying trigger remains a challenging problem. In this paper, we show that a client with access to multiple cloud providers can replicate a subset of training steps across multiple servers to detect deviation from the training procedure in a similar manner to differential testing. Assuming some cloud-provided servers are benign, we identify malicious servers by the substantial difference between model updates required for backdooring and those resulting from clean training. Perhaps the strongest advantage of our approach is its suitability to clients that have limited-to-no local compute capability to perform training; we leverage the existence of multiple cloud providers to identify malicious updates without expensive human labeling or heavy computation. We demonstrate the capabilities of our approach on an outsourced supervised learning task where 50% of the cloud providers insert their own backdoor; our approach is able to correctly identify 99.6% of them. In essence, our approach is successful because it replaces the signature-based paradigm taken by existing approaches with an anomaly-based detection paradigm. Furthermore, our approach is robust to several attacks from adaptive adversaries utilizing knowledge of our detection scheme. Choosing Public Datasets for Private Machine Learning via Gradient Subspace Distance Xin\u0026nbsp;Gu (Penn\u0026nbsp;State\u0026nbsp;University), Gautam\u0026nbsp;Kamath (University\u0026nbsp;of\u0026nbsp;Waterloo), Steven\u0026nbsp;Wu (Carnegie\u0026nbsp;Mellon\u0026nbsp;University) 📃 Abstract 📚 Arxiv Differentially private stochastic gradient descent privatizes model training by injecting noise into each iteration, where the noise magnitude increases with the number of model parameters. Recent works suggest that we can reduce the noise by leveraging public data for private machine learning, by projecting gradients onto a subspace prescribed by the public data. However, given a choice of public datasets, it is unclear why certain datasets perform better than others for a particular private task, or how to identify the best one. We provide a simple metric which measures a low-dimensional subspace distance between gradients of the public and private examples. We empirically demonstrate that it is well-correlated with resulting model utility when using the public and private dataset pair (i.e., trained model accuracy is monotone in the distance), and thus can be used to select an appropriate public dataset. We provide theoretical analysis demonstrating that the excess risk scales with this subspace distance. This distance is easy to compute and robust to modifications in the setting. ColorSense: A Study on Color Vision in Machine Visual Recognition Ming-Chang\u0026nbsp;Chiu, Yingfei\u0026nbsp;Wang (University\u0026nbsp;of\u0026nbsp;Southern\u0026nbsp;California), Derrick Eui Gyu\u0026nbsp;Kim (Brandeis\u0026nbsp;University), Pin-Yu\u0026nbsp;Chen (IBM\u0026nbsp;Research), Xuezhe\u0026nbsp;Ma (University\u0026nbsp;of\u0026nbsp;Southern\u0026nbsp;California) 📃 Abstract 📚 Arxiv Color vision is essential for human visual perception, but its impact on machine perception is still underexplored. There has been an intensified demand for understanding its role in machine perception for safety-critical tasks such as assistive driving and surgery but lacking suitable datasets. To fill this gap, we curate multipurpose datasets ColorSense, by collecting 110,000 non-trivial human annotations of foreground and background color labels from popular visual recognition benchmarks. To investigate the impact of color vision on machine perception, we assign each image a color discrimination level based on its dominant foreground and background colors and use it to study the impact of color vision on machine perception. We validate the use of our datasets by demonstrating that the level of color discrimination has a dominating effect on the performance of mainstream machine perception models. Specifically, we examine the perception ability of machine vision by considering key factors such as model architecture, training objective, model size, training data, and task complexity. Furthermore, to investigate how color and environmental factors affect the robustness of visual recognition in machine perception, we integrate our ColorSense datasets with image corruptions and perform a more comprehensive visual perception evaluation. We jointly analyze the impact of color vision and image corruption on machine perception. Our findings suggest that \"object recognition\" tasks such as \"classification\" and \"localization\" are susceptible to color vision bias, especially for high-stakes cases such as vehicle classes, and advanced mitigation techniques such as data augmentation and so on only give marginal improvement. Our analyses highlight the need for new approaches toward the performance evaluation of machine perception models in real-world applications. Lastly, we present various potential applications of \"ColorSense\" such as studying spurious correlations. Contextual Confidence and Generative AI Shrey\u0026nbsp;Jain (Microsoft), Zoe\u0026nbsp;Hitzig (Harvard/OpenAI), Pamela\u0026nbsp;Mishkin (OpenAI) 📃 Abstract Generative AI models perturb the foundations of effective human communication. They present new challenges to what we call contextual confidence, disrupting participants’ ability to identify the authentic context of communication and their ability to protect communication from reuse and recombination outside its intended context. In this paper, we describe strategies – tools, technologies and policies – that aim to stabilize communication in the face of these challenges. The strategies we discuss fall into two broad categories. Containment strategies aim to reassert context in environments where it is currently threatened – a reaction to the context-free expectations and norms established by the internet. Mobilization strategies, by contrast, view the rise of generative AI as an opportunity to proactively set new and higher expectations around privacy and authenticity in mediated communication. We apply this framework to a hypothetical scenario to show its value in pointing toward solutions to privacy and authenticity concerns in emerging uses of generative AI. Correlated Privacy Mechanisms for Differentially Private Distributed Mean Estimation Sajani\u0026nbsp;Vithana (Harvard\u0026nbsp;University), Viveck R.\u0026nbsp;Cadambe (Georgia\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology), Flavio P.\u0026nbsp;Calmon (Harvard\u0026nbsp;University), Haewon\u0026nbsp;Jeong (University\u0026nbsp;of\u0026nbsp;California,\u0026nbsp;Santa\u0026nbsp;Barbara) 📃 Abstract 📚 Arxiv Differentially private distributed mean estimation (DP-DME) is a fundamental building block in privacy-preserving federated learning, where a central server estimates the mean of d-dimensional vectors held by n users while ensuring (ε,\\delta)-DP. Local differential privacy (LDP) and distributed DP with secure aggregation (SecAgg) are the most common notions of DP used in DP-DME settings with an untrusted server. LDP provides strong resilience to dropouts, colluding users, and adversarial attacks, but suffers from poor utility. In contrast, SecAgg-based DP-DME achieves an O(n) utility gain over LDP in DME, but requires increased communication and computation overheads and complex multi-round protocols to handle dropouts and attacks. In this work, we present a generalized framework for DP-DME, that captures LDP and SecAgg-based mechanisms as extreme cases. Our framework provides a foundation for developing and analyzing a variety of DP-DME protocols that leverage correlated privacy mechanisms across users. To this end, we propose CorDP-DME, a novel DP-DME mechanism based on the correlated Gaussian mechanism, that spans the gap between DME with LDP and distributed DP. We prove that CorDP-DME offers a favorable balance between utility and resilience to dropout and collusion. We provide an information-theoretic analysis of CorDP-DME, and derive theoretical guarantees for utility under any given privacy parameters and dropout/colluding user thresholds. Our results demonstrate that (anti) correlated Gaussian DP mechanisms can significantly improve utility in mean estimation tasks compared to LDP \u0026ndash; even in adversarial settings \u0026ndash; while maintaining better resilience to dropouts and attacks compared to distributed DP. DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation Yunjuan\u0026nbsp;Wang (Johns\u0026nbsp;Hopkins\u0026nbsp;University), Hussein\u0026nbsp;Hazimeh, Natalia\u0026nbsp;Ponomareva, Alexey\u0026nbsp;Kurakin, Ibrahim\u0026nbsp;Hammoud (Google), Raman\u0026nbsp;Arora (Johns\u0026nbsp;Hopkins\u0026nbsp;University) 📃 Abstract Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift – unsupervised domain adaptation (UDA). Specifically, given a labeled source domain DS and an unlabeled target domain DT with related but different distributions, the goal is to obtain an adversarially robust model for DT. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to DT. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivated by this bound, we develop a novel unified defense framework called Divergence Aware adveRsarial Training (DART), which can be used in conjunction with a variety of standard UDA methods; e.g., DANN (Ganin \u0026 Lempitsky, 2015). DART is applicable to general threat models, including the popular lp-norm model, and does not require heuristic regularizers or architectural changes. We also release DomainRobust: a testbed for evaluating robustness of UDA models to adversarial attacks. DomainRobust consists of 4 multi-domain benchmark datasets (with 46 source-target pairs) and 7 meta-algorithms with a total of 11 variants. Our large-scale experiments demonstrate that on average, DART significantly enhances model robustness on all benchmarks compared to the state of the art, while maintaining competitive standard accuracy. The relative improvement in robustness from DART reaches up to 29.2% on the source-target domain pairs considered. Differentially Private Active Learning: Balancing Effective Data Selection and Privacy Kristian\u0026nbsp;Schwethelm, Johannes\u0026nbsp;Kaiser, Jonas\u0026nbsp;Kuntzer (Technical\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Munich), Mehmet\u0026nbsp;Yigitsoy (deepc\u0026nbsp;GmbH), Daniel\u0026nbsp;Rückert (Technical\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Munich,\u0026nbsp;Imperial\u0026nbsp;College\u0026nbsp;London), Georgios\u0026nbsp;Kaissis (Technical\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Munich,\u0026nbsp;Helmholtz\u0026nbsp;Munich) 📃 Abstract 📚 Arxiv Active learning (AL) is a widely used technique for optimizing data labeling in machine learning by iteratively selecting, labeling, and training on the most informative data. However, its integration with formal privacy-preserving methods, particularly differential privacy (DP), remains largely underexplored. While some works have explored differentially private AL for specialized scenarios like online learning, the fundamental challenge of combining AL with DP in standard learning settings has remained unaddressed, severely limiting AL's applicability in privacy-sensitive domains. This work addresses this gap by introducing differentially private active learning (DP-AL) for standard learning settings. We demonstrate that naively integrating DP-SGD training into AL presents substantial challenges in privacy budget allocation and data utilization. To overcome these challenges, we propose step amplification, which leverages individual sampling probabilities in batch creation to maximize data point participation in training steps, thus optimizing data utilization. Additionally, we investigate the effectiveness of various acquisition functions for data selection under privacy constraints, revealing that many commonly used functions become impractical. Our experiments on vision and natural language processing tasks show that DP-AL can improve performance for specific datasets and model architectures. However, our findings also highlight the limitations of AL in privacy-constrained environments, emphasizing the trade-offs between privacy, model accuracy, and data selection accuracy. Equilibria of Data Marketplaces with Privacy-Aware Sellers under Endogenous Privacy Costs Diptangshu\u0026nbsp;Sen (Georgia\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology), Jingyan\u0026nbsp;Wang (Toyota\u0026nbsp;Technological\u0026nbsp;Institute\u0026nbsp;at\u0026nbsp;Chicago), Juba\u0026nbsp;Ziani (Georgia\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology) 📃 Abstract 📚 Arxiv We study a two-sided online data ecosystem comprised of an online platform, users on the platform, and downstream data buyers. The buyers can buy user data on the platform to run a statistic or machine learning task. Potential users decide whether to join by looking at the trade-off between i) their benefit from joining the platform and interacting with other users and ii) the privacy costs they incur from sharing their data. In light of the rapidly changing user privacy attitudes, we introduce a novel modeling element for two-sided data platforms: the privacy costs of users are endogenous and depend on the number of downstream buyers who purchase and access their data. Then, we characterize marketplace equilibria in certain simple settings. In particular, we provide a full characterization in two variants of our model that correspond to different utility functions for the users: i) when each user gets a constant benefit for participating on the platform and ii) when each user's benefit is linearly increasing in the number of other users that participate. In both variants, equilibria in our setting are significantly different from equilibria when privacy costs are exogenous and fixed, the most significant point of difference being that under exogenous privacy costs, the user-side participation rate is completely independent of the platform's price and the buyer-side decisions and thus can never be improved without investing in improving the quality of service offered. This highlights the importance of taking endogeneity in privacy costs into account. Finally, we provide simulations and semi-synthetic experiments to extend our results to more general assumptions; we experiment with different distributions of users' privacy costs and different functional forms of the users' utilities for joining the platform. Err on the Side of Texture: Texture Bias on Real Data Blaine\u0026nbsp;Hoak, Ryan\u0026nbsp;Sheatsley, Patrick\u0026nbsp;McDaniel (University\u0026nbsp;of\u0026nbsp;Wisconsin-Madison) 📃 Abstract 📚 Arxiv Bias significantly undermines both the accuracy and trustworthiness of machine learning models. To date, one of the strongest biases observed in image classification models is texture bias where models overly rely on texture information rather than shape information. Yet, existing approaches for measuring and mitigating texture bias have not been able to capture how textures impact model robustness in real-world settings. In this work, we introduce the Texture Association Value (TAV), a novel metric that quantifies how strongly models rely on the presence of specific textures when classifying objects. Leveraging TAV, we demonstrate that model accuracy and robustness are heavily influenced by texture. Our results show that texture bias explains the existence of natural adversarial examples, where over 90% of these samples contain textures that are misaligned with the learned texture of their true label, resulting in confident mispredictions. Fair Decentralized Learning Sayan\u0026nbsp;Biswas, Anne-Marie\u0026nbsp;Kermarrec, Rishi\u0026nbsp;Sharma, Thibaud\u0026nbsp;Trinca, Martijn\u0026nbsp;de Vos (EPFL) 📃 Abstract 📚 Arxiv Decentralized learning (DL) is an emerging approach that enables nodes to collaboratively train a machine learning model without sharing raw data. In many application domains, such as healthcare, this approach faces challenges due to the high level of heterogeneity in the training data's feature space. Such feature heterogeneity lowers model utility and negatively impacts fairness, particularly for nodes with under-represented training data. In this paper, we introduce Facade, a clustering-based DL algorithm specifically designed for fair model training when the training data exhibits several distinct features. The challenge of Facade is to assign nodes to clusters, one for each feature, based on the similarity in the features of their local data, without requiring individual nodes to know apriori which cluster they belong to. Facade (1) dynamically assigns nodes to their appropriate clusters over time, and (2) enables nodes to collaboratively train a specialized model for each cluster in a fully decentralized manner. We theoretically prove the convergence of Facade, implement our algorithm, and compare it against three state-of-the-art baselines. Our experimental results on three datasets demonstrate the superiority of our approach in terms of model accuracy and fairness compared to all three competitors. Compared to the best-performing baseline, Facade on the CIFAR-10 dataset also reduces communication costs by 32.3% to reach a target accuracy when cluster sizes are imbalanced. FairDP: Achieving Fairness Certification with Differential Privacy Khang\u0026nbsp;Tran (New\u0026nbsp;Jersey\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology), Ferdinando\u0026nbsp;Fioretto (University\u0026nbsp;of\u0026nbsp;Virginia), Issa\u0026nbsp;Khalil (Qatar\u0026nbsp;Computing\u0026nbsp;Research\u0026nbsp;Institute\u0026nbsp;(QCRI),\u0026nbsp;HBKU), My\u0026nbsp;Thai (University\u0026nbsp;of\u0026nbsp;Florida), Linh\u0026nbsp;Phan (University\u0026nbsp;of\u0026nbsp;Pennsylvania), Hai\u0026nbsp;Phan (New\u0026nbsp;Jersey\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology) 📃 Abstract 📚 Arxiv This paper introduces FairDP, a novel training mechanism designed to provide group fairness certification for the trained model's decisions, along with a differential privacy (DP) guarantee to protect training data. The key idea of FairDP is to train models for distinct individual groups independently, add noise to each group's gradient for data privacy protection, and progressively integrate knowledge from group models to formulate a comprehensive model that balances privacy, utility, and fairness in downstream tasks. By doing so, FairDP ensures equal contribution from each group while gaining control over the amount of DP-preserving noise added to each group's contribution. To provide fairness certification, FairDP leverages the DP-preserving noise to statistically quantify and bound fairness metrics. An extensive theoretical and empirical analysis using benchmark datasets validates the efficacy of FairDP and improved trade-offs between model utility, privacy, and fairness compared with existing methods. Our empirical results indicate that FairDP can improve fairness metrics by more than 65% on average while attaining marginal utility drop (less than 4% on average) under a rigorous DP-preservation across benchmark datasets compared with existing baselines. Get my drift? Catching LLM Task Drift with Activation Deltas Sahar\u0026nbsp;Abdelnabi, Aideen\u0026nbsp;Fay, Giovanni\u0026nbsp;Cherubin, Ahmed\u0026nbsp;Salem (Microsoft), Mario\u0026nbsp;Fritz (CISPA\u0026nbsp;Helmholtz\u0026nbsp;Center\u0026nbsp;for\u0026nbsp;Information\u0026nbsp;Security), Andrew\u0026nbsp;Paverd (Microsoft) 📃 Abstract 📚 Arxiv Large Language Models (LLMs) are commonly used in retrieval-augmented applications to execute user instructions based on data from external sources. For example, modern search engines use LLMs to answer queries based on relevant search results; email plugins summarize emails by processing their content through an LLM. However, the potentially untrusted provenance of these data sources can lead to prompt injection attacks, where the LLM is manipulated by natural language instructions embedded in the external data, causing it to deviate from the user’s original instruction(s). We define this deviation as task drift. Task drift is a significant concern as it allows attackers to exfiltrate data or influence the LLM’s output for other users. We study LLM activations as a solution to detect task drift, showing that activation deltas - the difference in activations before and after processing external data - are strongly correlated with this phenomenon. Through two probing methods, we demonstrate that a simple linear classifier can detect drift with near-perfect ROC AUC on an out-of-distribution test set. We evaluate these methods by making minimal assumptions about how user’s tasks, system prompts, and attacks can be phrased. We observe that this approach generalizes surprisingly well to unseen task domains, such as prompt injections, jailbreaks, and malicious instructions, without being trained on any of these attacks. Interestingly, the fact that this solution does not require any modifications to the LLM (e.g., fine-tuning), as well as its compatibility with existing meta-prompting solutions, makes it cost-efficient and easy to deploy. To encourage further research on activation-based task inspection, decoding, and interpretability, we release our large-scale TaskTracker toolkit, featuring a dataset of over 500K instances, representations from six SoTA language models, and a suite of inspection tools. HALO: Robust Out-of-Distribution Detection via Joint Optimisation Hugo\u0026nbsp;Lyons Keenan, Sarah\u0026nbsp;Erfani, Christopher\u0026nbsp;Leckie (The\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Melbourne) 📃 Abstract 📚 Arxiv Effective out-of-distribution (OOD) detection is crucial for the safe deployment of machine learning models in real-world scenarios. However, recent work has shown that OOD detection methods are vulnerable to adversarial attacks, potentially leading to critical failures in high-stakes applications. This discovery has motivated work on robust OOD detection methods that are capable of maintaining performance under various attack settings. Prior approaches have made progress on this problem but face a number of limitations: often only exhibiting robustness to attacks on OOD data or failing to maintain strong clean performance. In this work, we adapt an existing robust classification framework, TRADES, extending it to the problem of robust OOD detection and discovering a novel objective function. Recognising the critical importance of a strong clean/robust trade-off for OOD detection, we introduce an additional loss term which boosts classification and detection performance. Our approach, called HALO (Helper-based AdversariaL OOD detection), surpasses existing methods and achieves state-of-the-art performance across a number of datasets and attack settings. Extensive experiments demonstrate an average AUROC improvement of 3.15 in clean settings and 7.07 under adversarial attacks when compared to the next best method. Furthermore, HALO exhibits resistance to transferred attacks, offers tuneable performance through hyper-parameter selection, and is compatible with existing OOD detection frameworks out-of-the-box, leaving open the possibility of future performance gains. Hi-ALPS - An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving Alexandra\u0026nbsp;Arzberger, Ramin Tavakoli\u0026nbsp;Kolagari (Technische\u0026nbsp;Hochschule\u0026nbsp;Nürnberg\u0026nbsp;Georg\u0026nbsp;Simon\u0026nbsp;Ohm) 📃 Abstract 📚 Arxiv Light Detection and Ranging (LiDAR) is an essential sensor technology for autonomous driving as it can capture high-resolution 3D data. As 3D object detection systems (OD) are able to interpret such point cloud data, they play a key role in the driving decisions of autonomous vehicles. Consequently, such 3D OD must be robust against all types of perturbations and must therefore be extensively tested. One approach is the use of Adversarial Examples (AE), which are small, sometimes sophisticated perturbations in the input data that change, i.e., falsify, the prediction of the OD. These perturbations are carefully designed based on the weaknesses of the OD. The robustness of the OD cannot be quantified with AE in general, because if the OD is vulnerable to a given attack, it is unclear whether this is due to the robustness of the OD or whether the AE algorithm produces particularly strong AE. The contribution of this work is Hi-ALPS\u0026mdash;Hierarchical AE-based LiDAR Perturbation Level System, where a higher robustness of the OD is required to withstand the perturbations as the perturbation levels increase. In doing so, the Hi-ALPS levels successively implement heuristics followed by established AE approaches. In a series of comprehensive experiments using Hi-ALPS, we quantify the robustness of six state-of-the-art 3D OD under different types of perturbations. The results of the series of experiments show that none of the OD is robust at all Hi-ALPS levels; an important factor for the ranking is that human observers can still correctly recognize the perturbed objects, as the respective perturbations are small. In order to increase the robustness of the OD, we discuss the applicability of state-of-the-art countermeasures. In addition, we derive further suggestions for countermeasures based on our experimental results. Hyperparameters in Score-Based Membership Inference Attacks Gauri\u0026nbsp;Pradhan, Joonas\u0026nbsp;Jälkö, Marlon\u0026nbsp;Tobaben, Antti\u0026nbsp;Honkela (University\u0026nbsp;of\u0026nbsp;Helsinki) 📃 Abstract 📚 Arxiv Membership Inference Attacks (MIAs) have emerged as a valuable framework for evaluating privacy leakage by machine learning models. Score-based MIAs are distinguished, in particular, by their ability to exploit the confidence scores that the model generates for particular inputs. Existing score-based MIAs implicitly assume that the adversary has access to the target model’s hyperparameters, which can be used to train the shadow models for the attack. In this work, we demonstrate that the knowledge of target hyperparameters is not a prerequisite for MIA in the transfer learning setting. Based on this, we propose a novel approach to select the hyperparameters for training the shadow models for MIA when the attacker has no prior knowledge about them by matching the output distributions of target and shadow models. We demonstrate that using the new approach yields hyperparameters that lead to an attack near indistinguishable in performance from an attack that uses target hyperparameters to train the shadow models. Furthermore, we study the empirical privacy risk of unaccounted use of training data for hyperparameter optimization (HPO) in differentially private (DP) transfer learning. We find no statistically significant evidence that performing HPO using training data would increase vulnerability to MIA. Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy Jamie\u0026nbsp;Hayes (Google\u0026nbsp;Deepmind), Amr\u0026nbsp;Khalifa, Ilia\u0026nbsp;Shumailov, Nicolas\u0026nbsp;Papernot, Eleni\u0026nbsp;Triantafillou (Google\u0026nbsp;DeepMind) 📃 Abstract 📚 Arxiv The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each example under attack, is significantly stronger. Indeed, our results show that the commonly used U-MIAs in the unlearning literature overestimate the privacy protection afforded by existing unlearning techniques on both vision and language models. Our investigation reveals a large variance in the vulnerability of different examples to per-example U-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerability for some, but not all, examples that we wish to unlearn, at the expense of increasing it for other examples. Notably, we find that the privacy protection for the remaining training examples may worsen as a consequence of unlearning. We also discuss the fundamental difficulty of equally protecting all examples using existing unlearning schemes, due to the different rates at which different examples are unlearned. We demonstrate that naive attempts at tailoring unlearning stopping criteria to different examples fail to alleviate these issues. Jailbreaking Black Box Large Language Models in Twenty Queries Patrick\u0026nbsp;Chao, Alexander\u0026nbsp;Robey, Edgar\u0026nbsp;Dobriban, Hamed\u0026nbsp;Hassani, George J.\u0026nbsp;Pappas, Eric\u0026nbsp;Wong (University\u0026nbsp;of\u0026nbsp;Pennsylvania) 📃 Abstract There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR\u0026mdash;which is inspired by social engineering attacks\u0026mdash;uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini. Krait: A Backdoor Attack Against Graph Prompt Tuning Ying\u0026nbsp;Song (University\u0026nbsp;of\u0026nbsp;Pittsburgh), Rita\u0026nbsp;Singh (Carnegie\u0026nbsp;Mellon\u0026nbsp;University), Balaji\u0026nbsp;Palanisamy (University\u0026nbsp;of\u0026nbsp;Pittsburgh) 📃 Abstract 📚 Arxiv Graph prompt tuning has emerged as a promising paradigm to effectively transfer general graph knowledge from pre-trained models to various downstream tasks, particularly in few-shot contexts. However, its susceptibility to backdoor attacks, where adversaries insert triggers to manipulate outcomes, raises a critical concern. We conduct the first study to investigate such vulnerability, revealing that backdoors can disguise benign graph prompts, thus evading detection. We introduce Krait, a novel graph prompt backdoor. Specifically, we propose a simple yet effective model-agnostic metric called label non-uniformity homophily to select poisoned candidates, significantly reducing computational complexity. To accommodate diverse attack scenarios and advanced attack types, we design three customizable trigger generation methods to craft prompts as triggers. We propose a novel centroid similarity-based loss function to optimize prompt tuning for attack effectiveness and stealthiness. Experiments on four real-world graphs demonstrate that Krait can efficiently embed triggers to merely 0.15% to 2% of training nodes, achieving high attack success rates without sacrificing clean accuracy. Notably, in one-to-one and all-to-one attacks, Krait can achieve 100% attack success rates by poisoning as few as 2 and 22 nodes, respectively. Our experiments further show that Krait remains potent across different transfer cases, attack types, and graph neural network backbones. Additionally, Krait can be successfully extended to the black-box setting, posing more severe threats. Finally, we analyze why Krait can evade both classical and state-of-the-art defenses, and provide practical insights for detecting and mitigating this class of attacks. Learning with User-Level Differential Privacy under Fixed Compute Budgets Zachary\u0026nbsp;Charles, Arun\u0026nbsp;Ganesh, Ryan\u0026nbsp;McKenna, H. Brendan\u0026nbsp;McMahan, Nicole\u0026nbsp;Mitchell (Google\u0026nbsp;Research), Krishna\u0026nbsp;Pillutla (IIT\u0026nbsp;Madras), Keith\u0026nbsp;Rush (Google\u0026nbsp;Research) 📃 Abstract We investigate practical and scalable algorithms for training machine learning models with user-level differential privacy (DP) in order to provably safeguard all the examples contributed by each user. Motivated by the application of large language model (LLM) fine-tuning, we analyze algorithms under fixed compute budgets, especially large budget settings. We study two variants of DP-SGD with: (1) example-level sampling (DP-SGD-ELS) and per-example gradient clipping, and (2) user-level sampling (DP-SGD-ULS) and per-user gradient clipping. We derive a novel user-level DP accountant that allows us to compute provably tight privacy guarantees for ELS. We show that for fixed compute and privacy budgets, ULS generally yields better results than ELS, especially when each user has a diverse collection of examples and the compute budget is large. We validate our findings through experiments in synthetic mean estimation and LLM fine-tuning tasks under fixed compute budgets. We find that ULS is significantly better in settings where either (1) strong privacy guarantees are required, or (2) the compute budget is large. Notably, our focus on scalability enables us to scale to models with hundreds of millions of parameters and datasets with hundreds of thousands of users. Locking Machine Learning Models into Hardware Eleanor\u0026nbsp;Clifford (Imperial\u0026nbsp;College\u0026nbsp;London), Adhithya\u0026nbsp;Saravanan, Harry\u0026nbsp;Langford (University\u0026nbsp;of\u0026nbsp;Cambridge), Cheng\u0026nbsp;Zhang, Yiren\u0026nbsp;Zhao (Imperial\u0026nbsp;College\u0026nbsp;London), Robert\u0026nbsp;Mullins (University\u0026nbsp;of\u0026nbsp;Cambridge), Ilia\u0026nbsp;Shumailov, Jamie\u0026nbsp;Hayes (Google\u0026nbsp;Deepmind) 📃 Abstract 📚 Arxiv Modern Machine Learning models are expensive IP and business competitiveness often depends on keeping this IP confidential. This in turn restricts how these models are deployed \u0026ndash; for example it is unclear how to deploy a model on-device without inevitably leaking the underlying model. At the same time, confidential computing technologies such as Multi-Party Computation or Homomorphic encryption remain impractical for wide adoption. In this paper we take a different approach and investigate feasibility of ML-specific mechanisms that deter unauthorized model use by restricting the model to only be usable on specific hardware, making adoption on unauthorized hardware inconvenient. That way, even if IP is compromised, it cannot be trivially used without specialised hardware or major model adjustment. In a sense, we seek to enable cheap locking of machine learning models into specific hardware. We demonstrate that locking mechanisms are feasible by either targeting efficiency of model representations, making such models incompatible with quantisation, or tying the model's operation to specific characteristics of hardware, such as the number of clock cycles for arithmetic operations. We demonstrate that locking comes with negligible work and latency overheads, while significantly restricting usability of the resultant model on unauthorized hardware. Mark My Words: Analyzing and Evaluating Language Model Watermarks Julien\u0026nbsp;Piet, Chawin\u0026nbsp;Sitawarin, Vivian\u0026nbsp;Fang, Norman\u0026nbsp;Mu, David\u0026nbsp;Wagner (UC\u0026nbsp;Berkeley) 📃 Abstract The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. It is important to be able to distinguish machine-generated text from human-authored content. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on LLM output watermarking techniques—as opposed to image or model watermarks—and proposes Mark My Words, a comprehensive benchmark for them under different natural language tasks. We focus on three main metrics: quality, size (i.e., the number of tokens needed to detect a watermark), and tamper resistance (i.e., the ability to detect a watermark after perturbing marked text). Current watermarking techniques are nearly practical enough for real-world use: Kirchenbauer et al. [33]’s scheme can watermark models like Llama 2 7B-chat or Mistral-7B-Instruct with no perceivable loss in quality on natural language tasks, the watermark can be detected with fewer than 100 tokens, and their scheme offers good tamper resistance to simple perturbations. However, they struggle to efficiently watermark code generations. We publicly release our benchmark (https://anonymous.4open.science/r/MarkMyWords). Minimax Group Fairness in Strategic Classification Emily\u0026nbsp;Diana (CMU), Saeed\u0026nbsp;Sharifi-Malvajerdi, Ali\u0026nbsp;Vakilian (TTIC) 📃 Abstract 📚 Arxiv In strategic classification, agents manipulate their features, at a cost, to receive a positive classification outcome from the learner's classifier. The goal of the learner in such settings is to learn a classifier that is robust to strategic manipulations. While the majority of works in this domain consider accuracy as the primary objective of the learner, in this work, we consider learning objectives that have group fairness guarantees in addition to accuracy guarantees. We work with the minimax group fairness notion that asks for minimizing the maximal group error rate across population groups. We formalize a fairness-aware Stackelberg game between a population of agents consisting of several groups, with each group having its own cost function, and a learner in the agnostic PAC setting in which the learner is working with a hypothesis class H. When the cost functions of the agents are separable, we show the existence of an efficient algorithm that finds an approximately optimal deterministic classifier for the learner when the number of groups is small. This algorithm remains efficient, both statistically and computationally, even when the hypothesis class H is the set of all classifiers. We then consider cost functions that are not necessarily separable and show the existence of oracle-efficient algorithms that find approximately optimal randomized classifiers for the learner when H has finite strategic VC dimension. These algorithms work under the assumption that the learner is fully transparent: the learner draws a classifier from its distribution (randomized classifier) before the agents best respond. We highlight the effectiveness of such transparency in developing oracle-efficient algorithms. We conclude with verifying the efficacy of our algorithms on real data by conducting an experimental analysis. ML-Based Behavioral Malware Detection Is Far From a Solved Problem Yigitcan\u0026nbsp;Kaya (UC\u0026nbsp;Santa\u0026nbsp;Barbara), Yizheng\u0026nbsp;Chen (University\u0026nbsp;of\u0026nbsp;Maryland\u0026nbsp;College\u0026nbsp;Park), Marcus\u0026nbsp;Botacin (Texas\u0026nbsp;A\u0026M\u0026nbsp;University), Shoumik\u0026nbsp;Saha (University\u0026nbsp;of\u0026nbsp;Maryland\u0026nbsp;College\u0026nbsp;Park), Fabio\u0026nbsp;Pierazzi (King’s\u0026nbsp;College\u0026nbsp;London\u0026nbsp;\u0026\u0026nbsp;University\u0026nbsp;College\u0026nbsp;London), Lorenzo\u0026nbsp;Cavallaro (University\u0026nbsp;College\u0026nbsp;London), David\u0026nbsp;Wagner (UC\u0026nbsp;Berkeley), Tudor\u0026nbsp;Dumitras (University\u0026nbsp;of\u0026nbsp;Maryland\u0026nbsp;College\u0026nbsp;Park) 📃 Abstract 📚 Arxiv Malware detection is a ubiquitous application of Machine Learning (ML) in security. In behavioral malware analysis, the detector relies on features extracted from program execution traces. The research literature has focused on detectors trained with features collected from sandbox environments and evaluated on samples also analyzed in a sandbox. However, in deployment, a malware detector at endpoint hosts often must rely on traces captured from endpoint hosts, not from a sandbox. Thus, there is a gap between the literature and real-world needs. We present the first measurement study of the performance of ML-based malware detectors at real-world endpoints. Leveraging a dataset of sandbox traces and a dataset of in-the-wild program traces, we evaluate two scenarios: (i) an endpoint detector trained on sandbox traces (convenient and easy to train), and (ii) an endpoint detector trained on endpoint traces (more challenging to train, since we need to collect telemetry data). We discover a wide gap between the performance as measured using prior evaluation methods in the literature—over 90%—vs. expected performance in endpoint detection—about 20% (scenario (i)) to 50% (scenario (ii)). We characterize the ML challenges that arise in this domain and contribute to this gap, including label noise, distribution shift, and spurious features. Moreover, we show several techniques that achieve 5–30% relative performance improvements over the baselines. Our evidence suggests that applying detectors trained on sandbox data to endpoint detection is challenging. The most promising direction is training detectors directly on endpoint data, which marks a departure from current practice. To promote progress, we will facilitate researchers to perform realistic detector evaluations against our real-world dataset. Non-Halting Queries: Exploiting Fixed Points in LLMs Ghaith\u0026nbsp;Hammouri, Kemal\u0026nbsp;Derya, Berk\u0026nbsp;Sunar (WPI) 📃 Abstract 📚 Arxiv We introduce a new vulnerability that exploits fixed points in autoregressive models and use it to craft queries that never halt, i.e. an LLM output that does not terminate. More precisely, for what we call non-halting queries, the LLM never samples the end-of-string token . We rigorously analyze the conditions under which the non-halting anomaly presents itself. In particular, at temperature zero, we prove that if a repeating (cyclic) sequence of tokens are observed at the output beyond the context size, then the LLM does not halt. We demonstrate the non-halting in a number of experiments performed in base unaligned models where repeating prompts immediately lead to a non-halting cyclic behavior as predicted by the analysis. Further, we develop a simple recipe that takes the same fixed points observed in the base model and creates a prompt structure %that samples the fixed points from a context to target aligned models. We study the recipe behavior in bypassing alignment in a number of LLMs including gpt-4o, llama-3.1-8b-instruct and gemma-2-9b-it where all models are forced into a non-halting state. Further, we demonstrate the success of the recipe in sending every major model released over the past year into a non-halting state with the same simple prompt even over higher temperatures. We also study direct inversion based techniques to craft new short prompts to induce the non-halting state. Our experiments with the gradient search based inversion technique ARCA show that non-halting is prevalent across models and may be easily induced with a few input tokens. While its impact on the reliability of hosted systems can be mitigated by configuring a hard maximum token limit in the sampler, the non-halting anomaly still manages to break alignment. This underlies the need for further studies and stronger forms of alignment against non-halting anomalies. On the Reliability of Membership Inference Attacks Amrita\u0026nbsp;Roy Chowdhury (University\u0026nbsp;of\u0026nbsp;Michigan,\u0026nbsp;Ann\u0026nbsp;Arbor), Zhifeng\u0026nbsp;Kong, Kamalika\u0026nbsp;Chaudhuri (UCSD) 📃 Abstract A membership inference (MI) attack predicts whether a data point was used for training a machine learning (ML) model. MI attacks are currently the most widely deployed attack for auditing privacy of a ML model. A recent work by Thudi et. al. show that approximate machine unlearning is ill-defined. For this, they introduce the notion of forgeability where using forged datasets, one could unlearn without modifying the model at all. In this paper, we show a connection between machine unlearning and membership inferencing. Specifically, we study how to leverage forgeability to repudiate claims on membership inferencing. We show that the ability to forge enables the dataset owner to construct a Witness-of-Repudiation (WoR) which empowers the dataset owner to plausibly repudiate the predictions of an MI attack. This casts a doubt on the reliability of MI attacks in practice. Our empirical evaluations show that it is possible to construct WoRs~efficiently. PEEL the Layers and Find Yourself: Revisiting Inference-time Data Leakage for Residual Neural Networks Huzaifa\u0026nbsp;Arif (Rensselaer\u0026nbsp;Polytechnic\u0026nbsp;Institute), Pin-Yu\u0026nbsp;Chen, Keerthiram\u0026nbsp;Murugesan, Payel\u0026nbsp;Das (IBM), Alex\u0026nbsp;Gittens (RPI) 📃 Abstract 📚 Arxiv This paper explores inference-time data leakage risks of deep neural networks (NNs), where a curious and honest model service provider is interested in retrieving users' private data inputs solely based on the model inference results. Particularly, we revisit residual NNs due to their popularity in computer vision and our hypothesis that residual blocks are a primary cause of data leakage owing to the use of skip connections. By formulating inference-time data leakage as a constrained optimization problem, we propose a novel backward feature inversion method, PEEL, which can effectively recover block-wise input features from the intermediate output of residual NNs. The surprising results in high-quality input data recovery can be explained by the intuition that the output from these residual blocks can be considered as a noisy version of the input and thus the output retains sufficient information for input recovery. We demonstrate the effectiveness of our layer-by-layer feature inversion method on facial image datasets and pre-trained classifiers. Our results show that PEEL outperforms the state-of-the-art recovery methods by an order of magnitude when evaluated by mean squared error (MSE). Privacy Vulnerabilities in Marginals-based Synthetic Data Steven\u0026nbsp;Golob, Sikha\u0026nbsp;Pentyala, Anuar\u0026nbsp;Maratkhan, Martine\u0026nbsp;De Cock (University\u0026nbsp;of\u0026nbsp;Washington\u0026nbsp;Tacoma) 📃 Abstract 📚 Arxiv When acting as a privacy-enhancing technology, synthetic data generation (SDG) aims to maintain a resemblance to the real data while excluding personally-identifiable information. Many SDG algorithms provide robust differential privacy (DP) guarantees to this end. However, we show that the strongest class of SDG algorithms–those that preserve marginal probabilities, or similar statistics, from the underlying data–leak information about individuals that can be recovered more efficiently than previously understood. We demonstrate this by presenting a novel membership inference attack, MAMA-MIA, and evaluate it against three seminal DP SDG algorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of which SDG algorithm was used, allowing it to learn information about the hidden data more accurately, and orders-of-magnitude faster, than other leading attacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our approach went on to win the first SNAKE (SaNitization Algorithm under attacK ... ε) competition. Private Selection with Heterogeneous Sensitivities Daniela\u0026nbsp;Antonova, Allegra\u0026nbsp;Latimer, Audra\u0026nbsp;McMillan (Apple), Lorenz\u0026nbsp;Wolf (University\u0026nbsp;College\u0026nbsp;London) 📃 Abstract 📚 Arxiv Differentially private (DP) selection involves choosing a high-scoring candidate from a finite candidate pool, where each score depends on a sensitive dataset. This problem arises naturally in a variety of contexts including model selection, hypothesis testing, and within many DP algorithms. Classical methods, such as Report Noisy Max (RNM) [5], assume all candidates' scores are equally sensitive to changes in a single individual's data, but this often isn’t the case. To address this, algorithms like the Generalised Exponential Mechanism (GEM) [19] leverage variability in candidate sensitivities. However, we observe that while these algorithms can outperform RNM in some situations, they may underperform in others—they can even perform worse than random selection. In this work, we explore how the distribution of scores and sensitivities impacts DP selection mechanisms. In all settings we study, we find that there exists a mechanism that utilises heterogeneity in the candidate sensitivities that outperforms standard mechanisms like RNM. However, no single mechanism uniformly outperforms RNM. We propose using the correlation between the scores and sensitivities as the basis for deciding which DP selection mechanism to use. Further, we design a slight variant of GEM that generally performs well whenever GEM performs poorly. Provably Secure Covert Messaging Using Image-based Diffusion Processes Luke\u0026nbsp;Bauer, Wenxuan\u0026nbsp;Bao, Vincent\u0026nbsp;Bindschaedler (University\u0026nbsp;of\u0026nbsp;Florida) 📃 Abstract 📚 Arxiv We consider the problem of securely and robustly embedding covert messages into an image-based diffusion model's output. The sender and receiver want to exchange the maximum amount of information possible per diffusion sampled image while remaining undetected. The adversary wants to detect that such communication is taking place by identifying those diffusion samples that contain covert messages. To maximize robustness to transformations of the diffusion sample, a strategy is for the sender and the receiver to embed the message in the initial latents. We first show that prior work that attempted this is easily broken because their embedding technique alters the latents' distribution. We then propose a straightforward method to embed covert messages in the initial latent without altering the distribution. We prove that our construction achieves indistinguishability to any probabilistic polynomial time adversary. Finally, we discuss and analyze empirically the tradeoffs between embedding capacity, message recovery rates, and robustness. We find that optimizing the inversion method for error correction is key to improve reliability. Range Membership Inference Attacks Jiashu\u0026nbsp;Tao, Reza\u0026nbsp;Shokri (National\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Singapore) 📃 Abstract 📚 Arxiv Machine learning models can leak private information about their training data, but the standard methods to measure this risk, based on membership inference attacks (MIAs), have a major limitation. They only check if a given data point exactly matches a training point, neglecting the potential of similar or partially overlapping data revealing the same private information. To address this issue, we introduce the class of range membership inference attacks (RaMIAs), testing if the model was trained on any data in a specified range (defined based on the semantics of privacy). We formulate the RaMIAs game and design a principled statistical test for its composite hypotheses. We show that RaMIAs can capture privacy loss more accurately and comprehensively than MIAs on various types of data, such as tabular, image, and language. RaMIA paves the way for a more comprehensive and meaningful privacy auditing of machine learning algorithms. Reliable Evaluation of Adversarial Transferability Wenqian\u0026nbsp;Yu (Wuhan\u0026nbsp;University), Jindong\u0026nbsp;Gu (University\u0026nbsp;of\u0026nbsp;Oxford), Zhijiang\u0026nbsp;Li (Wuhan\u0026nbsp;University), Philip\u0026nbsp;Torr (University\u0026nbsp;of\u0026nbsp;Oxford) 📃 Abstract Adversarial examples (AEs) with small adversarial perturbations can mislead deep neural networks (DNNs) into wrong predictions. The AEs created on one DNN can also fool other networks. Over the last few years, the transferability of AEs has garnered significant attention as it is a crucial property for facilitating black-box attacks. Many approaches have been proposed to improve it and transferability of adversarial attacks across Convolutional Neural Networks (CNNs) is remarkably high, as attested by previous research. However, such evaluation methods are not reliable since all CNNs share some similar architectural biases. In this work, we re-evaluate 13 representative transferability-enhancing attack methods where we test on 18 popular models from 4 types of neural networks. Contrary to the prevailing belief, our reevaluation revealed that the adversarial transferability across these diverse network types is notably diminished, and there is no single AE that can be transferred to all popular models. The transferability rank of previous attacking methods changes when under our comprehensive evaluation. Based on our analysis, we propose a reliable benchmark including three evaluation protocols. We release our benchmark to facilitate future research, which includes code, model checkpoints, and evaluation protocols. Robust Knowledge Distillation in Federated Learning: Counteracting Backdoor Attacks Ebtisaam\u0026nbsp;Alharbi (Lancaster\u0026nbsp;University;\u0026nbsp;Umm\u0026nbsp;AlQura\u0026nbsp;University), Leandro Soriano\u0026nbsp;Marcolino, Qiang\u0026nbsp;Ni, Antonios\u0026nbsp;Gouglidis (Lancaster\u0026nbsp;University) 📃 Abstract 📚 Arxiv Federated Learning (FL) enables collaborative model training across multiple devices while preserving data privacy. However, it remains susceptible to backdoor attacks, where malicious participants can compromise the global model. Existing defence methods are limited by strict assumptions on data heterogeneity (Non-Independent and Identically Distributed data) and the proportion of malicious clients, reducing their practicality and effectiveness. To overcome these limitations, we propose Robust Knowledge Distillation (RKD), a novel defence mechanism that enhances model integrity without relying on restrictive assumptions. RKD integrates clustering and model selection techniques to identify and filter out malicious updates, forming a reliable ensemble of models. It then employs knowledge distillation to transfer the collective insights from this ensemble to a global model. Extensive evaluations demonstrate that RKD effectively mitigates backdoor threats while maintaining high model performance, outperforming current state-of-the-art defence methods across various scenarios. SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks Yue\u0026nbsp;Gao (University\u0026nbsp;of\u0026nbsp;Wisconsin-Madison), Ilia\u0026nbsp;Shumailov (University\u0026nbsp;of\u0026nbsp;Oxford), Kassem\u0026nbsp;Fawaz (University\u0026nbsp;of\u0026nbsp;Wisconsin-Madison) 📃 Abstract 📚 Arxiv Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, our ML systems are still at risk, demanding a more comprehensive approach to security, including logging, analyzing, and sharing evidence. While classic security benefits from well-established forensics and intelligence sharing, ML has yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages Hidden Markov Models to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on the second incident, and is robust to adaptive strategies designed to evade forensic analysis. SEA's explanations of the attack's behavior allow us even to fingerprint specific minor bugs in widely used attack libraries. For example, we discover that the SignOPT and Square attacks in ART v1.14 send over 50% duplicated queries. We thoroughly evaluate SEA on a variety of settings and demonstrate that it can recognize the same attack with more than 90% Top-1 and 95% Top-3 accuracy. Finally, we demonstrate how SEA generalizes to other domains like text classification. SnatchML: Hijacking ML models without Training Access Mahmoud\u0026nbsp;Ghorbal (Université\u0026nbsp;Polytechnique\u0026nbsp;Hauts-de-France), Halima\u0026nbsp;Bouzidi (CSIT,\u0026nbsp;Queen's\u0026nbsp;University\u0026nbsp;Belfast), Ioan Marius\u0026nbsp;Bilasco (Université\u0026nbsp;de\u0026nbsp;Lille), Ihsen\u0026nbsp;Alouani (CSIT,\u0026nbsp;Queen's\u0026nbsp;University\u0026nbsp;Belfast) 📃 Abstract 📚 Arxiv The widespread deployment of Machine Learning (ML) models has been accompanied by the emergence of various attacks that threaten their trustworthiness and raise ethical and societal concerns. One such attack is model hijacking, where an adversary seeks to repurpose a victim model to perform a different task than originally intended. Model hijacking can cause significant accountability and security risks since the owner of a hijacked model can be framed for having their model offer illegal or unethical services. Prior works consider model hijacking as a training time attack, whereby an adversary requires full access to the ML model training. In this paper, we consider a stronger threat model for an inference-time hijacking attack, where the adversary has no access to the training phase of the victim model. Our intuition is that ML models, which are typically over-parameterized, might have the capacity to (unintentionally) learn more than the intended task they are trained for. We propose SnatchML, a new training-free model hijacking attack, that leverages the extra capacity learnt by the victim model to infer different tasks that can be semantically related or unrelated to the original one. Our results on models deployed on AWS Sagemaker showed that SnatchML can deliver high accuracy on hijacking tasks. Interestingly, while all previous approaches are limited by the number of classes in the benign task, SnatchML can hijack models for tasks that contain more classes than the original. We explore different methods to mitigate this risk; We propose meta-unlearning, which is designed to help the model unlearn a potentially malicious task while training for the original task. We also provide insights on over-parametrization as a possible inherent factor that facilitates model hijacking, and accordingly, we propose a compression-based countermeasure to counteract this attack. We believe this work offers a previously overlooked perspective on model hijacking attacks, presenting a stronger threat model and higher applicability in real-world contexts. SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds Mauricio\u0026nbsp;Byrd Victorica, György\u0026nbsp;Dán, Henrik\u0026nbsp;Sandberg (KTH\u0026nbsp;Royal\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology) 📃 Abstract State-of-the-art convolutional neural network models for object detection and image classification are vulnerable to physically realizable adversarial perturbations, such as patch attacks. Existing defenses have focused, implicitly or explicitly, on single-patch attacks, rendering them computationally infeasible or inefficient against attacks consisting of multiple patches in the worst cases, or leaving their sensitivity to the number of patches as an open question. In this work, we propose SpaNN, an attack detector whose computational complexity is independent of the expected number of adversarial patches. The key novelty of the proposed detector is that it builds an ensemble of binarized feature maps by applying a set of saliency thresholds to the neural activations of the first convolutional layer of the victim model. It then performs clustering on the ensemble and uses the cluster features as the input to a classifier for attack detection. Contrary to existing detectors, SpaNN does not rely on a fixed saliency threshold for identifying adversarial regions, which makes it robust against white box adversarial attacks. We evaluate SpaNN on four widely used data sets for object detection and classification, and our results show that SpaNN outperforms state-of-the-art defenses by up to 15 and 27 percentage points in the case of object detection and the case of image classification, respectively. Our code will be made available upon publication. Streaming Private Continual Counting via Binning Joel Daniel\u0026nbsp;Andersson, Rasmus\u0026nbsp;Pagh (University\u0026nbsp;of\u0026nbsp;Copenhagen) 📃 Abstract 📚 Arxiv In differential privacy, continual observation refers to problems in which we wish to continuously release a function of a dataset that is revealed one element at a time. The challenge is to maintain a good approximation while keeping the combined output over all time steps differentially private. In the special case of continual counting we seek to approximate a sum of binary input elements. This problem has received considerable attention lately, in part due to its relevance in implementations of differentially private stochastic gradient descent. Factorization mechanisms are the leading approach to continual counting but the best such mechanisms do not work well in streaming settings since they require space proportional to the size of the input. In this paper we present a simple approach to approximating factorization mechanisms in low space via binning, where adjacent matrix entries with similar values are changed to be identical in such a way that a matrix-vector product can be maintained in sublinear space. Our approach has provable sublinear space guarantees for a class of lower triangular matrices whose entries are monotonically decreasing away from the diagonal. We show empirically that even with very low space usage we are able to closely match, and sometimes surpass, the performance of asymptotically optimal factorization mechanisms. Recently, and independently of our work, Dvijotham et al.~have also suggested an approach to implementing factorization mechanisms in a streaming setting. Their work differs from ours in several respects: It only addresses factorization into Toeplitz matrices, only considers maximum error, and uses a different technique based on rational function approximation that seems less versatile than our binning approach. Targeted Manifold Manipulation Against Adversarial Attacks Banibrata\u0026nbsp;Ghosh, Haripriya\u0026nbsp;Harikumar, Svetha\u0026nbsp;Venkatesh, Santu\u0026nbsp;Rana (Deakin\u0026nbsp;University) 📃 Abstract Adversarial attacks on deep models are often guaranteed to find a small and innocuous perturbation to easily alter class label of a test input. We use a novel Targeted Manifold Manipulation (TMM) approach to direct the gradients from the genuine data manifold toward carefully planted traps during such adversarial attacks. The traps are assigned an additional class label (Trapclass) to make the attacks falling in them easily identifiable. Whilst low-perturbation budget attacks will necessarily end up in the traps, high-perturbation budget attacks may escape but only end up far away from the data manifold. Since our manifold manipulation is enforced only locally, we show that such out-of-distribution data can be easily detected by noting the absence of traps around them. Our detection algorithm, denoted as TMM-Def avoids learning a separate model for attack detection and thus remains semantically aligned with the original classifier. Further, since we manipulate the adversarial distribution, it avoids the fundamental difficulty associated with overlapping distributions of clean and attack samples for usual, unmanipulated models. We use nine state-of-the-art adversarial attacks with six well-known image datasets to evaluate our proposed defense. Our results show that the proposed method can detect \\sim99% attacks whilst also being robust to semantic-preserving, transformations, and adaptive attacks. The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes Wencong\u0026nbsp;You, Daniel\u0026nbsp;Lowd (University\u0026nbsp;of\u0026nbsp;Oregon) 📃 Abstract 📚 Arxiv Backdoor attacks on text classifiers can cause them to predict a predefined label when a particular \"trigger\" is present. However, the triggers used are often ungrammatical or otherwise conspicuous. As a result, human annotators, who play a critical role in curating training data, can easily detect and filter out these unnatural texts during manual inspection, reducing the risk of such attacks. We argue that a key criterion for a successful attack is for text with and without triggers to be indistinguishable to humans. To that end, we propose a new backdoor attack, Attribute Backdoor (AttrBkd), which uses fine-grained style attributes as triggers. The triggers are added by an instruct-tuned LLM, which paraphrases the input text using the specified attribute. We propose three recipes for crafting effective trigger attributes, such as extracting the attributes from existing baseline backdoor attacks. Our comprehensive human and automated evaluations find that AttrBkd with these baseline-derived attributes is often more effective (higher attack success rate) and more subtle (fewer instances detected by humans) than the original baseline backdoor attacks. Our human annotation also provides information not captured by automated metrics used in prior work, and scrutinizes the misalignment of these metrics with human judgment. Timber! Poisoning Decision Trees Stefano\u0026nbsp;Calzavara, Lorenzo\u0026nbsp;Cazzaro, Massimo\u0026nbsp;Vettori (Università\u0026nbsp;Ca'\u0026nbsp;Foscari\u0026nbsp;Venezia) 📃 Abstract 📚 Arxiv We present Timber, the first white-box poisoning attack targeting decision trees. Timber is based on a greedy attack strategy leveraging sub-tree retraining to efficiently estimate the damage performed by poisoning a given training instance. The attack relies on a tree annotation procedure which enables sorting training instances so that they are processed in increasing order of computational cost of sub-tree retraining. This sorting yields a variant of Timber supporting an early stopping criterion designed to make poisoning attacks more efficient and feasible on larger datasets. We also discuss an extension of Timber to traditional random forest models, which is useful because decision trees are normally combined into ensembles to improve their predictive power. Our experimental evaluation on public datasets shows that our attacks outperform existing baselines in terms of effectiveness, efficiency or both. Moreover, we show that two representative defenses can mitigate the effect of our attacks, but fail at effectively thwarting them. TS-Inverse: A Gradient Inversion Attack tailored for Federated Time Series Forecasting Models Caspar\u0026nbsp;Meijer, Jiyue\u0026nbsp;Huang (TU\u0026nbsp;Delft), Shreshtha\u0026nbsp;Sharma, Elena\u0026nbsp;Lazovik (TNO), Lydia Y.\u0026nbsp;Chen (TU\u0026nbsp;Delft) 📃 Abstract 📚 Arxiv Federated learning (FL) for time series forecasting (TSF) enables clients with privacy-sensitive time series (TS) data to collaboratively learn accurate forecasting models, e.g., in energy load prediction. Unfortunately, privacy risks in FL persist, as servers can potentially reconstruct clients’ training data through gradient inversion attacks (GIA). While GIA is demonstrated for image classification tasks, little is known for time series regression tasks. In this paper, we first conduct an extensive empirical study on inverting TS data across 4 TSF models and 4 datasets, identifying the unique challenges of reconstructing both observations and targets of TS data. We then propose TS-Inverse, a novel GIA that improves the inversion of TS data through (i) learning a gradient inversion model that outputs quantile predictions, (ii) a unique loss function incorporating periodicity and trend regularization, and (iii) regularization according to the quantile predictions. Our evaluations demonstrate a remarkable performance of TSInverse, achieving at least 2x-10x improvement in terms of sMAPE metric over existing GIA methods on TS data. Code repository: www.github.com/⟨anonymous⟩/⟨anonymous⟩ Verifiable and Provably Secure Machine Unlearning Thorsten\u0026nbsp;Eisenhofer (BIFOLD\u0026nbsp;\u0026\u0026nbsp;TU\u0026nbsp;Berlin), Doreen\u0026nbsp;Riepel (University\u0026nbsp;of\u0026nbsp;California\u0026nbsp;San\u0026nbsp;Diego), Varun\u0026nbsp;Chandrasekaran (University\u0026nbsp;of\u0026nbsp;Illinois\u0026nbsp;Urbana-Champaign), Esha\u0026nbsp;Ghosh (Microsoft\u0026nbsp;Research), Olga\u0026nbsp;Ohrimenko (The\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Melbourne), Nicolas\u0026nbsp;Papernot (University\u0026nbsp;of\u0026nbsp;Toronto\u0026nbsp;and\u0026nbsp;Vector\u0026nbsp;Institute) 📃 Abstract 📚 Arxiv Machine unlearning aims to remove points from the training dataset of a machine learning model after training: e.g., when a user requests their data to be deleted. While many unlearning methods have been proposed, none of them enable users to audit the procedure. Furthermore, recent work shows a user is unable to verify whether their data was unlearnt from an inspection of the model parameter alone. Rather than reasoning about parameters, we propose to view verifiable unlearning as a security problem. To this end, we present the first cryptographic definition of verifiable unlearning to formally capture the guarantees of an unlearning system. In this framework, the server first computes a proof that the model was trained on a dataset D. Given a user's data point d requested to be deleted, the server updates the model using an unlearning algorithm. It then provides a proof of the correct execution of unlearning and that d is not part of D', where D' is the new training dataset (i.e., d has been removed). Our framework is generally applicable to different unlearning techniques that we abstract as admissible functions. We instantiate a protocol in the framework, based on cryptographic assumptions, using SNARKs and hash chains. Finally, we implement the protocol for three different unlearning techniques and validate its feasibility for linear regression, logistic regression, and neural networks. When mitigating bias is unfair: multiplicity and arbitrariness in algorithmic group fairness Natasa\u0026nbsp;Krco (Imperial\u0026nbsp;College,\u0026nbsp;London,\u0026nbsp;United\u0026nbsp;Kingdom), Thibault\u0026nbsp;Laugel, Vincent\u0026nbsp;Grari (AXA,\u0026nbsp;Paris,\u0026nbsp;France), Jean-Michel\u0026nbsp;Loubes (Institut\u0026nbsp;de\u0026nbsp;Mathématiques\u0026nbsp;de\u0026nbsp;Toulouse,\u0026nbsp;Université\u0026nbsp;Paul\u0026nbsp;Sabatier,\u0026nbsp;Toulouse,\u0026nbsp;France), Marcin\u0026nbsp;Detyniecki (AXA,\u0026nbsp;Paris,\u0026nbsp;France) 📃 Abstract 📚 Arxiv Most research on fair machine learning has prioritized optimizing criteria such as Demographic Parity and Equalized Odds. Despite these efforts, there remains a limited understanding of how different bias mitigation strategies affect individual predictions and whether they introduce arbitrariness into the debiasing process. This paper addresses these gaps by exploring whether models that achieve comparable fairness and accuracy metrics impact the same individuals and mitigate bias in a consistent manner. We introduce the FRAME (FaiRness Arbitrariness and Multiplicity Evaluation) framework, which evaluates bias mitigation through five dimensions: Impact Size (how many people were affected), Change Direction (positive versus negative changes), Decision Rates (impact on models’ acceptance rates), Affected Subpopulations (who was affected), and Neglected Subpopulations (where unfairness persists). This framework is intended to help practitioners understand the impacts of debiasing processes and make better-informed decisions regarding model selection. Applying FRAME to various bias mitigation approaches across key datasets allows us to exhibit significant differences in the behaviors of debiasing methods. These findings highlight the limitations of current fairness criteria and the inherent arbitrariness in the debiasing process. Systematization of Knowledge Papers SoK: Fair Clustering: Critique, Caveats, and Future Directions John\u0026nbsp;Dickerson (University\u0026nbsp;of\u0026nbsp;Maryland), Seyed\u0026nbsp;Esmaeili (University\u0026nbsp;of\u0026nbsp;Chicago), Jamie\u0026nbsp;Morgenstern, Claire Jie\u0026nbsp;Zhang (University\u0026nbsp;of\u0026nbsp;Washington) 📃 Abstract 📚 Arxiv Clustering is a fundamental problem in machine learning and operations research. Given the fact that fairness considerations have become of paramount importance in algorithm design, fairness in clustering has received significant attention from the research community. The literature on fair clustering has resulted in a collection of interesting fairness notions and elaborate algorithms. In this paper, we take a critical view of fair clustering, identifying a collection of ignored issues such as the lack of a clear utility characterization and the difficulty in accounting for the downstream effects of a fair clustering algorithm in machine learning settings. In some cases, we demonstrate examples where the application of a fair clustering algorithm can have significant negative impacts on social welfare. We end by identifying a collection of steps that would lead towards more impactful research in fair clustering. SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It) Matthieu\u0026nbsp;Meeus, Igor\u0026nbsp;Shilov (Imperial\u0026nbsp;College\u0026nbsp;London), Shubham\u0026nbsp;Jain (Sense\u0026nbsp;Street), Manuel\u0026nbsp;Faysse (MICS,\u0026nbsp;CentraleSupélec,\u0026nbsp;Université\u0026nbsp;Paris-Saclay), Marek\u0026nbsp;Rei, Yves-Alexandre\u0026nbsp;de Montjoye (Imperial\u0026nbsp;College\u0026nbsp;London) 📃 Abstract Whether Large Language models (LLMs) memorize their training data and what this means, from the privacy leakage of finetuning data to detecting copyright violations \u0026mdash; has become a rapidly growing area of research over the last two years. In the last few months, more than 10 new methods have been proposed to perform sequence-level Membership Inference Attacks (MIAs) against LLMs. Contrary to traditional MIAs which rely on fixed, but randomized records or models, these methods are mostly trained and tested on datasets collected post-hoc. Sets of members and non-members, used to evaluate the MIA, are constructed using informed guesses after the release of a model. This lack of randomization, however, raises concerns of a distribution shift between members and non-members. We here extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, the literature considers a range of target models, motivations and units of interest. We then quantify distribution shifts present in the 6 datasets used in the literature, ranging from books to papers using a model-less bag of word classifier and compare them to MIA results. Our analysis show all of them suffer from such strong distribution shifts that they invalidate the claims of LLMs memorizing strongly in the wild and, potentially, the methodological contributions of the recent papers based on these datasets. Yet, all hope might not be lost. We introduce important considerations to properly evaluate MIAs against LLMs and discuss, in turn, potential ways forwards: randomized test splits, injections of randomized (unique) sequences, randomized fine-tuning, and several post-hoc control methods. While each option comes with its advantages and limitations, we believe they collectively provide solid grounds to guide the development of MIA methods and study LLM memorization. We conclude by proposing and releasing two comprehensive, easy-to-use benchmarks for sequence-level and document-level MIAs against LLMs. LLM memorization is an extremely important and multi-faceted question, yet meaningful progress can only be achieved with the use of robust, independent benchmarks such as the ones we propose here. SoK: On the Offensive Potential of AI Saskia Laura\u0026nbsp;Schröer, Giovanni\u0026nbsp;Apruzzese (University\u0026nbsp;of\u0026nbsp;Liechtenstein), Soheil\u0026nbsp;Human (Vienna\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Economics\u0026nbsp;and\u0026nbsp;Business), Pavel\u0026nbsp;Laskov (University\u0026nbsp;of\u0026nbsp;Liechtenstein), Hyrum S.\u0026nbsp;Anderson (Robust\u0026nbsp;Intelligence), Edward W.N.\u0026nbsp;Bernroider (Vienna\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Economics\u0026nbsp;and\u0026nbsp;Business), Aurore\u0026nbsp;Fass (CISPA\u0026nbsp;Helmholtz\u0026nbsp;Center\u0026nbsp;for\u0026nbsp;Information\u0026nbsp;Security), Ben\u0026nbsp;Nassi (Technion\u0026nbsp;-\u0026nbsp;Israel\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology)), Vera\u0026nbsp;Rimmer (DistriNet,\u0026nbsp;KU\u0026nbsp;Leuven), Fabio\u0026nbsp;Roli (Università\u0026nbsp;degli\u0026nbsp;Studi\u0026nbsp;di\u0026nbsp;Genova), Samer\u0026nbsp;Salam, Chi En (Ashley)\u0026nbsp;Shen (Cisco\u0026nbsp;Systems), Ali\u0026nbsp;Sunyaev (Karlsruhe\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technolog), Tim\u0026nbsp;Wadhwa-Brown (Cisco\u0026nbsp;Systems), Isabel\u0026nbsp;Wagner (University\u0026nbsp;of\u0026nbsp;Basel), Gang\u0026nbsp;Wang (University\u0026nbsp;of\u0026nbsp;Illinois\u0026nbsp;Urbana-Champaign) 📃 Abstract 📚 Arxiv Our society increasingly benefits from Artificial Intelligence (AI). Unfortunately, more and more evidence appears that AI is also used for offensive purposes. Prior works have revealed various examples of use cases in which the deployment of AI can lead to violation of security and privacy objectives. No extant work, however, has been able to draw a holistic picture of the offensive potential of AI. In this SoK paper we seek to lay the ground for a systematic analysis of the heterogeneous capabilities of offensive AI. In particular we (i) account for AI risks to both humans and systems while (ii) consolidating and distilling knowledge from academic literature, expert opinions, industrial venues, as well as laymen—all of which being valuable sources of information on offensive AI. To enable alignment of such diverse sources of knowledge, we devise a common set of criteria reflecting essential technological factors related to offensive AI. With the help of such criteria, we systematically analyze: 95 research papers; 38 InfoSec briefings (from, e.g., BlackHat); the thoughts of 549 participants of a survey we carried out with the general population; and the opinion of 12 experts. Our contributions not only reveal concerning ways (some of which overlooked by prior work) in which AI can be offensively used today, but also represent a foothold to address this threat in the years to come. SoK: What Makes Private Learning Unfair? Kai\u0026nbsp;Yao, Marc\u0026nbsp;Juarez (University\u0026nbsp;of\u0026nbsp;Edinburgh) 📃 Abstract 📚 Arxiv Differential privacy has emerged as the most studied framework for privacy-preserving machine learning. However, recent studies show that enforcing differential privacy guarantees can not only significantly degrade the utility of the model, but also amplify existing disparities in its predictive performance across demographic groups. Although there is extensive research on the identification of factors that contribute to this phenomenon, we still lack a complete understanding of the mechanisms through which differential privacy exacerbates disparities. The literature on this problem is muddled by varying definitions of fairness, differential privacy mechanisms, and inconsistent experimental settings, often leading to seemingly contradictory results. This survey provides the first comprehensive overview of the factors that contribute to the disparate effect of training models with differential privacy guarantees. We discuss their impact and analyze their causal role in such a disparate effect. Our analysis is guided by a taxonomy that categorizes these factors by their position within the machine learning pipeline, allowing us to draw conclusions about their interaction and the feasibility of potential mitigation strategies. We find that factors related to the training dataset and the underlying distribution play a decisive role in the occurrence of disparate impact, highlighting the need for research on these factors to address the issue. Position Papers Position: Episodic memory in AI agents poses risks that should be studied and mitigated Chad\u0026nbsp;DeChant (Columbia\u0026nbsp;University) 📃 Abstract 📚 Arxiv Most current AI models have little ability to store and later retrieve a record or representation of what they do. In human cognition, episodic memories play an important role in both recall of the past as well as planning for the future. The ability to form and use episodic memories would similarly enable a broad range of improved capabilities in an AI agent that interacts with and takes actions in the world. Researchers have begun directing more attention to developing memory abilities in AI models. It is therefore likely that models with such capability will be become widespread in the near future. This could in some ways contribute to making such AI agents safer by enabling users to better monitor, understand, and control their actions. However, as a new capability with wide applications, we argue that it will also introduce significant new risks that researchers should begin to study and address. We outline these risks and benefits and propose four principles to guide the development of episodic memory capabilities so that these will enhance, rather than undermine, the effort to keep AI safe and trustworthy. Position: LLM Unlearning Benchmarks are Weak Measures of Progress Pratiksha\u0026nbsp;Thaker, Shengyuan\u0026nbsp;Hu, Neil\u0026nbsp;Kale, Yash\u0026nbsp;Maurya, Zhiwei Steven\u0026nbsp;Wu, Virginia\u0026nbsp;Smith (CMU) 📃 Abstract 📚 Arxiv Unlearning methods have the potential to improve the privacy and safety of large language models (LLMs) by removing sensitive or harmful information post hoc. The LLM unlearning research community has increasingly turned toward empirical benchmarks to assess the effectiveness of such methods. In this paper, we find that existing benchmarks provide an overly optimistic and potentially misleading view on the effectiveness of candidate unlearning methods. By introducing simple, benign modifications to a number of popular benchmarks, we expose instances where supposedly unlearned information remains accessible, or where the unlearning process has degraded the model’s performance on retained information to a much greater extent than indicated by the original benchmark. We identify that existing benchmarks are particularly vulnerable to modifications that introduce even loose dependencies between the forget and retain information. Further, we show that ambiguity in unlearning targets in existing benchmarks can easily lead to the design of methods that overfit to the given test queries. Based on our findings, we urge the community to be cautious when interpreting benchmark results as reliable measures of progress, and we provide several recommendations to guide future LLM unlearning research. Position: Membership Inference Attacks Cannot Prove that a Model Was Trained On Your Data Jie\u0026nbsp;Zhang, Debeshee\u0026nbsp;Das (ETH\u0026nbsp;Zurich), Gautam\u0026nbsp;Kamath (University\u0026nbsp;of\u0026nbsp;Waterloo), Florian\u0026nbsp;Tramèr (ETH\u0026nbsp;Zurich) 📃 Abstract 📚 Arxiv We consider the problem of a training data proof, where a data creator or owner wants to demonstrate to a third party that some machine learning model was trained on their data. Training data proofs play a key role in recent lawsuits against foundation models trained on web-scale data. Many prior works suggest to instantiate training data proofs using membership inference attacks. We argue that this approach is fundamentally unsound: to provide convincing evidence, the data creator needs to demonstrate that their attack has a low false positive rate, i.e., that the attack's output is unlikely under the null hypothesis that the model was not trained on the target data. Yet, sampling from this null hypothesis is impossible, as we do not know the exact contents of the training set, nor can we (efficiently) retrain a large foundation model. We conclude by offering two paths forward, by showing that data extraction attacks and membership inference on special canary data can be used to create sound training data proofs. ","permalink":"https://satml.org/2025/accepted-papers/","tags":null,"title":"Accepted Papers"},{"categories":null,"contents":" This page provides a checklist for common formatting issues in preparing the final version of your paper for SaTML. The checklist is largely taken from the excellent author instructions provided by JMLR.\nGeneral Verify that you are using the exact template referenced in the CFP. This is a mandatory requirement.\nVerify that your submission adheres to the page limits and other requirements specified in the CFP as well.\nThoroughly proofread your source document to confirm that it requires no further revisions before submission.\nMain Text Prevent widows (a paragraph-ending line that starts at the top of the next page or column) and orphans (a paragraph-opening line that appears alone at the bottom of a page or column).\nAim to avoid runts as well (a single word, part of a word, or a very short line that appears alone at the end of a paragraph).\nAvoid using references as nouns. Do not write phrases like “in [12].” brackets can be omitted and thus do not form proper nouns. Instead, use references as supporting information, e.g., “This algorithm achieves optimal results [12].”\nWhen referring to figures, tables, sections, and similar elements (e.g., “Figure 1”, “Section 2.1”), ensure that terms like “Figure” and “Section” are capitalized.\nAt the end of your paper, the order of sections should be: Acknowledgments, References, then Appendices.\nAcknowledgments should be a unnumbered section.\nAppendix section number should be a letter. You can use \\appendix followed by a regular \\section command to format appendices\nHeadings, Figures and Tables Use title case for the title and all section and subsection headings. Capitalize the first letter of each word, except for small words like “and” and “or.”\nPlace floats (e.g., figure and table environments) at the top or bottom of the page whenever possible. Ensure pages with multiple floats are arranged neatly.\nPosition floats as close as possible to the point where they are first referenced in the text.\nCenter figures and tables within their respective environments.\nPlace captions below figures and above tables, as this is the standard convention.\nMath Math in formal writing should be punctuated as if the extra space is not there, for example, displayed equations should usually be followed by a comma or a period, depending on the surrounding text.\nBefore displayed equations, punctuation (e.g., \u0026ldquo;:\u0026rdquo;) is not necessary unless you would use the same punctuation if the equation were not there.\nProofs should end with a filled box.\nTheorems, Lemmas, etc., should be labeled like Theorem 1, Theorem 2, not as Theorem 2.1, Theorem 2.2.\nReferences It is preferable to cite published conference and journal papers, rather than tech reports.\nPlease cite conference titles consistently. We do not have a enforced style for conference names, but whichever style you choose, be consistent.\nMisc Footnote markers should follow punctuation.\nWhen using dashes in text, please use --- instead of --, with no space before or after the dash\u0026mdash;like this.\nUse \\url commands to reference URLs in papers. This causes hyperref to make the hyperlinks work nicely.\n","permalink":"https://satml.org/2025/final-checklist/","tags":null,"title":"Checklist"},{"categories":null,"contents":" Overview The SaTML 2025 conference will be held from April 9–11, 2025, at the University of Copenhagen. The program features keynote talks, paper presentations, a poster session, and a competition track.\nDay 1 - Wednesday, April 9 Day 2 - Thursday, April 10 Day 3 - Friday, April 11 For a high-level overview of the conference schedule, please visit our Google Calendar. You can also subscribe to the calendar via this iCal Link.\nLocation All events will take place in the Lundbeckfond Auditorium at the University of Copenhagen, except for the poster session and reception. The poster session and reception will be held in the ground floor lobby of the HC Ørsted Institute, located just a 6-minute walk from the auditorium.\nWednesday, April 9, 2025 08:40\u0026ndash;09:00 Opening Remarks 09:00\u0026ndash;09:40 Keynote 1 Session Chair: Konrad Rieck\nMalice, Models and Middlemen Michael Veale (University College London)\n📃 Abstract 👤 Speaker 09:40\u0026ndash;10:00 Coffee Break 10:00\u0026ndash;11:00 Session: Security of LLMs Session Chair: Pavel Laskov\nNon-Halting Queries: Exploiting Fixed Points in LLMs Ghaith\u0026nbsp;Hammouri, Kemal\u0026nbsp;Derya, Berk\u0026nbsp;Sunar (WPI) 📃 Abstract 📚 Arxiv We introduce a new vulnerability that exploits fixed points in autoregressive models and use it to craft queries that never halt, i.e. an LLM output that does not terminate. More precisely, for what we call non-halting queries, the LLM never samples the end-of-string token . We rigorously analyze the conditions under which the non-halting anomaly presents itself. In particular, at temperature zero, we prove that if a repeating (cyclic) sequence of tokens are observed at the output beyond the context size, then the LLM does not halt. We demonstrate the non-halting in a number of experiments performed in base unaligned models where repeating prompts immediately lead to a non-halting cyclic behavior as predicted by the analysis. Further, we develop a simple recipe that takes the same fixed points observed in the base model and creates a prompt structure %that samples the fixed points from a context to target aligned models. We study the recipe behavior in bypassing alignment in a number of LLMs including gpt-4o, llama-3.1-8b-instruct and gemma-2-9b-it where all models are forced into a non-halting state. Further, we demonstrate the success of the recipe in sending every major model released over the past year into a non-halting state with the same simple prompt even over higher temperatures. We also study direct inversion based techniques to craft new short prompts to induce the non-halting state. Our experiments with the gradient search based inversion technique ARCA show that non-halting is prevalent across models and may be easily induced with a few input tokens. While its impact on the reliability of hosted systems can be mitigated by configuring a hard maximum token limit in the sampler, the non-halting anomaly still manages to break alignment. This underlies the need for further studies and stronger forms of alignment against non-halting anomalies. Jailbreaking Black Box Large Language Models in Twenty Queries Patrick\u0026nbsp;Chao, Alexander\u0026nbsp;Robey, Edgar\u0026nbsp;Dobriban, Hamed\u0026nbsp;Hassani, George J.\u0026nbsp;Pappas, Eric\u0026nbsp;Wong (University\u0026nbsp;of\u0026nbsp;Pennsylvania) 📃 Abstract There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR\u0026mdash;which is inspired by social engineering attacks\u0026mdash;uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini. Get my drift? Catching LLM Task Drift with Activation Deltas Sahar\u0026nbsp;Abdelnabi, Aideen\u0026nbsp;Fay, Giovanni\u0026nbsp;Cherubin, Ahmed\u0026nbsp;Salem (Microsoft), Mario\u0026nbsp;Fritz (CISPA\u0026nbsp;Helmholtz\u0026nbsp;Center\u0026nbsp;for\u0026nbsp;Information\u0026nbsp;Security), Andrew\u0026nbsp;Paverd (Microsoft) 📃 Abstract 📚 Arxiv Large Language Models (LLMs) are commonly used in retrieval-augmented applications to execute user instructions based on data from external sources. For example, modern search engines use LLMs to answer queries based on relevant search results; email plugins summarize emails by processing their content through an LLM. However, the potentially untrusted provenance of these data sources can lead to prompt injection attacks, where the LLM is manipulated by natural language instructions embedded in the external data, causing it to deviate from the user’s original instruction(s). We define this deviation as task drift. Task drift is a significant concern as it allows attackers to exfiltrate data or influence the LLM’s output for other users. We study LLM activations as a solution to detect task drift, showing that activation deltas - the difference in activations before and after processing external data - are strongly correlated with this phenomenon. Through two probing methods, we demonstrate that a simple linear classifier can detect drift with near-perfect ROC AUC on an out-of-distribution test set. We evaluate these methods by making minimal assumptions about how user’s tasks, system prompts, and attacks can be phrased. We observe that this approach generalizes surprisingly well to unseen task domains, such as prompt injections, jailbreaks, and malicious instructions, without being trained on any of these attacks. Interestingly, the fact that this solution does not require any modifications to the LLM (e.g., fine-tuning), as well as its compatibility with existing meta-prompting solutions, makes it cost-efficient and easy to deploy. To encourage further research on activation-based task inspection, decoding, and interpretability, we release our large-scale TaskTracker toolkit, featuring a dataset of over 500K instances, representations from six SoTA language models, and a suite of inspection tools. Mark My Words: Analyzing and Evaluating Language Model Watermarks Julien\u0026nbsp;Piet, Chawin\u0026nbsp;Sitawarin, Vivian\u0026nbsp;Fang, Norman\u0026nbsp;Mu, David\u0026nbsp;Wagner (UC\u0026nbsp;Berkeley) 📃 Abstract The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. It is important to be able to distinguish machine-generated text from human-authored content. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on LLM output watermarking techniques—as opposed to image or model watermarks—and proposes Mark My Words, a comprehensive benchmark for them under different natural language tasks. We focus on three main metrics: quality, size (i.e., the number of tokens needed to detect a watermark), and tamper resistance (i.e., the ability to detect a watermark after perturbing marked text). Current watermarking techniques are nearly practical enough for real-world use: Kirchenbauer et al. [33]’s scheme can watermark models like Llama 2 7B-chat or Mistral-7B-Instruct with no perceivable loss in quality on natural language tasks, the watermark can be detected with fewer than 100 tokens, and their scheme offers good tamper resistance to simple perturbations. However, they struggle to efficiently watermark code generations. We publicly release our benchmark (https://anonymous.4open.science/r/MarkMyWords). 11:00\u0026ndash;11:20 Break 11:20\u0026ndash;12:20 Session: Adversaries and Attacks Session Chair: Lea Schönherr\nSnatchML: Hijacking ML models without Training Access Mahmoud\u0026nbsp;Ghorbal (Université\u0026nbsp;Polytechnique\u0026nbsp;Hauts-de-France), Halima\u0026nbsp;Bouzidi (CSIT,\u0026nbsp;Queen's\u0026nbsp;University\u0026nbsp;Belfast), Ioan Marius\u0026nbsp;Bilasco (Université\u0026nbsp;de\u0026nbsp;Lille), Ihsen\u0026nbsp;Alouani (CSIT,\u0026nbsp;Queen's\u0026nbsp;University\u0026nbsp;Belfast) 📃 Abstract 📚 Arxiv The widespread deployment of Machine Learning (ML) models has been accompanied by the emergence of various attacks that threaten their trustworthiness and raise ethical and societal concerns. One such attack is model hijacking, where an adversary seeks to repurpose a victim model to perform a different task than originally intended. Model hijacking can cause significant accountability and security risks since the owner of a hijacked model can be framed for having their model offer illegal or unethical services. Prior works consider model hijacking as a training time attack, whereby an adversary requires full access to the ML model training. In this paper, we consider a stronger threat model for an inference-time hijacking attack, where the adversary has no access to the training phase of the victim model. Our intuition is that ML models, which are typically over-parameterized, might have the capacity to (unintentionally) learn more than the intended task they are trained for. We propose SnatchML, a new training-free model hijacking attack, that leverages the extra capacity learnt by the victim model to infer different tasks that can be semantically related or unrelated to the original one. Our results on models deployed on AWS Sagemaker showed that SnatchML can deliver high accuracy on hijacking tasks. Interestingly, while all previous approaches are limited by the number of classes in the benign task, SnatchML can hijack models for tasks that contain more classes than the original. We explore different methods to mitigate this risk; We propose meta-unlearning, which is designed to help the model unlearn a potentially malicious task while training for the original task. We also provide insights on over-parametrization as a possible inherent factor that facilitates model hijacking, and accordingly, we propose a compression-based countermeasure to counteract this attack. We believe this work offers a previously overlooked perspective on model hijacking attacks, presenting a stronger threat model and higher applicability in real-world contexts. TS-Inverse: A Gradient Inversion Attack tailored for Federated Time Series Forecasting Models Caspar\u0026nbsp;Meijer, Jiyue\u0026nbsp;Huang (TU\u0026nbsp;Delft), Shreshtha\u0026nbsp;Sharma, Elena\u0026nbsp;Lazovik (TNO), Lydia Y.\u0026nbsp;Chen (TU\u0026nbsp;Delft) 📃 Abstract 📚 Arxiv Federated learning (FL) for time series forecasting (TSF) enables clients with privacy-sensitive time series (TS) data to collaboratively learn accurate forecasting models, e.g., in energy load prediction. Unfortunately, privacy risks in FL persist, as servers can potentially reconstruct clients’ training data through gradient inversion attacks (GIA). While GIA is demonstrated for image classification tasks, little is known for time series regression tasks. In this paper, we first conduct an extensive empirical study on inverting TS data across 4 TSF models and 4 datasets, identifying the unique challenges of reconstructing both observations and targets of TS data. We then propose TS-Inverse, a novel GIA that improves the inversion of TS data through (i) learning a gradient inversion model that outputs quantile predictions, (ii) a unique loss function incorporating periodicity and trend regularization, and (iii) regularization according to the quantile predictions. Our evaluations demonstrate a remarkable performance of TSInverse, achieving at least 2x-10x improvement in terms of sMAPE metric over existing GIA methods on TS data. Code repository: www.github.com/⟨anonymous⟩/⟨anonymous⟩ PEEL the Layers and Find Yourself: Revisiting Inference-time Data Leakage for Residual Neural Networks Huzaifa\u0026nbsp;Arif (Rensselaer\u0026nbsp;Polytechnic\u0026nbsp;Institute), Pin-Yu\u0026nbsp;Chen, Keerthiram\u0026nbsp;Murugesan, Payel\u0026nbsp;Das (IBM), Alex\u0026nbsp;Gittens (RPI) 📃 Abstract 📚 Arxiv This paper explores inference-time data leakage risks of deep neural networks (NNs), where a curious and honest model service provider is interested in retrieving users' private data inputs solely based on the model inference results. Particularly, we revisit residual NNs due to their popularity in computer vision and our hypothesis that residual blocks are a primary cause of data leakage owing to the use of skip connections. By formulating inference-time data leakage as a constrained optimization problem, we propose a novel backward feature inversion method, PEEL, which can effectively recover block-wise input features from the intermediate output of residual NNs. The surprising results in high-quality input data recovery can be explained by the intuition that the output from these residual blocks can be considered as a noisy version of the input and thus the output retains sufficient information for input recovery. We demonstrate the effectiveness of our layer-by-layer feature inversion method on facial image datasets and pre-trained classifiers. Our results show that PEEL outperforms the state-of-the-art recovery methods by an order of magnitude when evaluated by mean squared error (MSE). Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks Daryna\u0026nbsp;Oliynyk, Rudolf\u0026nbsp;Mayer (SBA\u0026nbsp;Research), Andreas\u0026nbsp;Rauber (TU\u0026nbsp;Wien) 📃 Abstract 📚 Arxiv Machine learning (ML) models were shown to be vulnerable to different security attacks \u0026ndash; including model stealing attacks, which lead to intellectual property infringement. Among other attack types, substitute model training is an all-encompassing attack applicable to any machine learning model whose behaviour can be approximated from input-output queries. Whereas previous works mainly focused on improving the performance of substitute models by, e.g. developing a new substitute training method, there have been only limited comprehensive ablation studies that try to understand the impact the strength of an attacker has on the substitute model's performance. As a result, different authors came to diverse, sometimes contradicting conclusions. In this work, we therefore exhaustively examine the influence of different factors, primarily forming the attacker's capabilities and knowledge, on a substitute training attack. We investigate how the quality of the substitute training data, the training strategy, and discrepancies between the characteristics of the target and substitute models impact the performance of the attack. Our findings suggest that some of the factors that have been considered important in the past are, in fact, not that influential; instead, we discover new correlations between the attack conditions and success rate. Moreover, our results often exceed or match the performance of attacks that assume a stronger attacker, suggesting that these stronger attacks are likely endangering a model owner's intellectual property to a significantly higher degree than shown until now. 12:20\u0026ndash;13:30 Lunch Break 13:30\u0026ndash;14:30 Session: Backdoor Attacks Session Chair: Yigitcan Kaya\nBackdoor Detection through Replicated Execution of Outsourced Training Hengrui\u0026nbsp;Jia, Sierra\u0026nbsp;Wyllie (University\u0026nbsp;of\u0026nbsp;Toronto\u0026nbsp;and\u0026nbsp;Vector\u0026nbsp;Institute), Akram Bin\u0026nbsp;Sediq, Ahmed A.\u0026nbsp;Ibrahim (Ericsson\u0026nbsp;Canada), Nicolas\u0026nbsp;Papernot (University\u0026nbsp;of\u0026nbsp;Toronto\u0026nbsp;and\u0026nbsp;Vector\u0026nbsp;Institute) 📃 Abstract 📚 Arxiv It is common practice to outsource the training of machine learning models to cloud providers. Clients who do so gain from the cloud's economies of scale, but implicitly assume trust: the server should not deviate from the client's training procedure. A malicious server may, for instance, seek to insert backdoors in the model. Detecting a backdoored model without prior knowledge of both the backdoor attack and its accompanying trigger remains a challenging problem. In this paper, we show that a client with access to multiple cloud providers can replicate a subset of training steps across multiple servers to detect deviation from the training procedure in a similar manner to differential testing. Assuming some cloud-provided servers are benign, we identify malicious servers by the substantial difference between model updates required for backdooring and those resulting from clean training. Perhaps the strongest advantage of our approach is its suitability to clients that have limited-to-no local compute capability to perform training; we leverage the existence of multiple cloud providers to identify malicious updates without expensive human labeling or heavy computation. We demonstrate the capabilities of our approach on an outsourced supervised learning task where 50% of the cloud providers insert their own backdoor; our approach is able to correctly identify 99.6% of them. In essence, our approach is successful because it replaces the signature-based paradigm taken by existing approaches with an anomaly-based detection paradigm. Furthermore, our approach is robust to several attacks from adaptive adversaries utilizing knowledge of our detection scheme. Robust Knowledge Distillation in Federated Learning: Counteracting Backdoor Attacks Ebtisaam\u0026nbsp;Alharbi (Lancaster\u0026nbsp;University;\u0026nbsp;Umm\u0026nbsp;AlQura\u0026nbsp;University), Leandro Soriano\u0026nbsp;Marcolino, Qiang\u0026nbsp;Ni, Antonios\u0026nbsp;Gouglidis (Lancaster\u0026nbsp;University) 📃 Abstract 📚 Arxiv Federated Learning (FL) enables collaborative model training across multiple devices while preserving data privacy. However, it remains susceptible to backdoor attacks, where malicious participants can compromise the global model. Existing defence methods are limited by strict assumptions on data heterogeneity (Non-Independent and Identically Distributed data) and the proportion of malicious clients, reducing their practicality and effectiveness. To overcome these limitations, we propose Robust Knowledge Distillation (RKD), a novel defence mechanism that enhances model integrity without relying on restrictive assumptions. RKD integrates clustering and model selection techniques to identify and filter out malicious updates, forming a reliable ensemble of models. It then employs knowledge distillation to transfer the collective insights from this ensemble to a global model. Extensive evaluations demonstrate that RKD effectively mitigates backdoor threats while maintaining high model performance, outperforming current state-of-the-art defence methods across various scenarios. The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes Wencong\u0026nbsp;You, Daniel\u0026nbsp;Lowd (University\u0026nbsp;of\u0026nbsp;Oregon) 📃 Abstract 📚 Arxiv Backdoor attacks on text classifiers can cause them to predict a predefined label when a particular \"trigger\" is present. However, the triggers used are often ungrammatical or otherwise conspicuous. As a result, human annotators, who play a critical role in curating training data, can easily detect and filter out these unnatural texts during manual inspection, reducing the risk of such attacks. We argue that a key criterion for a successful attack is for text with and without triggers to be indistinguishable to humans. To that end, we propose a new backdoor attack, Attribute Backdoor (AttrBkd), which uses fine-grained style attributes as triggers. The triggers are added by an instruct-tuned LLM, which paraphrases the input text using the specified attribute. We propose three recipes for crafting effective trigger attributes, such as extracting the attributes from existing baseline backdoor attacks. Our comprehensive human and automated evaluations find that AttrBkd with these baseline-derived attributes is often more effective (higher attack success rate) and more subtle (fewer instances detected by humans) than the original baseline backdoor attacks. Our human annotation also provides information not captured by automated metrics used in prior work, and scrutinizes the misalignment of these metrics with human judgment. Krait: A Backdoor Attack Against Graph Prompt Tuning Ying\u0026nbsp;Song (University\u0026nbsp;of\u0026nbsp;Pittsburgh), Rita\u0026nbsp;Singh (Carnegie\u0026nbsp;Mellon\u0026nbsp;University), Balaji\u0026nbsp;Palanisamy (University\u0026nbsp;of\u0026nbsp;Pittsburgh) 📃 Abstract 📚 Arxiv ⚠️ Video talk Graph prompt tuning has emerged as a promising paradigm to effectively transfer general graph knowledge from pre-trained models to various downstream tasks, particularly in few-shot contexts. However, its susceptibility to backdoor attacks, where adversaries insert triggers to manipulate outcomes, raises a critical concern. We conduct the first study to investigate such vulnerability, revealing that backdoors can disguise benign graph prompts, thus evading detection. We introduce Krait, a novel graph prompt backdoor. Specifically, we propose a simple yet effective model-agnostic metric called label non-uniformity homophily to select poisoned candidates, significantly reducing computational complexity. To accommodate diverse attack scenarios and advanced attack types, we design three customizable trigger generation methods to craft prompts as triggers. We propose a novel centroid similarity-based loss function to optimize prompt tuning for attack effectiveness and stealthiness. Experiments on four real-world graphs demonstrate that Krait can efficiently embed triggers to merely 0.15% to 2% of training nodes, achieving high attack success rates without sacrificing clean accuracy. Notably, in one-to-one and all-to-one attacks, Krait can achieve 100% attack success rates by poisoning as few as 2 and 22 nodes, respectively. Our experiments further show that Krait remains potent across different transfer cases, attack types, and graph neural network backbones. Additionally, Krait can be successfully extended to the black-box setting, posing more severe threats. Finally, we analyze why Krait can evade both classical and state-of-the-art defenses, and provide practical insights for detecting and mitigating this class of attacks. 14:30\u0026ndash;14:50 Coffee Break 14:50\u0026ndash;15:20 Competitions Session Chair: Konrad Rieck\n🏁 Adaptive Prompt Injection: LLMail Inject 📃 Abstract 🌐 Website 🏁 Membership Inference on Diffusion-model-based Synthetic Tabular Data 📃 Abstract 🌐 Website 15:20\u0026ndash;15:40 Break 15:40\u0026ndash;16:40 Session: Broader Perspectives Session Chair: Lorenzo Cavallaro\nSoK: On the Offensive Potential of AI Saskia Laura\u0026nbsp;Schröer, Giovanni\u0026nbsp;Apruzzese (University\u0026nbsp;of\u0026nbsp;Liechtenstein), Soheil\u0026nbsp;Human (Vienna\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Economics\u0026nbsp;and\u0026nbsp;Business), Pavel\u0026nbsp;Laskov (University\u0026nbsp;of\u0026nbsp;Liechtenstein), Hyrum S.\u0026nbsp;Anderson (Robust\u0026nbsp;Intelligence), Edward W.N.\u0026nbsp;Bernroider (Vienna\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Economics\u0026nbsp;and\u0026nbsp;Business), Aurore\u0026nbsp;Fass (CISPA\u0026nbsp;Helmholtz\u0026nbsp;Center\u0026nbsp;for\u0026nbsp;Information\u0026nbsp;Security), Ben\u0026nbsp;Nassi (Technion\u0026nbsp;-\u0026nbsp;Israel\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology)), Vera\u0026nbsp;Rimmer (DistriNet,\u0026nbsp;KU\u0026nbsp;Leuven), Fabio\u0026nbsp;Roli (Università\u0026nbsp;degli\u0026nbsp;Studi\u0026nbsp;di\u0026nbsp;Genova), Samer\u0026nbsp;Salam, Chi En (Ashley)\u0026nbsp;Shen (Cisco\u0026nbsp;Systems), Ali\u0026nbsp;Sunyaev (Karlsruhe\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technolog), Tim\u0026nbsp;Wadhwa-Brown (Cisco\u0026nbsp;Systems), Isabel\u0026nbsp;Wagner (University\u0026nbsp;of\u0026nbsp;Basel), Gang\u0026nbsp;Wang (University\u0026nbsp;of\u0026nbsp;Illinois\u0026nbsp;Urbana-Champaign) 📃 Abstract 📚 Arxiv Our society increasingly benefits from Artificial Intelligence (AI). Unfortunately, more and more evidence appears that AI is also used for offensive purposes. Prior works have revealed various examples of use cases in which the deployment of AI can lead to violation of security and privacy objectives. No extant work, however, has been able to draw a holistic picture of the offensive potential of AI. In this SoK paper we seek to lay the ground for a systematic analysis of the heterogeneous capabilities of offensive AI. In particular we (i) account for AI risks to both humans and systems while (ii) consolidating and distilling knowledge from academic literature, expert opinions, industrial venues, as well as laymen—all of which being valuable sources of information on offensive AI. To enable alignment of such diverse sources of knowledge, we devise a common set of criteria reflecting essential technological factors related to offensive AI. With the help of such criteria, we systematically analyze: 95 research papers; 38 InfoSec briefings (from, e.g., BlackHat); the thoughts of 549 participants of a survey we carried out with the general population; and the opinion of 12 experts. Our contributions not only reveal concerning ways (some of which overlooked by prior work) in which AI can be offensively used today, but also represent a foothold to address this threat in the years to come. Contextual Confidence and Generative AI Shrey\u0026nbsp;Jain (Microsoft), Zoe\u0026nbsp;Hitzig (Harvard/OpenAI), Pamela\u0026nbsp;Mishkin (OpenAI) 📃 Abstract Generative AI models perturb the foundations of effective human communication. They present new challenges to what we call contextual confidence, disrupting participants’ ability to identify the authentic context of communication and their ability to protect communication from reuse and recombination outside its intended context. In this paper, we describe strategies – tools, technologies and policies – that aim to stabilize communication in the face of these challenges. The strategies we discuss fall into two broad categories. Containment strategies aim to reassert context in environments where it is currently threatened – a reaction to the context-free expectations and norms established by the internet. Mobilization strategies, by contrast, view the rise of generative AI as an opportunity to proactively set new and higher expectations around privacy and authenticity in mediated communication. We apply this framework to a hypothetical scenario to show its value in pointing toward solutions to privacy and authenticity concerns in emerging uses of generative AI. Locking Machine Learning Models into Hardware Eleanor\u0026nbsp;Clifford (Imperial\u0026nbsp;College\u0026nbsp;London), Adhithya\u0026nbsp;Saravanan, Harry\u0026nbsp;Langford (University\u0026nbsp;of\u0026nbsp;Cambridge), Cheng\u0026nbsp;Zhang, Yiren\u0026nbsp;Zhao (Imperial\u0026nbsp;College\u0026nbsp;London), Robert\u0026nbsp;Mullins (University\u0026nbsp;of\u0026nbsp;Cambridge), Ilia\u0026nbsp;Shumailov, Jamie\u0026nbsp;Hayes (Google\u0026nbsp;Deepmind) 📃 Abstract 📚 Arxiv Modern Machine Learning models are expensive IP and business competitiveness often depends on keeping this IP confidential. This in turn restricts how these models are deployed \u0026ndash; for example it is unclear how to deploy a model on-device without inevitably leaking the underlying model. At the same time, confidential computing technologies such as Multi-Party Computation or Homomorphic encryption remain impractical for wide adoption. In this paper we take a different approach and investigate feasibility of ML-specific mechanisms that deter unauthorized model use by restricting the model to only be usable on specific hardware, making adoption on unauthorized hardware inconvenient. That way, even if IP is compromised, it cannot be trivially used without specialised hardware or major model adjustment. In a sense, we seek to enable cheap locking of machine learning models into specific hardware. We demonstrate that locking mechanisms are feasible by either targeting efficiency of model representations, making such models incompatible with quantisation, or tying the model's operation to specific characteristics of hardware, such as the number of clock cycles for arithmetic operations. We demonstrate that locking comes with negligible work and latency overheads, while significantly restricting usability of the resultant model on unauthorized hardware. Position: Episodic memory in AI agents poses risks that should be studied and mitigated Chad\u0026nbsp;DeChant (Columbia\u0026nbsp;University) 📃 Abstract 📚 Arxiv Most current AI models have little ability to store and later retrieve a record or representation of what they do. In human cognition, episodic memories play an important role in both recall of the past as well as planning for the future. The ability to form and use episodic memories would similarly enable a broad range of improved capabilities in an AI agent that interacts with and takes actions in the world. Researchers have begun directing more attention to developing memory abilities in AI models. It is therefore likely that models with such capability will be become widespread in the near future. This could in some ways contribute to making such AI agents safer by enabling users to better monitor, understand, and control their actions. However, as a new capability with wide applications, we argue that it will also introduce significant new risks that researchers should begin to study and address. We outline these risks and benefits and propose four principles to guide the development of episodic memory capabilities so that these will enhance, rather than undermine, the effort to keep AI safe and trustworthy. 16:40\u0026ndash;17:00 Break 17:00\u0026ndash;20:00 Poster Session and Reception Note: The poster session and reception will be held at HC. Ørsted Institute, ground floor lobby. It is located just a 6-minute walk from the Lundbeck Auditorium. The address is Universitetsparken 5, 2100 Copenhagen. Thursday, April 10, 2025 09:00\u0026ndash;09:40 Keynote 2 Session Chair: Kathrin Grosse\nThe Science of Empirical Privacy Measurement: Memorization and Beyond Kamalika Chaudhuri (University of California San Diego)\n📃 Abstract 👤 Speaker 09:40\u0026ndash;10:00 Coffee Break 10:00\u0026ndash;11:00 Session: Membership Inference Attacks Session Chair: Vera Rimmer\nPosition: Membership Inference Attacks Cannot Prove that a Model Was Trained On Your Data Jie\u0026nbsp;Zhang, Debeshee\u0026nbsp;Das (ETH\u0026nbsp;Zurich), Gautam\u0026nbsp;Kamath (University\u0026nbsp;of\u0026nbsp;Waterloo), Florian\u0026nbsp;Tramèr (ETH\u0026nbsp;Zurich) 📃 Abstract 📚 Arxiv We consider the problem of a training data proof, where a data creator or owner wants to demonstrate to a third party that some machine learning model was trained on their data. Training data proofs play a key role in recent lawsuits against foundation models trained on web-scale data. Many prior works suggest to instantiate training data proofs using membership inference attacks. We argue that this approach is fundamentally unsound: to provide convincing evidence, the data creator needs to demonstrate that their attack has a low false positive rate, i.e., that the attack's output is unlikely under the null hypothesis that the model was not trained on the target data. Yet, sampling from this null hypothesis is impossible, as we do not know the exact contents of the training set, nor can we (efficiently) retrain a large foundation model. We conclude by offering two paths forward, by showing that data extraction attacks and membership inference on special canary data can be used to create sound training data proofs. Range Membership Inference Attacks Jiashu\u0026nbsp;Tao, Reza\u0026nbsp;Shokri (National\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Singapore) 📃 Abstract 📚 Arxiv Machine learning models can leak private information about their training data, but the standard methods to measure this risk, based on membership inference attacks (MIAs), have a major limitation. They only check if a given data point exactly matches a training point, neglecting the potential of similar or partially overlapping data revealing the same private information. To address this issue, we introduce the class of range membership inference attacks (RaMIAs), testing if the model was trained on any data in a specified range (defined based on the semantics of privacy). We formulate the RaMIAs game and design a principled statistical test for its composite hypotheses. We show that RaMIAs can capture privacy loss more accurately and comprehensively than MIAs on various types of data, such as tabular, image, and language. RaMIA paves the way for a more comprehensive and meaningful privacy auditing of machine learning algorithms. Hyperparameters in Score-Based Membership Inference Attacks Gauri\u0026nbsp;Pradhan, Joonas\u0026nbsp;Jälkö, Marlon\u0026nbsp;Tobaben, Antti\u0026nbsp;Honkela (University\u0026nbsp;of\u0026nbsp;Helsinki) 📃 Abstract 📚 Arxiv Membership Inference Attacks (MIAs) have emerged as a valuable framework for evaluating privacy leakage by machine learning models. Score-based MIAs are distinguished, in particular, by their ability to exploit the confidence scores that the model generates for particular inputs. Existing score-based MIAs implicitly assume that the adversary has access to the target model’s hyperparameters, which can be used to train the shadow models for the attack. In this work, we demonstrate that the knowledge of target hyperparameters is not a prerequisite for MIA in the transfer learning setting. Based on this, we propose a novel approach to select the hyperparameters for training the shadow models for MIA when the attacker has no prior knowledge about them by matching the output distributions of target and shadow models. We demonstrate that using the new approach yields hyperparameters that lead to an attack near indistinguishable in performance from an attack that uses target hyperparameters to train the shadow models. Furthermore, we study the empirical privacy risk of unaccounted use of training data for hyperparameter optimization (HPO) in differentially private (DP) transfer learning. We find no statistically significant evidence that performing HPO using training data would increase vulnerability to MIA. SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It) Matthieu\u0026nbsp;Meeus, Igor\u0026nbsp;Shilov (Imperial\u0026nbsp;College\u0026nbsp;London), Shubham\u0026nbsp;Jain (Sense\u0026nbsp;Street), Manuel\u0026nbsp;Faysse (MICS,\u0026nbsp;CentraleSupélec,\u0026nbsp;Université\u0026nbsp;Paris-Saclay), Marek\u0026nbsp;Rei, Yves-Alexandre\u0026nbsp;de Montjoye (Imperial\u0026nbsp;College\u0026nbsp;London) 📃 Abstract Whether Large Language models (LLMs) memorize their training data and what this means, from the privacy leakage of finetuning data to detecting copyright violations \u0026mdash; has become a rapidly growing area of research over the last two years. In the last few months, more than 10 new methods have been proposed to perform sequence-level Membership Inference Attacks (MIAs) against LLMs. Contrary to traditional MIAs which rely on fixed, but randomized records or models, these methods are mostly trained and tested on datasets collected post-hoc. Sets of members and non-members, used to evaluate the MIA, are constructed using informed guesses after the release of a model. This lack of randomization, however, raises concerns of a distribution shift between members and non-members. We here extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, the literature considers a range of target models, motivations and units of interest. We then quantify distribution shifts present in the 6 datasets used in the literature, ranging from books to papers using a model-less bag of word classifier and compare them to MIA results. Our analysis show all of them suffer from such strong distribution shifts that they invalidate the claims of LLMs memorizing strongly in the wild and, potentially, the methodological contributions of the recent papers based on these datasets. Yet, all hope might not be lost. We introduce important considerations to properly evaluate MIAs against LLMs and discuss, in turn, potential ways forwards: randomized test splits, injections of randomized (unique) sequences, randomized fine-tuning, and several post-hoc control methods. While each option comes with its advantages and limitations, we believe they collectively provide solid grounds to guide the development of MIA methods and study LLM memorization. We conclude by proposing and releasing two comprehensive, easy-to-use benchmarks for sequence-level and document-level MIAs against LLMs. LLM memorization is an extremely important and multi-faceted question, yet meaningful progress can only be achieved with the use of robust, independent benchmarks such as the ones we propose here. 11:00\u0026ndash;11:20 Break 11:20\u0026ndash;12:20 Session: Detection and Forensics Session Chair: Giovanni Apruzzese\nHALO: Robust Out-of-Distribution Detection via Joint Optimisation Hugo\u0026nbsp;Lyons Keenan, Sarah\u0026nbsp;Erfani, Christopher\u0026nbsp;Leckie (The\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Melbourne) 📃 Abstract 📚 Arxiv Effective out-of-distribution (OOD) detection is crucial for the safe deployment of machine learning models in real-world scenarios. However, recent work has shown that OOD detection methods are vulnerable to adversarial attacks, potentially leading to critical failures in high-stakes applications. This discovery has motivated work on robust OOD detection methods that are capable of maintaining performance under various attack settings. Prior approaches have made progress on this problem but face a number of limitations: often only exhibiting robustness to attacks on OOD data or failing to maintain strong clean performance. In this work, we adapt an existing robust classification framework, TRADES, extending it to the problem of robust OOD detection and discovering a novel objective function. Recognising the critical importance of a strong clean/robust trade-off for OOD detection, we introduce an additional loss term which boosts classification and detection performance. Our approach, called HALO (Helper-based AdversariaL OOD detection), surpasses existing methods and achieves state-of-the-art performance across a number of datasets and attack settings. Extensive experiments demonstrate an average AUROC improvement of 3.15 in clean settings and 7.07 under adversarial attacks when compared to the next best method. Furthermore, HALO exhibits resistance to transferred attacks, offers tuneable performance through hyper-parameter selection, and is compatible with existing OOD detection frameworks out-of-the-box, leaving open the possibility of future performance gains. Targeted Manifold Manipulation Against Adversarial Attacks Banibrata\u0026nbsp;Ghosh, Haripriya\u0026nbsp;Harikumar, Svetha\u0026nbsp;Venkatesh, Santu\u0026nbsp;Rana (Deakin\u0026nbsp;University) 📃 Abstract Adversarial attacks on deep models are often guaranteed to find a small and innocuous perturbation to easily alter class label of a test input. We use a novel Targeted Manifold Manipulation (TMM) approach to direct the gradients from the genuine data manifold toward carefully planted traps during such adversarial attacks. The traps are assigned an additional class label (Trapclass) to make the attacks falling in them easily identifiable. Whilst low-perturbation budget attacks will necessarily end up in the traps, high-perturbation budget attacks may escape but only end up far away from the data manifold. Since our manifold manipulation is enforced only locally, we show that such out-of-distribution data can be easily detected by noting the absence of traps around them. Our detection algorithm, denoted as TMM-Def avoids learning a separate model for attack detection and thus remains semantically aligned with the original classifier. Further, since we manipulate the adversarial distribution, it avoids the fundamental difficulty associated with overlapping distributions of clean and attack samples for usual, unmanipulated models. We use nine state-of-the-art adversarial attacks with six well-known image datasets to evaluate our proposed defense. Our results show that the proposed method can detect \\sim99% attacks whilst also being robust to semantic-preserving, transformations, and adaptive attacks. SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks Yue\u0026nbsp;Gao (University\u0026nbsp;of\u0026nbsp;Wisconsin-Madison), Ilia\u0026nbsp;Shumailov (University\u0026nbsp;of\u0026nbsp;Oxford), Kassem\u0026nbsp;Fawaz (University\u0026nbsp;of\u0026nbsp;Wisconsin-Madison) 📃 Abstract 📚 Arxiv Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, our ML systems are still at risk, demanding a more comprehensive approach to security, including logging, analyzing, and sharing evidence. While classic security benefits from well-established forensics and intelligence sharing, ML has yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages Hidden Markov Models to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on the second incident, and is robust to adaptive strategies designed to evade forensic analysis. SEA's explanations of the attack's behavior allow us even to fingerprint specific minor bugs in widely used attack libraries. For example, we discover that the SignOPT and Square attacks in ART v1.14 send over 50% duplicated queries. We thoroughly evaluate SEA on a variety of settings and demonstrate that it can recognize the same attack with more than 90% Top-1 and 95% Top-3 accuracy. Finally, we demonstrate how SEA generalizes to other domains like text classification. SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds Mauricio\u0026nbsp;Byrd Victorica, György\u0026nbsp;Dán, Henrik\u0026nbsp;Sandberg (KTH\u0026nbsp;Royal\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology) 📃 Abstract State-of-the-art convolutional neural network models for object detection and image classification are vulnerable to physically realizable adversarial perturbations, such as patch attacks. Existing defenses have focused, implicitly or explicitly, on single-patch attacks, rendering them computationally infeasible or inefficient against attacks consisting of multiple patches in the worst cases, or leaving their sensitivity to the number of patches as an open question. In this work, we propose SpaNN, an attack detector whose computational complexity is independent of the expected number of adversarial patches. The key novelty of the proposed detector is that it builds an ensemble of binarized feature maps by applying a set of saliency thresholds to the neural activations of the first convolutional layer of the victim model. It then performs clustering on the ensemble and uses the cluster features as the input to a classifier for attack detection. Contrary to existing detectors, SpaNN does not rely on a fixed saliency threshold for identifying adversarial regions, which makes it robust against white box adversarial attacks. We evaluate SpaNN on four widely used data sets for object detection and classification, and our results show that SpaNN outperforms state-of-the-art defenses by up to 15 and 27 percentage points in the case of object detection and the case of image classification, respectively. Our code will be made available upon publication. 12:20\u0026ndash;13:30 Lunch Break 13:30\u0026ndash;14:30 Session: Machine Unlearning Session Chair: Yugeng Liu\nVerifiable and Provably Secure Machine Unlearning Thorsten\u0026nbsp;Eisenhofer (BIFOLD\u0026nbsp;\u0026\u0026nbsp;TU\u0026nbsp;Berlin), Doreen\u0026nbsp;Riepel (University\u0026nbsp;of\u0026nbsp;California\u0026nbsp;San\u0026nbsp;Diego), Varun\u0026nbsp;Chandrasekaran (University\u0026nbsp;of\u0026nbsp;Illinois\u0026nbsp;Urbana-Champaign), Esha\u0026nbsp;Ghosh (Microsoft\u0026nbsp;Research), Olga\u0026nbsp;Ohrimenko (The\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Melbourne), Nicolas\u0026nbsp;Papernot (University\u0026nbsp;of\u0026nbsp;Toronto\u0026nbsp;and\u0026nbsp;Vector\u0026nbsp;Institute) 📃 Abstract 📚 Arxiv Machine unlearning aims to remove points from the training dataset of a machine learning model after training: e.g., when a user requests their data to be deleted. While many unlearning methods have been proposed, none of them enable users to audit the procedure. Furthermore, recent work shows a user is unable to verify whether their data was unlearnt from an inspection of the model parameter alone. Rather than reasoning about parameters, we propose to view verifiable unlearning as a security problem. To this end, we present the first cryptographic definition of verifiable unlearning to formally capture the guarantees of an unlearning system. In this framework, the server first computes a proof that the model was trained on a dataset D. Given a user's data point d requested to be deleted, the server updates the model using an unlearning algorithm. It then provides a proof of the correct execution of unlearning and that d is not part of D', where D' is the new training dataset (i.e., d has been removed). Our framework is generally applicable to different unlearning techniques that we abstract as admissible functions. We instantiate a protocol in the framework, based on cryptographic assumptions, using SNARKs and hash chains. Finally, we implement the protocol for three different unlearning techniques and validate its feasibility for linear regression, logistic regression, and neural networks. Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy Jamie\u0026nbsp;Hayes (Google\u0026nbsp;Deepmind), Amr\u0026nbsp;Khalifa, Ilia\u0026nbsp;Shumailov, Nicolas\u0026nbsp;Papernot, Eleni\u0026nbsp;Triantafillou (Google\u0026nbsp;DeepMind) 📃 Abstract 📚 Arxiv The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each example under attack, is significantly stronger. Indeed, our results show that the commonly used U-MIAs in the unlearning literature overestimate the privacy protection afforded by existing unlearning techniques on both vision and language models. Our investigation reveals a large variance in the vulnerability of different examples to per-example U-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerability for some, but not all, examples that we wish to unlearn, at the expense of increasing it for other examples. Notably, we find that the privacy protection for the remaining training examples may worsen as a consequence of unlearning. We also discuss the fundamental difficulty of equally protecting all examples using existing unlearning schemes, due to the different rates at which different examples are unlearned. We demonstrate that naive attempts at tailoring unlearning stopping criteria to different examples fail to alleviate these issues. Position: LLM Unlearning Benchmarks are Weak Measures of Progress Pratiksha\u0026nbsp;Thaker, Shengyuan\u0026nbsp;Hu, Neil\u0026nbsp;Kale, Yash\u0026nbsp;Maurya, Zhiwei Steven\u0026nbsp;Wu, Virginia\u0026nbsp;Smith (CMU) 📃 Abstract 📚 Arxiv Unlearning methods have the potential to improve the privacy and safety of large language models (LLMs) by removing sensitive or harmful information post hoc. The LLM unlearning research community has increasingly turned toward empirical benchmarks to assess the effectiveness of such methods. In this paper, we find that existing benchmarks provide an overly optimistic and potentially misleading view on the effectiveness of candidate unlearning methods. By introducing simple, benign modifications to a number of popular benchmarks, we expose instances where supposedly unlearned information remains accessible, or where the unlearning process has degraded the model’s performance on retained information to a much greater extent than indicated by the original benchmark. We identify that existing benchmarks are particularly vulnerable to modifications that introduce even loose dependencies between the forget and retain information. Further, we show that ambiguity in unlearning targets in existing benchmarks can easily lead to the design of methods that overfit to the given test queries. Based on our findings, we urge the community to be cautious when interpreting benchmark results as reliable measures of progress, and we provide several recommendations to guide future LLM unlearning research. On the Reliability of Membership Inference Attacks Amrita\u0026nbsp;Roy Chowdhury (University\u0026nbsp;of\u0026nbsp;Michigan,\u0026nbsp;Ann\u0026nbsp;Arbor), Zhifeng\u0026nbsp;Kong, Kamalika\u0026nbsp;Chaudhuri (UCSD) 📃 Abstract A membership inference (MI) attack predicts whether a data point was used for training a machine learning (ML) model. MI attacks are currently the most widely deployed attack for auditing privacy of a ML model. A recent work by Thudi et. al. show that approximate machine unlearning is ill-defined. For this, they introduce the notion of forgeability where using forged datasets, one could unlearn without modifying the model at all. In this paper, we show a connection between machine unlearning and membership inferencing. Specifically, we study how to leverage forgeability to repudiate claims on membership inferencing. We show that the ability to forge enables the dataset owner to construct a Witness-of-Repudiation (WoR) which empowers the dataset owner to plausibly repudiate the predictions of an MI attack. This casts a doubt on the reliability of MI attacks in practice. Our empirical evaluations show that it is possible to construct WoRs~efficiently. 14:30\u0026ndash;14:50 Coffee Break 14:50\u0026ndash;15:20 Competitions Session Chair: Konrad Rieck\n🏁 Inference Attacks Against Document VQA 📃 Abstract 🌐 Website 🏁 Robust Android Malware Detection Competition 📃 Abstract 🌐 Website 15:20\u0026ndash;15:40 Break 15:40\u0026ndash;16:40 Session: Private Algorithms Session Chair: Saeyoung Rho\nStreaming Private Continual Counting via Binning Joel Daniel\u0026nbsp;Andersson, Rasmus\u0026nbsp;Pagh (University\u0026nbsp;of\u0026nbsp;Copenhagen) 📃 Abstract 📚 Arxiv In differential privacy, continual observation refers to problems in which we wish to continuously release a function of a dataset that is revealed one element at a time. The challenge is to maintain a good approximation while keeping the combined output over all time steps differentially private. In the special case of continual counting we seek to approximate a sum of binary input elements. This problem has received considerable attention lately, in part due to its relevance in implementations of differentially private stochastic gradient descent. Factorization mechanisms are the leading approach to continual counting but the best such mechanisms do not work well in streaming settings since they require space proportional to the size of the input. In this paper we present a simple approach to approximating factorization mechanisms in low space via binning, where adjacent matrix entries with similar values are changed to be identical in such a way that a matrix-vector product can be maintained in sublinear space. Our approach has provable sublinear space guarantees for a class of lower triangular matrices whose entries are monotonically decreasing away from the diagonal. We show empirically that even with very low space usage we are able to closely match, and sometimes surpass, the performance of asymptotically optimal factorization mechanisms. Recently, and independently of our work, Dvijotham et al.~have also suggested an approach to implementing factorization mechanisms in a streaming setting. Their work differs from ours in several respects: It only addresses factorization into Toeplitz matrices, only considers maximum error, and uses a different technique based on rational function approximation that seems less versatile than our binning approach. Correlated Privacy Mechanisms for Differentially Private Distributed Mean Estimation Sajani\u0026nbsp;Vithana (Harvard\u0026nbsp;University), Viveck R.\u0026nbsp;Cadambe (Georgia\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology), Flavio P.\u0026nbsp;Calmon (Harvard\u0026nbsp;University), Haewon\u0026nbsp;Jeong (University\u0026nbsp;of\u0026nbsp;California,\u0026nbsp;Santa\u0026nbsp;Barbara) 📃 Abstract 📚 Arxiv Differentially private distributed mean estimation (DP-DME) is a fundamental building block in privacy-preserving federated learning, where a central server estimates the mean of d-dimensional vectors held by n users while ensuring (ε,\\delta)-DP. Local differential privacy (LDP) and distributed DP with secure aggregation (SecAgg) are the most common notions of DP used in DP-DME settings with an untrusted server. LDP provides strong resilience to dropouts, colluding users, and adversarial attacks, but suffers from poor utility. In contrast, SecAgg-based DP-DME achieves an O(n) utility gain over LDP in DME, but requires increased communication and computation overheads and complex multi-round protocols to handle dropouts and attacks. In this work, we present a generalized framework for DP-DME, that captures LDP and SecAgg-based mechanisms as extreme cases. Our framework provides a foundation for developing and analyzing a variety of DP-DME protocols that leverage correlated privacy mechanisms across users. To this end, we propose CorDP-DME, a novel DP-DME mechanism based on the correlated Gaussian mechanism, that spans the gap between DME with LDP and distributed DP. We prove that CorDP-DME offers a favorable balance between utility and resilience to dropout and collusion. We provide an information-theoretic analysis of CorDP-DME, and derive theoretical guarantees for utility under any given privacy parameters and dropout/colluding user thresholds. Our results demonstrate that (anti) correlated Gaussian DP mechanisms can significantly improve utility in mean estimation tasks compared to LDP \u0026ndash; even in adversarial settings \u0026ndash; while maintaining better resilience to dropouts and attacks compared to distributed DP. Private Selection with Heterogeneous Sensitivities Daniela\u0026nbsp;Antonova, Allegra\u0026nbsp;Latimer, Audra\u0026nbsp;McMillan (Apple), Lorenz\u0026nbsp;Wolf (University\u0026nbsp;College\u0026nbsp;London) 📃 Abstract 📚 Arxiv Differentially private (DP) selection involves choosing a high-scoring candidate from a finite candidate pool, where each score depends on a sensitive dataset. This problem arises naturally in a variety of contexts including model selection, hypothesis testing, and within many DP algorithms. Classical methods, such as Report Noisy Max (RNM) [5], assume all candidates' scores are equally sensitive to changes in a single individual's data, but this often isn’t the case. To address this, algorithms like the Generalised Exponential Mechanism (GEM) [19] leverage variability in candidate sensitivities. However, we observe that while these algorithms can outperform RNM in some situations, they may underperform in others—they can even perform worse than random selection. In this work, we explore how the distribution of scores and sensitivities impacts DP selection mechanisms. In all settings we study, we find that there exists a mechanism that utilises heterogeneity in the candidate sensitivities that outperforms standard mechanisms like RNM. However, no single mechanism uniformly outperforms RNM. We propose using the correlation between the scores and sensitivities as the basis for deciding which DP selection mechanism to use. Further, we design a slight variant of GEM that generally performs well whenever GEM performs poorly. Equilibria of Data Marketplaces with Privacy-Aware Sellers under Endogenous Privacy Costs Diptangshu\u0026nbsp;Sen (Georgia\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology), Jingyan\u0026nbsp;Wang (Toyota\u0026nbsp;Technological\u0026nbsp;Institute\u0026nbsp;at\u0026nbsp;Chicago), Juba\u0026nbsp;Ziani (Georgia\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology) 📃 Abstract 📚 Arxiv ⚠️ Video talk We study a two-sided online data ecosystem comprised of an online platform, users on the platform, and downstream data buyers. The buyers can buy user data on the platform to run a statistic or machine learning task. Potential users decide whether to join by looking at the trade-off between i) their benefit from joining the platform and interacting with other users and ii) the privacy costs they incur from sharing their data. In light of the rapidly changing user privacy attitudes, we introduce a novel modeling element for two-sided data platforms: the privacy costs of users are endogenous and depend on the number of downstream buyers who purchase and access their data. Then, we characterize marketplace equilibria in certain simple settings. In particular, we provide a full characterization in two variants of our model that correspond to different utility functions for the users: i) when each user gets a constant benefit for participating on the platform and ii) when each user's benefit is linearly increasing in the number of other users that participate. In both variants, equilibria in our setting are significantly different from equilibria when privacy costs are exogenous and fixed, the most significant point of difference being that under exogenous privacy costs, the user-side participation rate is completely independent of the platform's price and the buyer-side decisions and thus can never be improved without investing in improving the quality of service offered. This highlights the importance of taking endogeneity in privacy costs into account. Finally, we provide simulations and semi-synthetic experiments to extend our results to more general assumptions; we experiment with different distributions of users' privacy costs and different functional forms of the users' utilities for joining the platform. 16:40\u0026ndash;17:00 Break 17:00\u0026ndash;17:45 Session: Vision and Perception Session Chair: Adam Dziedzic\nAdversarially Robust CLIP Models Can Induce Better (Robust) Perceptual Metrics Francesco\u0026nbsp;Croce (EPFL), Christian\u0026nbsp;Schlarmann, Naman Deep\u0026nbsp;Singh, Matthias\u0026nbsp;Hein (University\u0026nbsp;of\u0026nbsp;Tuebingen) 📃 Abstract 📚 Arxiv Measuring perceptual similarity is a key tool in computer vision. In recent years perceptual metrics based on features extracted from neural networks with large and diverse training sets, e.g. CLIP, have become popular. At the same time, the metrics extracted from features of neural networks are not adversarially robust. In this paper we show that adversarially robust CLIP models, called \\rclipf, obtained by unsupervised adversarial finetuning induce a better and adversarially robust perceptual metric that outperforms existing metrics in a zero-shot setting, and further matches the performance of state-of-the-art metrics while being robust after fine-tuning. Moreover, our perceptual metric achieves strong performance on related task such as robust image-to-image retrieval, which becomes especially relevant when applied to ``Not Safe for Work'' (NSFW) content detection and dataset filtering. While standard perceptual metrics can be easily attacked by a small perturbation completely degrading NSFW detection, our robust perceptual metric maintains high accuracy under an attack while having similar performance for unperturbed images. Finally, perceptual metrics induced by robust CLIP models have higher interpretability: feature inversion can show which images are considered similar, while text inversion can find what images are associated to a given prompt. This also allows us to visualize the very rich visual concepts learned by a CLIP model, including memorized persons, paintings and complex queries. Err on the Side of Texture: Texture Bias on Real Data Blaine\u0026nbsp;Hoak, Ryan\u0026nbsp;Sheatsley, Patrick\u0026nbsp;McDaniel (University\u0026nbsp;of\u0026nbsp;Wisconsin-Madison) 📃 Abstract 📚 Arxiv Bias significantly undermines both the accuracy and trustworthiness of machine learning models. To date, one of the strongest biases observed in image classification models is texture bias where models overly rely on texture information rather than shape information. Yet, existing approaches for measuring and mitigating texture bias have not been able to capture how textures impact model robustness in real-world settings. In this work, we introduce the Texture Association Value (TAV), a novel metric that quantifies how strongly models rely on the presence of specific textures when classifying objects. Leveraging TAV, we demonstrate that model accuracy and robustness are heavily influenced by texture. Our results show that texture bias explains the existence of natural adversarial examples, where over 90% of these samples contain textures that are misaligned with the learned texture of their true label, resulting in confident mispredictions. ColorSense: A Study on Color Vision in Machine Visual Recognition Ming-Chang\u0026nbsp;Chiu, Yingfei\u0026nbsp;Wang (University\u0026nbsp;of\u0026nbsp;Southern\u0026nbsp;California), Derrick Eui Gyu\u0026nbsp;Kim (Brandeis\u0026nbsp;University), Pin-Yu\u0026nbsp;Chen (IBM\u0026nbsp;Research), Xuezhe\u0026nbsp;Ma (University\u0026nbsp;of\u0026nbsp;Southern\u0026nbsp;California) 📃 Abstract 📚 Arxiv Color vision is essential for human visual perception, but its impact on machine perception is still underexplored. There has been an intensified demand for understanding its role in machine perception for safety-critical tasks such as assistive driving and surgery but lacking suitable datasets. To fill this gap, we curate multipurpose datasets ColorSense, by collecting 110,000 non-trivial human annotations of foreground and background color labels from popular visual recognition benchmarks. To investigate the impact of color vision on machine perception, we assign each image a color discrimination level based on its dominant foreground and background colors and use it to study the impact of color vision on machine perception. We validate the use of our datasets by demonstrating that the level of color discrimination has a dominating effect on the performance of mainstream machine perception models. Specifically, we examine the perception ability of machine vision by considering key factors such as model architecture, training objective, model size, training data, and task complexity. Furthermore, to investigate how color and environmental factors affect the robustness of visual recognition in machine perception, we integrate our ColorSense datasets with image corruptions and perform a more comprehensive visual perception evaluation. We jointly analyze the impact of color vision and image corruption on machine perception. Our findings suggest that \"object recognition\" tasks such as \"classification\" and \"localization\" are susceptible to color vision bias, especially for high-stakes cases such as vehicle classes, and advanced mitigation techniques such as data augmentation and so on only give marginal improvement. Our analyses highlight the need for new approaches toward the performance evaluation of machine perception models in real-world applications. Lastly, we present various potential applications of \"ColorSense\" such as studying spurious correlations. Friday, April 11, 2025 09:00\u0026ndash;09:40 Keynote 3 Session Chair: Amartya Sanyal\nArtificial Intelligence: Should you trust it? Matt Turek (Defense Advanced Research Agency)\n📃 Abstract 👤 Speaker 09:40\u0026ndash;10:00 Coffee Break 10:00\u0026ndash;11:00 Session: Fairness and Bias Session Chair: Rasmus Pagh\nFair Decentralized Learning Sayan\u0026nbsp;Biswas, Anne-Marie\u0026nbsp;Kermarrec, Rishi\u0026nbsp;Sharma, Thibaud\u0026nbsp;Trinca, Martijn\u0026nbsp;de Vos (EPFL) 📃 Abstract 📚 Arxiv Decentralized learning (DL) is an emerging approach that enables nodes to collaboratively train a machine learning model without sharing raw data. In many application domains, such as healthcare, this approach faces challenges due to the high level of heterogeneity in the training data's feature space. Such feature heterogeneity lowers model utility and negatively impacts fairness, particularly for nodes with under-represented training data. In this paper, we introduce Facade, a clustering-based DL algorithm specifically designed for fair model training when the training data exhibits several distinct features. The challenge of Facade is to assign nodes to clusters, one for each feature, based on the similarity in the features of their local data, without requiring individual nodes to know apriori which cluster they belong to. Facade (1) dynamically assigns nodes to their appropriate clusters over time, and (2) enables nodes to collaboratively train a specialized model for each cluster in a fully decentralized manner. We theoretically prove the convergence of Facade, implement our algorithm, and compare it against three state-of-the-art baselines. Our experimental results on three datasets demonstrate the superiority of our approach in terms of model accuracy and fairness compared to all three competitors. Compared to the best-performing baseline, Facade on the CIFAR-10 dataset also reduces communication costs by 32.3% to reach a target accuracy when cluster sizes are imbalanced. When mitigating bias is unfair: multiplicity and arbitrariness in algorithmic group fairness Natasa\u0026nbsp;Krco (Imperial\u0026nbsp;College,\u0026nbsp;London,\u0026nbsp;United\u0026nbsp;Kingdom), Thibault\u0026nbsp;Laugel, Vincent\u0026nbsp;Grari (AXA,\u0026nbsp;Paris,\u0026nbsp;France), Jean-Michel\u0026nbsp;Loubes (Institut\u0026nbsp;de\u0026nbsp;Mathématiques\u0026nbsp;de\u0026nbsp;Toulouse,\u0026nbsp;Université\u0026nbsp;Paul\u0026nbsp;Sabatier,\u0026nbsp;Toulouse,\u0026nbsp;France), Marcin\u0026nbsp;Detyniecki (AXA,\u0026nbsp;Paris,\u0026nbsp;France) 📃 Abstract 📚 Arxiv Most research on fair machine learning has prioritized optimizing criteria such as Demographic Parity and Equalized Odds. Despite these efforts, there remains a limited understanding of how different bias mitigation strategies affect individual predictions and whether they introduce arbitrariness into the debiasing process. This paper addresses these gaps by exploring whether models that achieve comparable fairness and accuracy metrics impact the same individuals and mitigate bias in a consistent manner. We introduce the FRAME (FaiRness Arbitrariness and Multiplicity Evaluation) framework, which evaluates bias mitigation through five dimensions: Impact Size (how many people were affected), Change Direction (positive versus negative changes), Decision Rates (impact on models’ acceptance rates), Affected Subpopulations (who was affected), and Neglected Subpopulations (where unfairness persists). This framework is intended to help practitioners understand the impacts of debiasing processes and make better-informed decisions regarding model selection. Applying FRAME to various bias mitigation approaches across key datasets allows us to exhibit significant differences in the behaviors of debiasing methods. These findings highlight the limitations of current fairness criteria and the inherent arbitrariness in the debiasing process. Minimax Group Fairness in Strategic Classification Emily\u0026nbsp;Diana (CMU), Saeed\u0026nbsp;Sharifi-Malvajerdi, Ali\u0026nbsp;Vakilian (TTIC) 📃 Abstract 📚 Arxiv In strategic classification, agents manipulate their features, at a cost, to receive a positive classification outcome from the learner's classifier. The goal of the learner in such settings is to learn a classifier that is robust to strategic manipulations. While the majority of works in this domain consider accuracy as the primary objective of the learner, in this work, we consider learning objectives that have group fairness guarantees in addition to accuracy guarantees. We work with the minimax group fairness notion that asks for minimizing the maximal group error rate across population groups. We formalize a fairness-aware Stackelberg game between a population of agents consisting of several groups, with each group having its own cost function, and a learner in the agnostic PAC setting in which the learner is working with a hypothesis class H. When the cost functions of the agents are separable, we show the existence of an efficient algorithm that finds an approximately optimal deterministic classifier for the learner when the number of groups is small. This algorithm remains efficient, both statistically and computationally, even when the hypothesis class H is the set of all classifiers. We then consider cost functions that are not necessarily separable and show the existence of oracle-efficient algorithms that find approximately optimal randomized classifiers for the learner when H has finite strategic VC dimension. These algorithms work under the assumption that the learner is fully transparent: the learner draws a classifier from its distribution (randomized classifier) before the agents best respond. We highlight the effectiveness of such transparency in developing oracle-efficient algorithms. We conclude with verifying the efficacy of our algorithms on real data by conducting an experimental analysis. SoK: Fair Clustering: Critique, Caveats, and Future Directions John\u0026nbsp;Dickerson (University\u0026nbsp;of\u0026nbsp;Maryland), Seyed\u0026nbsp;Esmaeili (University\u0026nbsp;of\u0026nbsp;Chicago), Jamie\u0026nbsp;Morgenstern, Claire Jie\u0026nbsp;Zhang (University\u0026nbsp;of\u0026nbsp;Washington) 📃 Abstract 📚 Arxiv ⚠️ Video talk Clustering is a fundamental problem in machine learning and operations research. Given the fact that fairness considerations have become of paramount importance in algorithm design, fairness in clustering has received significant attention from the research community. The literature on fair clustering has resulted in a collection of interesting fairness notions and elaborate algorithms. In this paper, we take a critical view of fair clustering, identifying a collection of ignored issues such as the lack of a clear utility characterization and the difficulty in accounting for the downstream effects of a fair clustering algorithm in machine learning settings. In some cases, we demonstrate examples where the application of a fair clustering algorithm can have significant negative impacts on social welfare. We end by identifying a collection of steps that would lead towards more impactful research in fair clustering. 11:00\u0026ndash;11:20 Break 11:20\u0026ndash;12:20 Session: Robustness and Transferability Session Chair: Kathrin Grosse\nDART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation Yunjuan\u0026nbsp;Wang (Johns\u0026nbsp;Hopkins\u0026nbsp;University), Hussein\u0026nbsp;Hazimeh, Natalia\u0026nbsp;Ponomareva, Alexey\u0026nbsp;Kurakin, Ibrahim\u0026nbsp;Hammoud (Google), Raman\u0026nbsp;Arora (Johns\u0026nbsp;Hopkins\u0026nbsp;University) 📃 Abstract Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift – unsupervised domain adaptation (UDA). Specifically, given a labeled source domain DS and an unlabeled target domain DT with related but different distributions, the goal is to obtain an adversarially robust model for DT. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to DT. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivated by this bound, we develop a novel unified defense framework called Divergence Aware adveRsarial Training (DART), which can be used in conjunction with a variety of standard UDA methods; e.g., DANN (Ganin \u0026 Lempitsky, 2015). DART is applicable to general threat models, including the popular lp-norm model, and does not require heuristic regularizers or architectural changes. We also release DomainRobust: a testbed for evaluating robustness of UDA models to adversarial attacks. DomainRobust consists of 4 multi-domain benchmark datasets (with 46 source-target pairs) and 7 meta-algorithms with a total of 11 variants. Our large-scale experiments demonstrate that on average, DART significantly enhances model robustness on all benchmarks compared to the state of the art, while maintaining competitive standard accuracy. The relative improvement in robustness from DART reaches up to 29.2% on the source-target domain pairs considered. Reliable Evaluation of Adversarial Transferability Wenqian\u0026nbsp;Yu (Wuhan\u0026nbsp;University), Jindong\u0026nbsp;Gu (University\u0026nbsp;of\u0026nbsp;Oxford), Zhijiang\u0026nbsp;Li (Wuhan\u0026nbsp;University), Philip\u0026nbsp;Torr (University\u0026nbsp;of\u0026nbsp;Oxford) 📃 Abstract Adversarial examples (AEs) with small adversarial perturbations can mislead deep neural networks (DNNs) into wrong predictions. The AEs created on one DNN can also fool other networks. Over the last few years, the transferability of AEs has garnered significant attention as it is a crucial property for facilitating black-box attacks. Many approaches have been proposed to improve it and transferability of adversarial attacks across Convolutional Neural Networks (CNNs) is remarkably high, as attested by previous research. However, such evaluation methods are not reliable since all CNNs share some similar architectural biases. In this work, we re-evaluate 13 representative transferability-enhancing attack methods where we test on 18 popular models from 4 types of neural networks. Contrary to the prevailing belief, our reevaluation revealed that the adversarial transferability across these diverse network types is notably diminished, and there is no single AE that can be transferred to all popular models. The transferability rank of previous attacking methods changes when under our comprehensive evaluation. Based on our analysis, we propose a reliable benchmark including three evaluation protocols. We release our benchmark to facilitate future research, which includes code, model checkpoints, and evaluation protocols. Hi-ALPS - An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving Alexandra\u0026nbsp;Arzberger, Ramin Tavakoli\u0026nbsp;Kolagari (Technische\u0026nbsp;Hochschule\u0026nbsp;Nürnberg\u0026nbsp;Georg\u0026nbsp;Simon\u0026nbsp;Ohm) 📃 Abstract 📚 Arxiv Light Detection and Ranging (LiDAR) is an essential sensor technology for autonomous driving as it can capture high-resolution 3D data. As 3D object detection systems (OD) are able to interpret such point cloud data, they play a key role in the driving decisions of autonomous vehicles. Consequently, such 3D OD must be robust against all types of perturbations and must therefore be extensively tested. One approach is the use of Adversarial Examples (AE), which are small, sometimes sophisticated perturbations in the input data that change, i.e., falsify, the prediction of the OD. These perturbations are carefully designed based on the weaknesses of the OD. The robustness of the OD cannot be quantified with AE in general, because if the OD is vulnerable to a given attack, it is unclear whether this is due to the robustness of the OD or whether the AE algorithm produces particularly strong AE. The contribution of this work is Hi-ALPS\u0026mdash;Hierarchical AE-based LiDAR Perturbation Level System, where a higher robustness of the OD is required to withstand the perturbations as the perturbation levels increase. In doing so, the Hi-ALPS levels successively implement heuristics followed by established AE approaches. In a series of comprehensive experiments using Hi-ALPS, we quantify the robustness of six state-of-the-art 3D OD under different types of perturbations. The results of the series of experiments show that none of the OD is robust at all Hi-ALPS levels; an important factor for the ranking is that human observers can still correctly recognize the perturbed objects, as the respective perturbations are small. In order to increase the robustness of the OD, we discuss the applicability of state-of-the-art countermeasures. In addition, we derive further suggestions for countermeasures based on our experimental results. Timber! Poisoning Decision Trees Stefano\u0026nbsp;Calzavara, Lorenzo\u0026nbsp;Cazzaro, Massimo\u0026nbsp;Vettori (Università\u0026nbsp;Ca'\u0026nbsp;Foscari\u0026nbsp;Venezia) 📃 Abstract 📚 Arxiv We present Timber, the first white-box poisoning attack targeting decision trees. Timber is based on a greedy attack strategy leveraging sub-tree retraining to efficiently estimate the damage performed by poisoning a given training instance. The attack relies on a tree annotation procedure which enables sorting training instances so that they are processed in increasing order of computational cost of sub-tree retraining. This sorting yields a variant of Timber supporting an early stopping criterion designed to make poisoning attacks more efficient and feasible on larger datasets. We also discuss an extension of Timber to traditional random forest models, which is useful because decision trees are normally combined into ensembles to improve their predictive power. Our experimental evaluation on public datasets shows that our attacks outperform existing baselines in terms of effectiveness, efficiency or both. Moreover, we show that two representative defenses can mitigate the effect of our attacks, but fail at effectively thwarting them. 12:20\u0026ndash;13:30 Lunch Break 13:30\u0026ndash;14:30 Session: Private Learning Session Chair: Franziska Boenisch\nSoK: What Makes Private Learning Unfair? Kai\u0026nbsp;Yao, Marc\u0026nbsp;Juarez (University\u0026nbsp;of\u0026nbsp;Edinburgh) 📃 Abstract 📚 Arxiv Differential privacy has emerged as the most studied framework for privacy-preserving machine learning. However, recent studies show that enforcing differential privacy guarantees can not only significantly degrade the utility of the model, but also amplify existing disparities in its predictive performance across demographic groups. Although there is extensive research on the identification of factors that contribute to this phenomenon, we still lack a complete understanding of the mechanisms through which differential privacy exacerbates disparities. The literature on this problem is muddled by varying definitions of fairness, differential privacy mechanisms, and inconsistent experimental settings, often leading to seemingly contradictory results. This survey provides the first comprehensive overview of the factors that contribute to the disparate effect of training models with differential privacy guarantees. We discuss their impact and analyze their causal role in such a disparate effect. Our analysis is guided by a taxonomy that categorizes these factors by their position within the machine learning pipeline, allowing us to draw conclusions about their interaction and the feasibility of potential mitigation strategies. We find that factors related to the training dataset and the underlying distribution play a decisive role in the occurrence of disparate impact, highlighting the need for research on these factors to address the issue. Differentially Private Active Learning: Balancing Effective Data Selection and Privacy Kristian\u0026nbsp;Schwethelm, Johannes\u0026nbsp;Kaiser, Jonas\u0026nbsp;Kuntzer (Technical\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Munich), Mehmet\u0026nbsp;Yigitsoy (deepc\u0026nbsp;GmbH), Daniel\u0026nbsp;Rückert (Technical\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Munich,\u0026nbsp;Imperial\u0026nbsp;College\u0026nbsp;London), Georgios\u0026nbsp;Kaissis (Technical\u0026nbsp;University\u0026nbsp;of\u0026nbsp;Munich,\u0026nbsp;Helmholtz\u0026nbsp;Munich) 📃 Abstract 📚 Arxiv Active learning (AL) is a widely used technique for optimizing data labeling in machine learning by iteratively selecting, labeling, and training on the most informative data. However, its integration with formal privacy-preserving methods, particularly differential privacy (DP), remains largely underexplored. While some works have explored differentially private AL for specialized scenarios like online learning, the fundamental challenge of combining AL with DP in standard learning settings has remained unaddressed, severely limiting AL's applicability in privacy-sensitive domains. This work addresses this gap by introducing differentially private active learning (DP-AL) for standard learning settings. We demonstrate that naively integrating DP-SGD training into AL presents substantial challenges in privacy budget allocation and data utilization. To overcome these challenges, we propose step amplification, which leverages individual sampling probabilities in batch creation to maximize data point participation in training steps, thus optimizing data utilization. Additionally, we investigate the effectiveness of various acquisition functions for data selection under privacy constraints, revealing that many commonly used functions become impractical. Our experiments on vision and natural language processing tasks show that DP-AL can improve performance for specific datasets and model architectures. However, our findings also highlight the limitations of AL in privacy-constrained environments, emphasizing the trade-offs between privacy, model accuracy, and data selection accuracy. Choosing Public Datasets for Private Machine Learning via Gradient Subspace Distance Xin\u0026nbsp;Gu (Penn\u0026nbsp;State\u0026nbsp;University), Gautam\u0026nbsp;Kamath (University\u0026nbsp;of\u0026nbsp;Waterloo), Steven\u0026nbsp;Wu (Carnegie\u0026nbsp;Mellon\u0026nbsp;University) 📃 Abstract 📚 Arxiv Differentially private stochastic gradient descent privatizes model training by injecting noise into each iteration, where the noise magnitude increases with the number of model parameters. Recent works suggest that we can reduce the noise by leveraging public data for private machine learning, by projecting gradients onto a subspace prescribed by the public data. However, given a choice of public datasets, it is unclear why certain datasets perform better than others for a particular private task, or how to identify the best one. We provide a simple metric which measures a low-dimensional subspace distance between gradients of the public and private examples. We empirically demonstrate that it is well-correlated with resulting model utility when using the public and private dataset pair (i.e., trained model accuracy is monotone in the distance), and thus can be used to select an appropriate public dataset. We provide theoretical analysis demonstrating that the excess risk scales with this subspace distance. This distance is easy to compute and robust to modifications in the setting. Learning with User-Level Differential Privacy under Fixed Compute Budgets Zachary\u0026nbsp;Charles, Arun\u0026nbsp;Ganesh, Ryan\u0026nbsp;McKenna, H. Brendan\u0026nbsp;McMahan, Nicole\u0026nbsp;Mitchell (Google\u0026nbsp;Research), Krishna\u0026nbsp;Pillutla (IIT\u0026nbsp;Madras), Keith\u0026nbsp;Rush (Google\u0026nbsp;Research) 📃 Abstract We investigate practical and scalable algorithms for training machine learning models with user-level differential privacy (DP) in order to provably safeguard all the examples contributed by each user. Motivated by the application of large language model (LLM) fine-tuning, we analyze algorithms under fixed compute budgets, especially large budget settings. We study two variants of DP-SGD with: (1) example-level sampling (DP-SGD-ELS) and per-example gradient clipping, and (2) user-level sampling (DP-SGD-ULS) and per-user gradient clipping. We derive a novel user-level DP accountant that allows us to compute provably tight privacy guarantees for ELS. We show that for fixed compute and privacy budgets, ULS generally yields better results than ELS, especially when each user has a diverse collection of examples and the compute budget is large. We validate our findings through experiments in synthetic mean estimation and LLM fine-tuning tasks under fixed compute budgets. We find that ULS is significantly better in settings where either (1) strong privacy guarantees are required, or (2) the compute budget is large. Notably, our focus on scalability enables us to scale to models with hundreds of millions of parameters and datasets with hundreds of thousands of users. 14:30\u0026ndash;14:50 Coffee Break 14:50\u0026ndash;15:20 Session: Malware and Steganography Session Chair: Ilia Shumailov\nML-Based Behavioral Malware Detection Is Far From a Solved Problem Yigitcan\u0026nbsp;Kaya (UC\u0026nbsp;Santa\u0026nbsp;Barbara), Yizheng\u0026nbsp;Chen (University\u0026nbsp;of\u0026nbsp;Maryland\u0026nbsp;College\u0026nbsp;Park), Marcus\u0026nbsp;Botacin (Texas\u0026nbsp;A\u0026M\u0026nbsp;University), Shoumik\u0026nbsp;Saha (University\u0026nbsp;of\u0026nbsp;Maryland\u0026nbsp;College\u0026nbsp;Park), Fabio\u0026nbsp;Pierazzi (King’s\u0026nbsp;College\u0026nbsp;London\u0026nbsp;\u0026\u0026nbsp;University\u0026nbsp;College\u0026nbsp;London), Lorenzo\u0026nbsp;Cavallaro (University\u0026nbsp;College\u0026nbsp;London), David\u0026nbsp;Wagner (UC\u0026nbsp;Berkeley), Tudor\u0026nbsp;Dumitras (University\u0026nbsp;of\u0026nbsp;Maryland\u0026nbsp;College\u0026nbsp;Park) 📃 Abstract 📚 Arxiv Malware detection is a ubiquitous application of Machine Learning (ML) in security. In behavioral malware analysis, the detector relies on features extracted from program execution traces. The research literature has focused on detectors trained with features collected from sandbox environments and evaluated on samples also analyzed in a sandbox. However, in deployment, a malware detector at endpoint hosts often must rely on traces captured from endpoint hosts, not from a sandbox. Thus, there is a gap between the literature and real-world needs. We present the first measurement study of the performance of ML-based malware detectors at real-world endpoints. Leveraging a dataset of sandbox traces and a dataset of in-the-wild program traces, we evaluate two scenarios: (i) an endpoint detector trained on sandbox traces (convenient and easy to train), and (ii) an endpoint detector trained on endpoint traces (more challenging to train, since we need to collect telemetry data). We discover a wide gap between the performance as measured using prior evaluation methods in the literature—over 90%—vs. expected performance in endpoint detection—about 20% (scenario (i)) to 50% (scenario (ii)). We characterize the ML challenges that arise in this domain and contribute to this gap, including label noise, distribution shift, and spurious features. Moreover, we show several techniques that achieve 5–30% relative performance improvements over the baselines. Our evidence suggests that applying detectors trained on sandbox data to endpoint detection is challenging. The most promising direction is training detectors directly on endpoint data, which marks a departure from current practice. To promote progress, we will facilitate researchers to perform realistic detector evaluations against our real-world dataset. Provably Secure Covert Messaging Using Image-based Diffusion Processes Luke\u0026nbsp;Bauer, Wenxuan\u0026nbsp;Bao, Vincent\u0026nbsp;Bindschaedler (University\u0026nbsp;of\u0026nbsp;Florida) 📃 Abstract 📚 Arxiv We consider the problem of securely and robustly embedding covert messages into an image-based diffusion model's output. The sender and receiver want to exchange the maximum amount of information possible per diffusion sampled image while remaining undetected. The adversary wants to detect that such communication is taking place by identifying those diffusion samples that contain covert messages. To maximize robustness to transformations of the diffusion sample, a strategy is for the sender and the receiver to embed the message in the initial latents. We first show that prior work that attempted this is easily broken because their embedding technique alters the latents' distribution. We then propose a straightforward method to embed covert messages in the initial latent without altering the distribution. We prove that our construction achieves indistinguishability to any probabilistic polynomial time adversary. Finally, we discuss and analyze empirically the tradeoffs between embedding capacity, message recovery rates, and robustness. We find that optimizing the inversion method for error correction is key to improve reliability. 15:20\u0026ndash;15:40 Break 15:40\u0026ndash;16:40 Session: Differential Privacy Session Chair: Antti Honkela\nFairDP: Achieving Fairness Certification with Differential Privacy Khang\u0026nbsp;Tran (New\u0026nbsp;Jersey\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology), Ferdinando\u0026nbsp;Fioretto (University\u0026nbsp;of\u0026nbsp;Virginia), Issa\u0026nbsp;Khalil (Qatar\u0026nbsp;Computing\u0026nbsp;Research\u0026nbsp;Institute\u0026nbsp;(QCRI),\u0026nbsp;HBKU), My\u0026nbsp;Thai (University\u0026nbsp;of\u0026nbsp;Florida), Linh\u0026nbsp;Phan (University\u0026nbsp;of\u0026nbsp;Pennsylvania), Hai\u0026nbsp;Phan (New\u0026nbsp;Jersey\u0026nbsp;Institute\u0026nbsp;of\u0026nbsp;Technology) 📃 Abstract 📚 Arxiv This paper introduces FairDP, a novel training mechanism designed to provide group fairness certification for the trained model's decisions, along with a differential privacy (DP) guarantee to protect training data. The key idea of FairDP is to train models for distinct individual groups independently, add noise to each group's gradient for data privacy protection, and progressively integrate knowledge from group models to formulate a comprehensive model that balances privacy, utility, and fairness in downstream tasks. By doing so, FairDP ensures equal contribution from each group while gaining control over the amount of DP-preserving noise added to each group's contribution. To provide fairness certification, FairDP leverages the DP-preserving noise to statistically quantify and bound fairness metrics. An extensive theoretical and empirical analysis using benchmark datasets validates the efficacy of FairDP and improved trade-offs between model utility, privacy, and fairness compared with existing methods. Our empirical results indicate that FairDP can improve fairness metrics by more than 65% on average while attaining marginal utility drop (less than 4% on average) under a rigorous DP-preservation across benchmark datasets compared with existing baselines. Privacy Vulnerabilities in Marginals-based Synthetic Data Steven\u0026nbsp;Golob, Sikha\u0026nbsp;Pentyala, Anuar\u0026nbsp;Maratkhan, Martine\u0026nbsp;De Cock (University\u0026nbsp;of\u0026nbsp;Washington\u0026nbsp;Tacoma) 📃 Abstract 📚 Arxiv When acting as a privacy-enhancing technology, synthetic data generation (SDG) aims to maintain a resemblance to the real data while excluding personally-identifiable information. Many SDG algorithms provide robust differential privacy (DP) guarantees to this end. However, we show that the strongest class of SDG algorithms–those that preserve marginal probabilities, or similar statistics, from the underlying data–leak information about individuals that can be recovered more efficiently than previously understood. We demonstrate this by presenting a novel membership inference attack, MAMA-MIA, and evaluate it against three seminal DP SDG algorithms: MST, PrivBayes, and Private-GSD. MAMA-MIA leverages knowledge of which SDG algorithm was used, allowing it to learn information about the hidden data more accurately, and orders-of-magnitude faster, than other leading attacks. We use MAMA-MIA to lend insight into existing SDG vulnerabilities. Our approach went on to win the first SNAKE (SaNitization Algorithm under attacK ... ε) competition. Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition Christian Janos\u0026nbsp;Lebeda (Inria), Matthew\u0026nbsp;Regehr, Gautam\u0026nbsp;Kamath (University\u0026nbsp;of\u0026nbsp;Waterloo), Thomas\u0026nbsp;Steinke (Google\u0026nbsp;DeepMind) 📃 Abstract 📚 Arxiv We consider the problem of computing tight privacy guarantees for the composition of subsampled differentially private mechanisms. Recent algorithms can numerically compute the privacy parameters to arbitrary precision but must be carefully applied. Our main contribution is to address two common points of confusion. First, some privacy accountants assume that the privacy guarantees for the composition of a subsampled mechanism are determined by self-composing the worst-case datasets for the uncomposed mechanism. We show that this is not true in general. Second, Poisson subsampling is sometimes assumed to have similar privacy guarantees compared to sampling without replacement. We show that the privacy guarantees may in fact differ significantly between the two sampling schemes. In particular, we give an example of hyperparameters that result in ε ≈ 1 for Poisson subsampling and ε \u003e 10 for sampling without replacement. This occurs for some parameters that could realistically be chosen for DP-SGD. Auditing Differential Privacy Guarantees Using Density Estimation Antti\u0026nbsp;Koskela, Jafar\u0026nbsp;Mohammadi (Nokia\u0026nbsp;Bell\u0026nbsp;Labs) 📃 Abstract We present a novel method for accurately auditing the differential privacy (DP) guarantees of DP mechanisms. In particular, our solution is applicable to auditing DP guarantees of machine learning (ML) models. Previous auditing methods tightly capture the privacy guarantees of DP-SGD trained models in the white-box setting where the auditor has access to all intermediate models; however, the success of these methods depends on a priori information about the parametric form of the noise and the subsampling ratio used for sampling the gradients. We present a method that does not require such information and is agnostic to the randomization used for the underlying mechanism. Similarly to several previous DP auditing methods, we assume that the auditor has access to a set of independent observations from two one-dimensional distributions corresponding to outputs from two neighbouring datasets. Furthermore, our solution is based on a simple histogram-based density estimation technique to find lower bounds for the statistical distance between these distributions when measured using the hockey-stick divergence. We show that our approach also naturally generalizes the previously considered class of threshold membership inference auditing methods. We improve upon accurate auditing methods such as the f-DP auditing. Moreover, we address an open problem posed by Nasr et al. (2023) on how to accurately audit the subsampled Gaussian mechanism without any knowledge of the parameters of the underlying mechanism. 16:40\u0026ndash;17:00 Closing Remarks ","permalink":"https://satml.org/2025/program/","tags":null,"title":"Program"},{"categories":null,"contents":null,"permalink":"https://satml.org/2025/","tags":null,"title":"IEEE SaTML"},{"categories":null,"contents":" Opening Remarks Nicolas Papernot, Carmela Troncoso\nKeynotes Watermarking (The State of the Union) Somesh Jha\nAbstract. Generative AI (GenAI) Turing test (did the content originate from a model, such as DALLE, Gemini, Claude,...?) is an important primitive for several downstream tasks, such as detecting fake media. Several legislations are also mandating that companies should watermark the content generated by their models. Several watermarking schemes exist in the literature. We will discuss a few of them. However, some very powerful attacks exist on the watermarking schemes. We also cover some of these attacks. We will then ponder the following question: what use cases are appropriate for watermarking? We will conclude with some future directions. Audits and Accountability in the Age of 'Artificial Intelligence' Deborah Raji\nAbstract. When AI systems fall short of articulated expectations, people get hurt. In order to hold those who build AI systems accountable for the consequences of their actions, we need to operationalize a system for auditing. AI audits have for years been part of the conversation in the context of online platforms but are now just beginning to emerge as a mode of external oversight and evaluation regarding the deployment of a broader range of \"automated decision systems\" (ADS) and other AI-branded products, including the latest crop of \"generative AI\" models. As AI auditing makes its way into critical policy proposals as a primary mechanism for algorithmic accountability, we must think critically about the necessary technical and institutional infrastructure required for this form of oversight to be successful. Tutorials Detecting the use of copyright-protected content by LLMs Yves-Alexandre de Montjoye\nAbstract. A year ago, ChatGPT surprised the world with its extraordinary language generation capabilities, quickly becoming one of the fastest adopted consumer products in history. Concerns were however quickly raised regarding the reliance of LLMs, developed by OpenAI or its competitors, on high-quality content that is often protected by copyright. In this talk, I will briefly describe the current legal landscape. I will then discuss the different technical approaches that have been proposed to detect the use of copyright-protected content by LLMs. I will first briefly discuss data provenance techniques, a set of tools to efficiently search very large datasets. I will then discuss techniques to audit trained models, inspired by previous work in the privacy literature: post-hoc document-level membership inference attacks, which relies on naturally occurring memorization, and copyright traps, the injection of unique and highly memorizable sequences in original content. (Formal) Languages Help AI agents Learn and Reason Sheila McIlraith\nAbstract. How do we communicate with AI Agents that learn? One obvious answer is via language. Indeed, humans have evolved languages over tens of thousands of years to provide useful abstractions for understanding and interacting with each other and with the physical world. The claim advanced by some is that language influences how we think, what we perceive, how we focus our attention, and what we remember. We use language to capture and share our understanding of the world around us, to communicate high-level goals, intentions and objectives, and to support coordination with others. Importantly, language can provide us with useful and purposeful abstractions that can help us to generalize and transfer knowledge to new situations. Language comes in many forms. In Computer Science and in the study of AI, we have historically used formal knowledge representation languages and programming languages to capture our understanding of the world and to communicate unambiguously with computers. In this talk I will discuss how formal language can help agents learn and reason with a deep dive on one particular topic – reinforcement learning. I’ll show how we can exploit the syntax and semantics of formal language and automata to aid in the specification of complex reward-worthy behavior. In doing so, formal language can help us address some of the challenges to reinforcement learning in settings where trustworthiness is important - such as settings where properties like safety or fairness need to be enforced. Session A Probabilistic Dataset Reconstruction from Interpretable ModelsOpenReview Julien Ferry (École Polytechnique de Montréal, Université de Montréal), Ulrich Aïvodji (École de technologie supérieure, Université du Québec), Sébastien Gambs (Université du Québec à Montréal), Marie-José Huguet (LAAS / CNRS), Mohamed Siala (LAAS / CNRS)\nAbstract. Interpretability is often pointed out as a key requirement for trustworthy machine learning. However, learning and releasing models that are inherently interpretable leaks information regarding the underlying training data. As such disclosure may directly conflict with privacy, a precise quantification of the privacy impact of such breach is a fundamental problem. For instance, previous work have shown that the structure of a decision tree can be leveraged to build a probabilistic reconstruction of its training dataset, with the uncertainty of the reconstruction being a relevant metric for the information leak. In this paper, we propose of a novel framework generalizing these probabilistic reconstructions in the sense that it can handle other forms of interpretable models and more generic types of knowledge. In addition, we demonstrate that under realistic assumptions regarding the interpretable models' structure, the uncertainty of the reconstruction can be computed efficiently. Finally, we illustrate the applicability of our approach on both decision trees and rule lists, by comparing the theoretical information leak associated to either exact or heuristic learning algorithms. Our results suggest that optimal interpretable models are often more compact and leak less information regarding their training data than greedily-built ones, for a given accuracy level. Shake to Leak: Amplifying the Generative Privacy Risk through Fine-tuningOpenReview Zhangheng LI (University of Texas at Austin), Junyuan Hong (University of Texas at Austin), Bo Li (University of Illinois, Urbana Champaign), Zhangyang Wang (University of Texas at Austin)\nAbstract. While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arose -- published models or APIs could generate the training images and thus leak private sensitive training information. In this paper, we reveal a new risk, \\textbf{Shake-to-Leak} (S2L), that fine-tuning the pre-trained models on manipulated data can amplify the existing privacy risk risks. We demonstrate that S2L could happen in various off-the-shelf fine-tuning strategies for diffusion models, including embedding-based ones (DreamBooth and Textual Inversion), and parameter-efficient methods (LoRA and Hypernetwork) and their combinations. S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by up to 5% (absolute difference) attacking AUC and also achieve significant results in data extraction. This discovery underscores that the privacy risk problem with diffusion models may be even more severe than previously recognized. The code will be released. Improved Differentially Private Regression via Gradient BoostingOpenReview Shuai Tang (Amazon Web Services), Sergul Aydore (Amazon), Michael Kearns (University of Pennsylvania), Saeyoung Rho (Columbia University), Aaron Roth (Amazon), Yichen Wang (Amazon), Yu-Xiang Wang (UC Santa Barbara), Steven Wu (Carnegie Mellon University)\nAbstract. We revisit the problem of differentially private squared error linear regression. We observe that existing state-of-the-art methods are sensitive to the choice of hyperparameters --- including the ``clipping threshold'' that cannot be set optimally in a data-independent way. We give a new algorithm for private linear regression based on gradient boosting. We show that our method consistently improves over the previous state of the art when the clipping threshold is taken to be fixed without knowledge of the data, rather than optimized in a non-private way --- and that even when we optimize the hyperparameters of competitor algorithms non-privately, our algorithm is no worse and often better. Additional experiments also that our algorithm is also more robust to outliers. SoK: A Review of Differentially Private Linear Models For High Dimensional DataOpenReview Amol Khanna (Booz Allen Hamilton), Edward Raff (Booz Allen Hamilton), Nathan Inkawhich (Air Force Research Laboratory)\nAbstract. Linear models are ubiquitous in data science, but are particularly prone to overfitting and data memorization in high dimensions. To combat memorization, differential privacy can be used. Many papers have proposed optimization techniques for high dimensional differentially private linear models, but a systematic comparison between these methods does not exist. We close this gap by providing a comprehensive review of optimization methods for private high dimensional linear models. Empirical tests on all methods demonstrate surprising results which can inform future research. Code for implementing all methods is released in the supplementary material. Concentrated Differential Privacy for BanditsOpenReview Achraf Azize (INRIA), Debabrota Basu (INRIA)\nAbstract. Bandits serve as the theoretical foundation of sequential learning and an algorithmic foundation of modern recommender systems. However, recommender systems often rely on user-sensitive data, making privacy a critical concern. This paper contributes to the understanding of Differential Privacy (DP) in bandits with a trusted centralised decision-maker, and especially the implications of ensuring *zero Concentrated Differential Privacy* (zCDP). First, we formalise and compare different adaptations of DP to bandits, depending on the considered input and the interaction protocol. Then, we propose three private algorithms, namely AdaC-UCB, AdaC-GOPE and AdaC-OFUL, for three bandit settings, namely finite-armed bandits, linear bandits, and linear contextual bandits. The three algorithms share a generic algorithmic blueprint, i.e. the Gaussian mechanism and adaptive episodes, to ensure a good privacy-utility trade-off. We analyse and upper bound the **regret** of these three algorithms. Our analysis shows that in all of these settings, the prices of imposing zCDP are (asymptotically) negligible in comparison with the regrets incurred oblivious to privacy. Next, we complement our regret upper bounds with the first **minimax lower bounds** on the regret of bandits with zCDP. To prove the lower bounds, we elaborate a new proof technique based on couplings and optimal transport. We conclude by experimentally validating our theoretical results for the three different settings of bandits. PILLAR: How to make semi-private learning more effectiveOpenReview Yaxi Hu (Max Planck Institute for Intelligent Systems), Francesco Pinto (University of Oxford), Fanny Yang (Swiss Federal Institute of Technology), Amartya Sanyal (Max-Planck Institute)\nAbstract. In Semi-Supervised Semi-Private (SP) learning, the learner has access to both public unlabelled and private labelled data. We propose PILLAR, an easy-to-implement and computationally efficient algorithm that, under mild assumptions on the data, provably achieves significantly lower private labelled sample complexity and can be efficiently run on real-world datasets. The key idea is to use public data to estimate the principal components of the pre-trained features and subsequently project the private dataset onto the top-$k$ Principal Components. We empirically validate the effectiveness of our algorithm in a wide variety of experiments under tight privacy constraints $\\epsilon \u003c 1$ and probe its effectiveness in low-data regimes and when the pre-training distribution significantly differs from the one on which SP learning is performed. Despite its simplicity, our algorithm exhibits significantly improved performance, in all of these setting, over all available baselines that use similar amounts of public data while often being more computationally expensive [1]-[3]. For example, in the case of CIFAR-100 for $\\epsilon=0.1$, our algorithm improves over the most competitive baselines by a factor of at least two. Session B Fair Federated Learning via Bounded Group LossOpenReview Shengyuan Hu (Carnegie Mellon University), Steven Wu (Carnegie Mellon University), Virginia Smith (Carnegie Mellon University)\nAbstract. Fair prediction across protected groups is an important consideration in federated learning applications. In this work we propose a general framework for provably fair federated learning. In particular, we explore and extend the notion of Bounded Group Loss as a theoretically-grounded approach for group fairness that offers favorable trade-offs between fairness and utility relative to prior work. Using this setup, we propose a scalable federated optimization method that optimizes the empirical risk under a number of group fairness constraints. We provide convergence guarantees for the method as well as fairness guarantees for the resulting solution. Empirically, we evaluate our method across common benchmarks from fair ML and federated learning, showing that it can provide both fairer and more accurate predictions than existing approaches in fair federated learning. Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected FeaturesOpenReview Hadi Elzayn (Stanford University), Emily Black (Barnard College), Patrick Vossler (Stanford University), Nathanael Jo (Massachusetts Institute of Technology), JACOB GOLDIN (University of Chicago), Daniel E. Ho (Stanford University)\nAbstract. The vast majority of techniques to train fair models require access to the protected attribute (e.g., race, gender), either at train time or in production. However, in many practically important applications this protected attribute is largely unavailable. Still, AI systems used in sensitive business and government applications---such as housing ad delivery and credit underwriting---are increasingly legally required to measure and mitigate their bias. In this paper, we develop methods for measuring and reducing fairness violations in a setting with limited access to protected attribute labels. Specifically, we assume access to protected attribute labels on a small subset of the dataset of interest, but only probabilistic estimates of protected attribute labels (e.g., via Bayesian Improved Surname Geocoding) for the rest of the dataset. With this setting in mind, we propose a method to estimate bounds on common fairness metrics for an existing model, as well as a method for training a model to limit fairness violations by solving a constrained non-convex optimization problem. Unlike similar existing approaches, our methods take advantage of contextual information -- specifically, the relationships between a model's predictions and the probabilistic prediction of protected attributes, given the true protected attribute, and vice versa -- to provide tighter bounds on the true disparity. We provide an empirical illustration of our methods using voting data as well as the COMPAS dataset. First, we show our measurement method can bound the true disparity up to 5.5x tighter than previous methods in these applications. Then, we demonstrate that our training technique effectively reduces disparity in comparison to an unconstrained model while often incurring lesser fairness-accuracy trade-offs than other fair optimization methods with limited access to protected attributes. Session C Evaluating Superhuman Models with Consistency ChecksOpenReview Lukas Fluri (ETHZ - ETH Zurich), Daniel Paleka (Department of Computer Science, ETHZ - ETH Zurich), Florian Tramèr (ETHZ - ETH Zurich)\nAbstract. If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. As case studies, we instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to semantically identical boards; GPT-4 forecasting that sports records will evolve non-monotonically over time; or an AI judge assigning bail to a defendant only after we add a felony to their criminal record. Certifiably Robust Reinforcement Learning through Model-Based Abstract InterpretationOpenReview Chenxi Yang (University of Texas, Austin), Greg Anderson (Reed College), Swarat Chaudhuri (University of Texas at Austin)\nAbstract. We present a reinforcement learning (RL) framework in which the learned policy comes with a machine-checkable certificate of provable adversarial robustness. Our approach, called CAROL, learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide learning, and the abstract interpretation used to construct it directly leads to the robustness certificate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of CAROL. We also experimentally evaluate CAROL on four MuJoCo environments with continuous state and action spaces. On these tasks, CAROL learns policies that, when contrasted with policies from two state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified performance lower bounds; and (ii) comparable performance under empirical adversarial attacks. Fast Certification of Vision-Language Models Using Incremental Randomized SmoothingOpenReview Ashutosh Kumar Nirala (Iowa State University), Ameya Joshi (InstaDeep), Soumik Sarkar (Iowa State University), Chinmay Hegde (New York University)\nAbstract. A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild. In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base ``training'' set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets. Session D Backdoor Attack on Un-paired Medical Image-Text Pretrained Models: A Pilot Study on MedCLIPOpenReview Ruinan Jin (University of British Columbia), Chun-Yin Huang (University of British Columbia), Chenyu You (Yale University), Xiaoxiao Li (University of British Columbia)\nAbstract. In recent years, foundation models (FMs) have solidified their role as cornerstone advancements in the deep learning domain. By extracting intricate patterns from vast datasets, these models consistently achieve state-of-the-art results across a spectrum of downstream tasks, all without necessitating extensive computational resources. Notably, MedCLIP, a vision-text contrastive learning-based medical FM, has been designed using unpaired image-text training. While the medical domain has often adopted unpaired training to amplify data, the exploration of potential security concerns linked to this approach hasn't kept pace with its practical usage. Notably, the augmentation capabilities inherent in unpaired training also indicate that minor label discrepancies can result in significant model deviations. In this study, we frame this label discrepancy as a backdoor attack problem. We further analyze its impact on medical FMs throughout the FM supply chain. Our evaluation primarily revolves around MedCLIP, emblematic of medical FM employing the unpaired strategy. We begin with an exploration of vulnerabilities in MedCLIP stemming from unpaired image-text matching, termed BadMatch. BadMatch is achieved using a modest set of wrongly labeled data. Subsequently, we disrupt MedCLIP's contrastive learning through BadDist-assisted BadMatch by introducing a Bad-Distance between the embeddings of clean and poisoned data. Intriguingly, when BadMatch and BadDist are combined, a slight 0.05 percent of misaligned image-text data can yield a staggering 99 percent attack success rate, all the while maintaining MedCLIP's efficacy on untainted data. Additionally, combined with BadMatch and BadDist, the attacking pipeline consistently fends off backdoor assaults across diverse model designs, datasets, and triggers. Also, our findings reveal that current defense strategies are insufficient in detecting these latent threats in medical FMs' supply chains. REStore: Black-Box Defense against DNN Backdoors with Rare Event SimulationOpenReview Quentin Le Roux (INRIA), Kassem Kallas (INRIA), Teddy Furon (INRIA)\nAbstract. Backdoor attacks pose a significant threat to deep neural networks, as they allow an adversary to inject a malicious behavior in a victim model during training. This paper addresses the challenge of defending against backdoor attacks in a black-box setting where the defender has a limited access to a suspicious model, only observing its inputs and outputs. This scenario is particularly relevant in the context of the Machine Learning as a Service economy. We introduce Importance Splitting, a Sequential Monte-Carlo method previously used in neural network robustness certification, as an off-the-shelf tool for defending against backdoors. We demonstrate that a black-box defender can leverage the outputs of Importance Splitting to assess the presence of a backdoor, reconstruct its trigger, and finally purify test-time input data in real-time. So-called REStore, our input purification defense proves effective in black-box scenarios because it uses triggers recovered with a query access to a model, only observing its outputs (whether logits, probits, or top-1 labels). We test our method on MNIST, CIFAR-10, and CASIA-Webface. To the best of our knowledge, REStore is the first one-stage, black-box input purification defense that matches previous, more complex comparable methods. REStore avoids gradient estimation, model reconstruction, or the vulnerable training of supplemental neural networks and anomaly detectors. EdgePruner: Poisoned Edge Pruning in Graph Contrastive LearningOpenReview Hiroya Kato (KDDI Research, Inc.), Kento Hasegawa (KDDI Research, Inc.), Seira Hidano (KDDI Research, Inc.), Kazuhide Fukushima (KDDI Research, Inc.)\nAbstract. Graph Contrastive Learning (GCL) is unsupervised graph representation learning that can obtain useful representation of unknown nodes. The node representation can be utilized as features of downstream tasks. However, GCL is vulnerable to poisoning attacks as with existing learning models. A state-of-the-art defense cannot sufficiently negate adverse effects by poisoned graphs although such a defense introduces adversarial training in the GCL. To achieve further improvement, pruning adversarial edges is important. To the best of our knowledge, the feasibility remains unexplored in the GCL domain. In this paper, we propose a simple defense for GCL, EdgePruner. We focus on the fact that the state-of-the-art poisoning attack on GCL tends to mainly add adversarial edges to create poisoned graphs, which means that pruning edges is important to sanitize the graphs. Thus, EdgePruner prunes edges that contribute to minimizing the contrastive loss based on the node representation obtained after training on poisoned graphs by GCL. Furthermore, we focus on the fact that nodes with distinct features are connected by adversarial edges in poisoned graphs. Thus, we introduce feature similarity between neighboring nodes to help more appropriately determine adversarial edges. This similarity is helpful in further eliminating adverse effects from poisoned graphs on various datasets. Finally, EdgePruner outputs a graph that yields the minimum contrastive loss as the sanitized graph. Our results demonstrate that pruning adversarial edges is feasible on six datasets. EdgePruner can improve the accuracy of node classification under the attack by up to 5.55% compared with that of the state-of-the-art defense. Moreover, we show that EdgePruner is immune to an adaptive attack. Indiscriminate Data Poisoning Attacks on Pre-trained Feature ExtractorsOpenReview Yiwei Lu (University of Waterloo), Matthew Y. R. Yang (University of Waterloo), Gautam Kamath (University of Waterloo), Yaoliang Yu (University of Waterloo)\nAbstract. Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning (SSL) methods (e.g., contrastive learning) that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-trained feature extractors. Specifically, we propose two types of attacks: (1) the input space attacks, where we modify existing attacks (e.g., TGDA attack and GC attack) to directly craft poisoned data in the input space. However, due to the difficulty of optimization under constraints, we further propose (2) the feature targeted attacks, where we mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space. Our experiments examine such attacks in popular downstream tasks of fine-tuning on the same dataset and transfer learning that considers domain adaptation. Empirical results reveal that transfer learning is more vulnerable to our attacks. Additionally, input space attacks are a strong threat if no countermeasures are posed, but are otherwise weaker than feature targeted attacks. ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networksOpenReview Eleanor Clifford (Imperial College London), Ilia Shumailov (Google DeepMind), Yiren Zhao (Imperial College London), Ross Anderson (University of Edinburgh, University of Edinburgh), Robert D. Mullins (University of Cambridge)\nAbstract. Early backdoor attacks against machine learning set off an arms race in attack and defence development. Defences have since appeared demonstrating some ability to detect backdoors in models or even remove them. These defences work by inspecting the training data, the model, or the integrity of the training procedure. In this work, we show that backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages. The attacker can not only insert existing weight-based backdoors during compilation, but also a new class of weight-independent backdoors, such as ImpNet. These backdoors are impossible to detect during the training or data preparation processes, because they are not yet present. Next, we demonstrate that some backdoors, including ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge. We conclude that machine learning model security requires assurance of provenance along the entire technical pipeline, including the data, model architecture, compiler, and hardware specification. The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion ModelsOpenReview Hadi Mohaghegh Dolatabadi (University of Melbourne), Sarah Monazam Erfani (The University of Melbourne), Christopher Leckie (The University of Melbourne)\nAbstract. Protecting personal data against exploitation of machine learning models is crucial. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data unexploitable.'' This paper provides a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can counteract the effectiveness of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training even under distribution mismatch between the diffusion model and the protected data. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over. Our implementation is available at this repository: https://github.com/hmdolatabadi/AVATAR. Session E SoK: Pitfalls in Evaluating Black-Box AttacksOpenReview Fnu Suya (University of Maryland, College Park), Anshuman Suri (University of Virginia), Tingwei Zhang (Cornell University), Jingtao Hong (Columbia University), Yuan Tian (UCLA), David Evans (University of Virginia)\nAbstract. Numerous works study black-box attacks on image classifiers, where adversaries generate adversarial examples against unknown target models without having access to their internal information. However, these works make different assumptions about the adversary's knowledge, and current literature lacks cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector but show how it still falls short of the more restrictive setting that only obtains the prediction label, highlighting the need for more research. 2) Identifying the threat models for different attacks uncovers stronger baselines that challenge prior state-of-the-art claims. We demonstrate this by enhancing an initially weaker baseline (under interactive query access) via surrogate models, effectively overturning claims in the respective paper. 3) Our taxonomy reveals interactions between attacker knowledge that connect well to related areas, such as model inversion and extraction attacks. We discuss how advances in other areas can enable stronger black-box attacks. Finally, we emphasize the need for a more realistic assessment of attack success by factoring in local attack runtime. This approach reveals the potential for certain attacks to achieve notably higher success rates. We also highlight the need to evaluate attacks in diverse and harder settings and underscore the need for better selection criteria when picking the best candidate adversarial examples. Evading Black-box Classifiers Without Breaking EggsOpenReview Edoardo Debenedetti (Department of Computer Science, ETHZ - ETH Zurich), Nicholas Carlini (Google), Florian Tramèr (ETHZ - ETH Zurich)\nAbstract. Decision-based evasion attacks repeatedly query a black-box classifier to generate adversarial examples. Prior work measures the cost of such attacks by the total number of queries made to the classifier. We argue this metric is incomplete. Many security-critical machine learning systems aim to weed out bad\" data (e.g., malware, harmful content, etc). Queries to such systems carry a fundamentally *asymmetric cost*: \"flagged\" queries, i.e., detected as \"bad\", come at a higher cost because they trigger additional security filters, e.g., usage throttling or account suspension. Yet, we find that existing decision-based attacks issue a large number queries that would get flagged by a real-world system, which likely renders them ineffective against security-critical systems. We then design new attacks that reduce the number of flagged queries by $1.5$-$7.3\\times$, but often at a significant increase in total (non-flagged) queries. We thus pose it as an open problem to build black-box attacks that are more effective under realistic cost metrics.\" Segment (Almost) Nothing: Prompt-Agnostic Adversarial Attacks on Segmentation ModelsOpenReview Francesco Croce (EPFL - EPF Lausanne), Matthias Hein (University of Tübingen)\nAbstract. General purpose segmentation models are able to generate (semantic) segmentation masks from a variety of prompts, including visual (points, boxed, etc.) and textual (object names) ones. In particular, input images are pre-processed by an image encoder to obtain embedding vectors which are later used for mask predictions. Existing adversarial attacks target the end-to-end tasks, i.e. aim at altering the segmentation mask predicted for a specific image-prompt pair. However, this requires running an individual attack for each new prompt for the same image. We propose instead to generate prompt-agnostic adversarial attacks by maximizing the $\\ell_2$-distance, in the latent space, between the embedding of the original and perturbed images. Since the encoding process only depends on the image, distorted image representations will cause perturbations in the segmentation masks for a variety of prompts. We show that even imperceptible $\\ell_\\infty$-bounded perturbations of radius $\\epsilon=1/255$ are often sufficient to drastically modify the masks predicted with point, box and text prompts by recently proposed foundation models for segmentation. Moreover, we explore the possibility of creating universal, i.e. non image-specific, attacks which can be readily applied to any input without further computational cost. Session F Improving Privacy-Preserving Vertical Federated Learning by Efficient Communication with ADMMOpenReview Chulin Xie (University of Illinois, Urbana Champaign), Pin-Yu Chen (International Business Machines), Qinbin Li (University of California, Berkeley), Arash Nourian (UC Berkeley, University of California, Berkeley), Ce Zhang (University of Chicago), Bo Li (University of Illinois, Urbana Champaign)\nAbstract. Federated learning (FL) enables distributed resource-constrained devices to jointly train shared models while keeping the training data local for privacy purposes. Vertical FL (VFL), which allows each client to collect partial features, has attracted intensive research efforts recently. We identified the main challenges that existing VFL frameworks are facing: the server needs to communicate gradients with the clients for each training step, incurring high communication cost that leads to rapid consumption of privacy budgets. To address these challenges, in this paper, we introduce a VFL framework with multiple heads (VIM), which takes the separate contribution of each client into account, and enables an efficient decomposition of the VFL optimization objective to sub-objectives that can be iteratively tackled by the server and the clients on their own. In particular, we propose an Alternating Direction Method of Multipliers (ADMM)-based method to solve our optimization problem, which allows clients to conduct multiple local updates before communication, and thus reduces the communication cost and leads to better performance under differential privacy (DP). We provide the client-level DP mechanism for our framework to protect user privacy. Moreover, we show that a byproduct of VIM is that the weights of learned heads reflect the importance of local clients. We conduct extensive evaluations and show that on four VFL datasets, VIM achieves significantly higher performance and faster convergence compared with the state-of-the-art. We also explicitly evaluate the importance of local clients and show that VIM enables functionalities such as client-level explanation and client denoising. We hope this work will shed light on a new way of effective VFL training and understanding. Differentially Private Multi-Site Treatment Effect EstimationOpenReview Tatsuki Koga (University of California, San Diego), Kamalika Chaudhuri (UC San Diego, University of California, San Diego), David Page (Duke University)\nAbstract. Patient privacy is a major barrier to healthcare AI. For confidentiality reasons, most patient data remains in silo in separate hospitals, preventing the design of data-driven healthcare AI systems that need large volumes of patient data to make effective decisions. A solution to this is collective learning across multiple sites through federated learning with differential privacy. However, literature in this space typically focuses on differentially private statistical estimation and machine learning, which is different from the causal inference-related problems that arise in healthcare. In this work, we take a fresh look at federated learning with a focus on causal inference; specifically, we look at estimating the average treatment effect (ATE), an important task in causal inference for healthcare applications, and provide a federated analytics approach to enable ATE estimation across multiple sites along with differential privacy (DP) guarantees at each site. The main challenge comes from site heterogeneity—different sites have different sample sizes and privacy budgets. We address this through a class of per-site estimation algorithms that reports the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate. Our experiments on real and synthetic data show that our method reliably aggregates private statistics across sites and provides better privacy-utility tradeoff under site heterogeneity than baselines. ScionFL: Efficient and Robust Secure Quantized AggregationOpenReview Yaniv Ben-Itzhak (VMware), Helen Möllering (Technical University of Darmstadt), Benny Pinkas (Bar-Ilan University), Thomas Schneider (Technische Universität Darmstadt), Ajith Suresh (Technology Innovation Institute (TII)), Oleksandr Tkachenko (Technische Universität Darmstadt), shay vargaftik (VMware Research), Christian Weinert (Royal Holloway, University of London), Hossein Yalame (TU Darmstadt), Avishay Yanai (Vmware)\nAbstract. Secure aggregation is commonly used in federated learning (FL) to alleviate privacy concerns related to the central aggregator seeing all parameter updates in the clear. Unfortunately, most existing secure aggregation schemes ignore two critical orthogonal research directions that aim to (i) significantly reduce client-server communication and (ii) mitigate the impact of malicious clients. However, both of these additional properties are essential to facilitate cross-device FL with thousands or even millions of (mobile) participants. In this paper, we unite both research directions by introducing ScionFL, the first secure aggregation framework for FL that operates efficiently on quantized inputs and simultaneously provides robustness against malicious clients. Our framework leverages novel multi-party computation MPC techniques and supports multiple linear (1-bit) quantization schemes, including ones that utilize the randomized Hadamard transform and Kashin's representation. Our theoretical results are supported by extensive evaluations. We show that with no overhead for clients and moderate overhead for the server compared to transferring and processing quantized updates in plaintext, we obtain comparable accuracy for standard FL benchmarks. Moreover, we demonstrate the robustness of our framework against state-of-the-art poisoning attacks. Differentially Private Heavy Hitter Detection using Federated AnalyticsOpenReview Karan Chadha (Stanford University), Hanieh Hashemi (Apple), John Duchi (Stanford University), Vitaly Feldman (Apple AI Research), Hanieh Hashemi (Apple), Omid Javidbakht (Apple), Audra McMillan (Apple), Kunal Talwar (Apple)\nAbstract. In this work, we study practical heuristics to improve the performance of prefix-tree based algorithms for differentially private heavy hitter detection. Our model assumes each user has multiple data points and the goal is to learn as many of the most frequent data points as possible across all users' data with aggregate and local differential privacy. We propose an adaptive hyperparameter tuning algorithm that improves the performance of the algorithm while satisfying computational, communication and privacy constraints. We explore the impact of different data-selection schemes as well as the impact of introducing deny lists during multiple runs of the algorithm. We test these improvements using extensive experimentation on the Reddit dataset on the task of learning the most frequent words. OLYMPIA: A Simulation Framework for Evaluating the Concrete Scalability of Secure Aggregation ProtocolsOpenReview Ivoline Ngong (University of Vermont), Nicholas Gibson (University of Vermont), Joseph Near (University of Vermont)\nAbstract. Recent secure aggregation protocols enable privacy-preserving federated learning for high-dimensional models among thousands or even millions of participants. Due to the scale of these use cases, however, end-to-end empirical evaluation of these protocols is impossible. We present OLYMPIA, a framework for empirical evaluation of secure protocols via simulation. OLYMPIA provides an embedded domain-specific language for defining protocols and a simulation framework for evaluating their performance. We implement several recent secure aggregation protocols using OLYMPIA and perform the first empirical comparison of their end-to-end running times. We release OLYMPIA as open open-source. Session G Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image EncodersOpenReview Andrew Geng (University of Wisconsin, Madison), Pin-Yu Chen (International Business Machines)\nAbstract. When evaluating the performance of a pre-trained model transferred to a downstream task, it is imperative to assess not only the in-distribution (ID) accuracy of the downstream model but also its capacity to generalize and identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden costs associated with intrusive fine-tuning techniques. Specifically, we demonstrate that commonly used fine-tuning methods not only distort the representations necessary for generalizing to covariate-shifted OOD samples (OOD generalization) but also distort the representations necessary for detecting semantically-shifted OOD samples (OOD detection). To address these challenges, we introduce a new model reprogramming approach for fine-tuning, which we name Reprogrammer. Reprogrammer aims to improve the holistic performance of the downstream model across ID, OOD generalization, and OOD detection tasks. Our empirical evidence reveals that Reprogrammer is less intrusive and yields superior downstream models. Furthermore, we demonstrate that by appending an additional residual connection to Reprogrammer, we can further preserve pre-training representations, resulting in an even more safe and robust downstream model capable of excelling in many ID classification, OOD generalization, and OOD detection settings. Data Redaction from Conditional Generative ModelsOpenReview Zhifeng Kong (NVIDIA), Kamalika Chaudhuri (UC San Diego, University of California, San Diego)\nAbstract. Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality. Towards Scalable and Robust Model VersioningOpenReview Wenxin Ding (University of Chicago), Arjun Nitin Bhagoji (University of Chicago), Ben Y. Zhao (University of Chicago), Haitao Zheng (University of Chicago)\nAbstract. As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise. Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks. Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital. In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture. The model owner can deploy one version at a time and replace a leaked version immediately with a new version. The newly deployed model version can resist adversarial attacks generated leveraging white-box access to one or all previously leaked versions. We show theoretically that this can be accomplished by incorporating parameterized *hidden distributions* into the model training data, forcing the model to learn task-irrelevant features uniquely defined by the chosen data. Additionally, optimal choices of hidden distributions can produce a sequence of model versions capable of resisting compound transferability attacks over time. Leveraging our analytical insights, we design and implement a practical model versioning method for DNN classifiers, which leads to significant robustness improvements over existing methods. We believe our work presents a promising direction for safeguarding DNN services beyond their initial deployment. Session H SoK: AI Auditing: The Broken Bus on the Road to AI AccountabilityOpenReview Abeba Birhane (Trinity College Dublin), Ryan Steed (Carnegie Mellon University), Victor Ojewale (Brown University), Briana Vecchione (Cornell University), Inioluwa Deborah Raji (Mozilla Foundation)\nAbstract. One of the most concrete measures to take towards meaningful AI accountability is to consequentially assess and report the systems’ performance and impact. However, the practical nature of the “AI audit” ecosystem is muddled and imprecise, difficult to work through various concepts, and map out the stakeholders involved in the practice. First, we taxonomize current AI audit practices as completed by regulators, law firms, civil society, journalism, academia, and consulting agencies. Next, we assess the impact of audits done by stakeholders within each domain. We find that only a subset of AI audit studies translate to the desired accountability outcomes. We thus assess and isolate practices necessary for effective AI audit results, articulating the observed connections between AI audit design, methodology and institutional context on its effectiveness as a meaningful mechanism for accountability. Under manipulations, are there AI models harder to audit?OpenReview Augustin Godinot (Université Rennes I), Gilles Tredan (LAAS / CNRS), Erwan Le Merrer (INRIA), Camilla Penzo (PEReN - French Center of Expertise for Digital Platform Regulation), Francois Taiani (INRIA Rennes)\nAbstract. Auditors need robust methods to assess the compliance of web platforms to the law. However, since they hardly ever have access to the algorithm, implementation or training data used by a platform, the problem is harder than a simple metric estimation. Within the recent framework of manipulation-proofness auditing, we study in this paper the feasibility of robust audits in realistic settings, in which models exhibit large capacities. We first prove a constraining result: if a web platform uses models that may fit any data, no audit strategy—whether active or not—can outperform random sampling when estimating properties such as demographic parity, under minimal assumptions regarding the balance of sensitive attributes. To better understand the conditions under which state-of-the-art audit techniques may still remain competitive, we then relate the difficulty of audits to the capacity of the targeted models, using the Rademacher complexity. We empirically validate these results on popular models of increasing capacities, thus confirming experimentally that large-capacity models, which are commonly used in practice, are particularly hard to audit. These results refine the limits of the auditing problem, and opens up enticing questions on the connection between model capacity and the ability of platforms to manipulate audit attempts. Session I SoK: Unifying Corroborative and Contributive Attributions in Large Language ModelsOpenReview Theodora Worledge (Computer Science Department, Stanford University), Judy Hanwen Shen (Stanford University), Nicole Meister (Stanford University), Caleb Winston (Computer Science Department, Stanford University), Carlos Guestrin (Stanford University)\nAbstract. As businesses, products, and services spring up around large language models, the trustworthiness of these models hinges on the verifiability of their outputs. However, methods for explaining language model outputs fall across two distinct fields of study which both use the term attribution\" to refer to entirely separate techniques: citation generation and training data attribution. In many modern applications, such as legal document generation and medical question answering, both types of attributions are important. In this systematization of knowledge paper, we argue for and present a unified framework of large language model attributions. We show how existing methods of different types of attribution fall under the unified framework. We also use the framework to discuss real-world use cases where one or both types of attributions are required. We believe that this unified framework will guide the use case driven development of systems that leverage both types of attribution, as well as the standardization of their evaluation.\" CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language ModelsOpenReview Hossein Hajipour (CISPA, saarland university, saarland informatics campus), Keno Hassler (CISPA Helmholtz Center for Information Security), Thorsten Holz (CISPA Helmholtz Center for Information Security), Lea Schönherr (CISPA Helmholtz Center for Information Security), Mario Fritz (CISPA Helmholtz Center for Information Security)\nAbstract. Large language models (LLMs) for automatic code generation have recently achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. Training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively evaluated for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models. In this work, we propose a method to systematically study the security issues of code language models to assess their susceptibility to generating vulnerable code. To this end, we introduce the first approach to automatically find generated code that contains vulnerabilities in black-box code generation models. This involves proposing a novel few-shot prompting approach. We evaluate the effectiveness of our approach by examining code language models in generating high-risk security weaknesses. Furthermore, we use our method to create a collection of diverse non-secure prompts for various vulnerability scenarios. This dataset serves as a benchmark to evaluate and compare the security weaknesses of code language models. Navigating the Structured What-If Spaces: Counterfactual Generation via Structured DiffusionOpenReview Nishtha Madaan (Indian Institute of Technology Delhi), Srikanta J. Bedathur (Indian Institute of Technology, Delhi)\nAbstract. Generating counterfactual explanations is one of the most effective approaches for uncovering the inner workings of black-box neural network models and building user trust. While remarkable strides have been made in generative modelling using diffusion models in domains like vision, their utility in generating counterfactual explanations in structured modalities remains unexplored. In this paper, we introduce Structured Counterfactual Diffuser or SCD, the first plug-and-play framework leveraging diffusion for generating counterfactual explanations in structured data. SCD learns the underlying data distribution via a diffusion model which is then guided at test time to generate counterfactuals for any arbitrary black-box model, input, and desired prediction. Our experiments show that our counterfactuals not only exhibit high plausibility compared to the existing state-of-the-art but also show significantly better proximity and diversity. Understanding, Uncovering, and Mitigating the Causes of Inference Slowdown for Language ModelsOpenReview Kamala Varma (University of Maryland, College Park), Arda Numanoğlu (Middle East Technical University), Yigitcan Kaya (University of California, Santa Barbara), Tudor Dumitras (University of Maryland, College Park)\nAbstract. Dynamic neural networks (DyNNs) have shown promise for alleviating the high computational costs of pre-trained language models (PLMs), such as BERT and GPT. Emerging slowdown attacks have shown to inhibit the ability of DyNNs to omit computation, e.g., by skipping layers that are deemed unnecessary. As a result, these attacks can cause significant delays in inference speed for DyNNs and may erase their cost savings altogether. Most research in slowdown attacks has been in the image domain, despite the ever-growing computational costs---and relevance of DyNNs---in the language domain. Unfortunately, it is still not understood what language artifacts trigger extra processing in a PLM, or what causes this behavior. We aim to fill this gap through an empirical exploration of the slowdown effect on language models. Specifically, we uncover a crucial difference between the slowdown effect in the image and language domains, illuminate the efficacy of pre-existing and novel techniques for causing slowdown, and report circumstances where slowdown does not occur. Building on these observations, we propose the first approach for mitigating the slowdown effect. Our results suggest that slowdown attacks can provide new insights that can inform the development of more efficient PLMs. Competitions Find the Trojan: Universal Backdoor Detection in Aligned Large Language ModelsWebsite organized by Javier Rando \u0026 Stephen Casper \u0026 Florian Tramèr.\nCNN Interpretability CompetitionWebsite organized by Stephen Casper \u0026 Dylan Hadfield-Menell.\nLarge Language Models Capture-the-FlagWebsite organized by Sahar Abdelnabi \u0026 Nicholas Carlini \u0026 Edoardo Debenedetti \u0026 Mario Fritz \u0026 Kai Greshake \u0026 Richard Hadzic \u0026 Thorsten Holz \u0026 Daphne Ippolito \u0026 Daniel Paleka \u0026 Javier Rando \u0026 Lea Schönherr \u0026 Florian Tramèr \u0026 Yiming Zhang.\nClosing Remarks Nicolas Papernot, Carmela Troncoso\n","permalink":"https://satml.org/2025/videos/","tags":null,"title":"Video Recordings for SaTML 2024"},{"categories":null,"contents":"The schedule is still subject to change. Each paper accepted to the conference will be presented in the form of:\nA 15 min talk followed by a 5 min Q\u0026amp;A session A poster at the poster session The poster sessions are an opportunity for attendees to continue conversations with the paper authors.\nOn the schedule below: click on each session to obtain more information like the list of corresponding papers or the keynote and tutorial details.\n","permalink":"https://satml.org/2025/schedule/","tags":null,"title":"Schedule"},{"categories":null,"contents":"Competition Track SaTML traditionally includes a Competition Track as part of its program. Participants are invited to engage in selected data science competitions, competing to achieve the highest score on relevant machine learning or security tasks. These tasks are based on well-defined problems and corresponding datasets provided by the competition organizers. The competition results will be presented and discussed during dedicated sessions at the conference (see Call for Competitions for details).\nAccepted Competitions For this year, the following competitions have been accepted for the conference. Interested researchers can participate in any of these by following the instructions provided on the competition websites. For more information or specific inquiries, please contact the respective competition organizers directly.\n🏁 Adaptive Prompt Injection: LLMail Inject Website: https://microsoft.github.io/llmail-inject/\nThis competition invites participants to navigate and evade multiple prompt injection defences within an LLM-integrated email client. As an attacker, your goal is to craft emails with instructions for the LLM to execute your chosen task while avoiding detection. The competition is structured into various scenarios, each reflecting different levels of attacker knowledge and requiring successful email delivery, retrieval, and processing.\nOrganizers: Sahar Abdelnabi, Giovanni Cherubin, Aideen Fay, Andrew Paverd, Mark Russinovich, Ahmed Salem, Egor Zverev, and Javier Rando.\n🏁 Inference Attacks Against Document VQA Website: https://benchmarks.elsa-ai.eu/?ch=2\u0026amp;com=introduction\nThis competition invites the development of inference attacks to extract sensitive information from Document Visual Question Answering models. These models are especially vulnerable to leakage of private data, and so are especially relevant in the contexts of privacy risks and privacy-preserving ML. We provide a competition framework to encourage the implementation of practical methods that expose such privacy vulnerabilities, with the aim of better understanding real-world threat models and the robustness of differential privacy in multimodal models.\nOrganizers: Dimosthenis Karatzas, Andrey Barsky, Mohamed Ali Souibgui, Khanh Nguyen, Raouf Kerkouche, Marlon Tobaben, Kangsoo Jung, Joonas Jälkö, Vincent Poulain, Aurélie Joseph, Ernest Valveny, Josep Lladós, Catuscia Palamidessi, Antti Honkela, and Mario Fritz.\n🏁 Membership Inference on Diffusion-model-based Synthetic Tabular Data Website: https://vectorinstitute.github.io/MIDST\nThe MIDST challenge invites participants to assess the privacy risks of synthetic tabular data generated by diffusion models using membership inference attacks. Participants will be able to develop and apply strategies in both blackbox and whitebox settings to determine if specific data points were included in training during the synthesis of single or multi-relational tables.\nOrganizers: Masoumeh Shafieinejad, Xi He, John Jewell, Mahshid Alinoori, Sana Ayromlou, Wei Pang, Gauri Sharma, Veronica Chatrath, and Deval Pandya.\n🏁 Robust Android Malware Detection Competition Website: https://ramd-competition.github.io\nThe Robust Android Malware Detection Competition aims to evaluate machine learning-based detectors with respect to (i) temporal data drift due to the evolution of both malware and legitimate applications and (ii) adversarial manipulations of malware samples to evade detection. The competition consists of three separate tracks (i.e., Adversarial Robustness to Feature-space Attacks, Adversarial Robustness to Problem-space Attacks, and Temporal Robustness to Data Drift).\nOrganizers: Angelo Sotgiu, Maura Pintor, Ambra Demontis, and Battista Biggio.\n","permalink":"https://satml.org/2025/competitions/","tags":null,"title":"SaTML Competitions"},{"categories":null,"contents":"","permalink":"https://satml.org/2025/past/","tags":null,"title":"Past Editions"},{"categories":null,"contents":"Competition track The SaTML 2025 conference will feature a competition track. We invite proposals for competitions on topics that advance trustworthy and secure machine learning. The results of these competitions will be presented and discussed by participants and organizers at the conference. Further details on the organization will be released in the following months.\nWe seek proposals with a clear scientific question in areas where machine learning can advance scientific, technological, or business domains. We especially encourage submissions that demonstrate a positive societal impact, particularly those using AI to support disadvantaged communities.\nOur focus is on data science competitions where participants will compete to achieve the highest score on a relevant machine learning task. These tasks will be based on well-defined problems and corresponding data sets provided by the competition organizers.\nProposal submission Proposals for competitions at SaTML must be submitted via email to pcchairs@satml.org. They should be concise, with a maximum length of three pages, to ensure a swift and efficient review process.\nReviewing and selection process Proposals will be evaluated based on factors such as:\nTask(s): Impact, originality, and relevance to the SaTML community will all be considered. Tasks that will foster positive societal impact are highly encouraged, although other topics relevant to the SaTML community are also welcomed.\nEvaluation Protocol: Feasibility of the task chosen, sufficient data for training and testing algorithms to solve the proposed task, soundness of the evaluation criteria, and clarity and fairness of the competition rules will all be evaluated.\nLogistics: The competition schedule, plan for attracting competition participants, and experience and diversity of the organizers will all be considered. The specific plan for attracting competition participants, including groups under-represented at SaTML, will be important during the review process.\nImportant dates The competition track includes the following dates:\nCompetition proposal deadline: August 7, 2024 Acceptance notification: August 16, 2024 Competition track (during the conference): April 9-11, 2025 Competition organizers should propose a timeline for running the competition to ensure participants have enough time to contribute high-quality entries. It is recommended that competitions be completed by early March 2025 (one month prior to the SaTML conference) at the absolute latest.\nContact and help Competition organizers who require help or suggestions regarding competition platforms for running the competition can contact pcchairs@satml.org for advice. This call for competitions is adapted from the NeurIPS competition track.\n","permalink":"https://satml.org/2025/participate-cfc/","tags":null,"title":"Call For Competitions"},{"categories":null,"contents":"Areas of interest IEEE SaTML expands upon the theoretical and practical understandings of vulnerabilities inherent to machine learning (ML), explores the robustness of learning algorithms and systems, and aids in developing a unified, coherent scientific community aiming to establish trustworthy machine learning. Topics of interest include (but are not limited to):\nNovel attacks on machine learning Novel defenses for machine learning Secure and safe machine learning in practice Verification of algorithms and systems Machine learning system security Privacy in machine learning Forensic analysis of machine learning Fairness and interpretability Trustworthy data curation Important date Paper submission deadline: Friday, September 27, 2024 (extended) Early reject notification: Thursday, October 31, 2024 Interactive discussion \u0026amp; revision phase: November 21 \u0026ndash; December 5, 2024 Decision notification: Thursday, December 12, 2024 Conference dates: April 9\u0026ndash;11, 2025 All deadlines are set to 11:59 PM AoE (Anywhere on Earth), which corresponds to UTC-12 time zone.\nSubmission categories We solicit research papers, systematization of knowledge papers, and position papers:\nResearch Papers: These papers should present new work, evidence, or ideas related to secure and trustworthy machine learning. Submission must be up to 12 pages of body text, with unlimited additional space for references and appendices. Research papers must be well-argued and worthy of publication and​ ​citation,​ ​on​ ​one of the​ ​topics listed​ ​above.​\nSystematization of Knowledge (SoK) Papers: These papers should either consolidate and clarify ideas in a major research area within secure and trustworthy machine learning or provide compelling evidence to support or challenge long-held beliefs in such areas. Submissions must be up to 12 pages of body text. SoK papers must include \u0026ldquo;SoK:\u0026rdquo; at the beginning of their title.\nPosition Papers: These papers should cover broader issues and visions related to secure and trustworthy machine learning, including open challenges, technical perspectives, educational aspects, societal impact, or notable research results. Submissions must be very well-argued and consist of 5 to 12 pages of body text. Position papers must include \u0026ldquo;Position:\u0026rdquo; at the beginning of their title.\nSubmission information All submissions must be received by 11:59 PM AoE (UTC-12) on the day of the deadline. The submission site is available here:\nSubmission site: https://hotcrp.satml.org Submitted papers must not substantially overlap with papers that have been published or accepted for publication, or that are simultaneously in submission to a journal, conference, or workshop with published proceedings. However, authors may choose to give talks about their work, post a preprint of the paper online, and disclose security vulnerabilities to vendors.\nDouble-blind review: SaTML follows a double-blind reviewing process. Submitted papers must (a) omit any reference to the authors\u0026rsquo; names or their institutions, and (b) cite the authors\u0026rsquo; own related work in the third person. It is important, however, to ensure that efforts to maintain anonymity do not compromise the quality of the submission or complicate the review process. Essential background references, for example, should not be omitted or anonymized. Please see this double-blind FAQ for the answers to many common concerns about double-blind reviewing.\nPrevious reviews: For papers that were previously submitted to, and rejected from, another conference, authors are required to append to their submission PDF the (anonymized, but otherwise unedited) prior reviews along with a description of how those reviews were addressed in the current version of the paper. Authors are only required to include reviews from the last time the paper was submitted. Authors who try to circumvent this rule (e.g., by changing the title of the paper without significantly changing the contents) may have their papers rejected without further consideration, at the discretion of the PC chairs.\nLarge-language models: We encourage authors to use any suitable tools, including Large Language Models (LLMs), for preparing high-quality papers and research. However, authors must adhere to two key criteria: First, the methodology must be thoroughly described, and if LLMs are integral to this methodology, their use should be explicitly detailed. Second, authors are responsible for the entire content of their paper, including all text and figures. While any tool may be used for writing, it is crucial that all content is accurate and original, ensuring transparency and maintaining the integrity of the research process (Adapted from the NeurIPS 2024 Call for Papers).\nSubmission format: Submissions must be a PDF file in two-column IEEE proceedings style. That is, authors must use \\documentclass[conference]{IEEEtran} when preparing their paper. The number of allowed pages for a submission depends on the submission category, see above.\nAuthors need to closely follow the rules provided in this document and precisely adhere to the format guidelines. Failure to comply with these rules is grounds for rejection.\nReviewing process All submissions to the conference will be evaluated based on their merits, particularly their relevance to the conference’s areas of interest, novelty, quality of execution, and presentation.\nTo decrease the load of reviewing on PC members, SaTML implements a two-round reviewing process. Each paper is initially assigned two reviews. If the PC chairs conclude that there is no path for acceptance at SaTML upon considering these initial reviews, the paper is early-rejected. This means that the paper is not assigned additional reviews and the authors are notified that their paper will not be included in the conference.\nAuthor discussion phase SaTML will have a discussion period during which authors can exchange messages with reviewers, respond to their questions, and address their comments through direct changes to the paper. To facilitate this, we will use an anonymous communication feature to enable interaction between authors and reviewers. Authors should primarily focus on correcting factual errors in the reviews and answering specific questions posed by the reviewers. New research results may also be discussed if they help clarify open questions. More instructions will be sent to the authors at the beginning of the discussion phase.\nSubmission decisions For each submission, one of the following decisions will be made:\nAccept: Papers in this category will be accepted for publication in the proceedings and presentation at the conference, possibly after making minor changes with the oversight of a shepherd (Minor Revision). Within one month of acceptance, all accepted papers must submit a camera-ready copy incorporating reviewer feedback. The papers will immediately be published, open access, in the Computer Society’s Digital Library, and they may be cited as \u0026ldquo;To appear in the IEEE Conference on Secure and Trustworthy Machine Learning, 2025\u0026rdquo;.\nMajor Revision: A limited number of papers will be invited to submit a major revision; such papers will receive a detailed summary of expectations for revision, in addition to standard reviewer comments. Authors will have a limited time window to submit a revision after the notification is sent. The authors should clearly explain in a well-marked appendix how the revisions address the comments of the reviewers. The revised paper will then be re-evaluated, and either accepted or rejected. We will assign the same set of reviewers. Authors can choose to withdraw their paper and not submit a revision.\nReject: Papers in this category are declined for inclusion in the conference.\nBest paper award Outstanding paper(s) will be selected by the Program Committee, with input from the Steering Committee, for the best paper award. The award will be announced at the conference. Best paper awards are intended to highlight papers which significantly challenge the state of the art in research areas relevant to SaTML.\nAttendance for accepted papers At least one author of accepted papers must present their work at the conference, and their papers will appear in the conference’s formal IEEE proceedings. In the event of difficulty obtaining visas for travel or other exceptional circumstances, exceptions may be made and will be discussed on a case-by-case basis.\nIf you have any questions, please email us at pcchairs@satml.org\n","permalink":"https://satml.org/2025/participate-cfp/","tags":null,"title":"Call For Papers"},{"categories":null,"contents":"\nKeynote 1: Malice, Models and Middlemen Michael Veale, University College London Abstract In recent years, machine learning systems have demonstrated impressive outputs, yet their robustness and overall performance remain complex issues. Criminal enterprises, however, are less concerned with certain types of errors, as they can externalize the cost of these errors onto their victims. This dynamic is evident in areas like spam and fraud, where automated systems have long been prevalent. As criminal enterprises begin deploying machine learning systems for malicious purposes, it becomes increasingly challenging to directly identify and stop these actors. Disrupting their activities requires targeting the intermediaries that connect criminals to their victims, including communication providers, cloud compute services, operating systems providing local compute like smartphone providers, model marketplaces like Hugging Face and GitHub, and content platforms like social media services. The safety and trustworthiness of AI will likely depend on the cooperation and governance mechanisms established by these intermediaries.\nLegal regimes and technical governance methods will play a significant role in this landscape. This involves monitoring cloud computing, controlling operations on operating systems, removing controversial dual-use models from online repositories, and requiring platforms to manage content decisions at scale. Each of these measures presents challenges, and achieving the right balance is complex. In this keynote, I will explore the role of intermediaries, examine emerging governance practices, and highlight the core tensions, difficulties, and opportunities in this evolving space.\nSpeaker Bio Michael Veale is Associate Professor in Digital Rights and Regulation and Vice-Dean (Education Innovation) at the Faculty of Laws, University College London (UCL), and Fellow at the Institute for Information Law, University of Amsterdam. His research focusses on how to understand and address challenges of power and justice that digital technologies and their users create and exacerbate, in areas such as privacy-enhancing technologies and machine learning. Veale has advised a wide variety of actors across the world on these issues, and his work is cited by hundreds of public policy players including courts, regulators, governments, legislatures, civil society and business, as well as thousands of academics. He sits on the advisory councils for the Open Rights Group and Foxglove, the Panel of Expert of the Digital Freedom Fund, and the Technology Advisory Panel for the Information Commissioner’s Office. He holds a PhD in the governance of machine learning from the Faculty of Engineering, UCL, as well as degrees from LSE and U Maastricht.\nKeynote 2: The Science of Empirical Privacy Measurement: Memorization and Beyond Kamalika Chaudhuri, University of California San Diego Abstract Since Fredriksson et al (2014), a body of work has emerged on the empirical measurement of privacy leakage, both from machine learning models and in other settings. In this talk, I will describe some recent advancements on this topic. First, we will look at memorization in vision encoder models, and propose a principled measurement of \u0026ldquo;deja vu memorization\u0026rdquo;; we will show how to scale it to off-the-shelf vision encoder models. Next, we will go beyond memorization, and talk about how privacy and memorization may be decoupled in more complicated settings.\nSpeaker Bio Kamalika Chaudhuri is a Director, Research Scientist at FAIR in Meta and an adjunct professor at University of California, San Diego. She was formerly a full professor at University of California, San Diego. She received a Bachelor of Technology degree in Computer Science and Engineering in 2002 from Indian Institute of Technology, Kanpur, and a PhD in Computer Science from University of California at Berkeley in 2007. She received an NSF CAREER Award in 2013 and a Hellman Faculty Fellowship in 2012. She has served as the program co-chair for AISTATS 2019 and ICML 2019, and as the General Chair for ICML 2022.\nKeynote 3: Artificial Intelligence: Should you trust it? Matt Turek, Defense Advanced Research Agency Abstract We have seen significant progress in Artificial Intelligence (AI) over the last ten years, predominantly driven by dramatic advances in machine learning and particularly deep learning. Society is realizing the benefits across a wide range of application domains. However, within the military, the consequence of making a wrong decision based on AI could be catastrophic. And the United States Department of Defense must defend against nation-state level adversaries with significant resources, the ability to create deception, and the desire to change our way of life. The US’s Defense Advanced Research Projects Agency (DARPA) is funding research in trustworthy AI technologies and systems that can be trusted to perform as expected despite the efforts of sophisticated adversaries. In this presentation, I will discuss research efforts in AI that we can trust with our (and warfighters’) lives and explore DARPA-funded advances that appear promising toward reaching the goal of trustworthy AI.\nSpeaker Bio Matt Turek is the deputy office director for the Defense Advanced Research Agency’s (DARPA) Information Innovation Office (I2O), where he provides technical leadership and works with program managers to envision, create, and transition capabilities that ensure enduring information advantage for the United States and its allies. Previously, Turek served as I2O’s acting deputy director and as a program manager for AI-related programs, including Explainable AI, Machine Common Sense, Media Forensics, and Semantic Forensics. He joined DARPA from Kitware, Inc., where he led a team developing computer vision technologies. Prior to that role, he was at GE Global Research conducting research in medical imaging and industrial inspection.\n","permalink":"https://satml.org/2025/keynotes/","tags":null,"title":"Keynotes"},{"categories":null,"contents":" General chair Amartya Sanyal University of Copenhagen Program chairs Konrad Rieck BIFOLD \u0026amp; TU Berlin Somesh Jha University of Wisconsin Treasurer Kathrin Grosse IBM Social media chair Gautam Kamath University of Waterloo \u0026amp; Vector Institute Donations chair Lea Schönherr CISPA Helmholtz Center for Information Security Web chairs Thorsten Eisenhofer BIFOLD \u0026amp; TU Berlin Stefan Czybik BIFOLD \u0026amp; TU Berlin Steering committee Nicolas Papernot University of Toronto \u0026amp; Vector Institute Carmela Troncoso EPFL Been Kim Google Brain Ben Zhao University of Chicago Bo Li University of Illinois at Urbana–Champaign Zico Kolter Carnegie Mellon University Program committee Syed Ishtiaque Ahmed University of Toronto Giovanni Apruzzese University of Liechtenstein Daniel Arp TU Wien Hilal Asi Apple Laura Ball Alan Turing Institute Solon Barocas Microsoft Research \u0026amp; Cornell University Aurélien Bellet INRIA Arjun Bhagoji University of Chicago Battista Biggio University of Cagliari Sayan Biswas EPFL Franziska Boenisch CISPA Helmholtz Center for Information Security Kirill Bykov ATB Potsdam, TU Berlin \u0026amp; BIFOLD Yinzhi Cao Johns Hopkins University Karan Chadha Meta Varun Chandrasekaran University of Illinois Urbana-Champaign Kai Chen Institute of Information Engineering, Chinese Academy of Sciences Sizhe Chen University of California, Berkeley Antonio Emanuele Cinà University of Genova Aloni Cohen University of Chicago Tianshuo Cong Tsinghua University A. Feder Cooper Microsoft Research \u0026amp; Stanford Scott Coull Google Ana-Maria Cretu EPFL Francesco Croce EPFL Andrew Cullen University of Melbourne Rachel Cummings Columbia University Wenxin Ding University of Chicago Adam Dziedzic CISPA Helmholtz Center for Information Security Thorsten Eisenhofer BIFOLD \u0026amp; TU Berlin Alessandro Erba Karlsruhe Institute of Technology (KIT) David Evans University of Virginia Georgina Evans Google DeepMind Kevin Eykholt IBM Research Minghong Fang Duke University Joel Frank Meta Sébastien Gambs Université du Québec à Montréal Neil Gong Duke University Parikshit Gopalan Apple Nirupam Gupta University of Copenhagen Xingshuo Han Nanyang Technological University Jamie Hayes Deepmind Xinlei He Hong Kong University of Science and Technology Matthias Hein University of Tuebingen Antti Honkela University of Helsinki Shengyuan Hu Carnegie Mellon University Mathias Humbert University of Lausanne Jacob Imola University of Copenhagen Suman Jana Columbia University Bargav Jayaraman Meta Shouling Ji Zhejiang University Jinyuan Jia Penn State Yigitcan Kaya University of California, Santa Barbara Raouf Kerkouche CISPA Helmholtz Center for Information Security Mahdi Khalili Ohio State University Klim Kireev EPFL Boris Koepf Azure Research Pavel Laskov University of Liechtenstein Mathias Lecuyer University of British Columbia Tian Li University of Chicago Zheng Li CISPA Helmholtz Center for Information Security Jian Liu University of Tennessee, Knoxville Ruixuan Liu Emory University Sijia Liu Michigan State University Yugeng Liu CISPA Helmholtz Center for Information Security Keane Lucas Carnegie Mellon University Shiqing Ma University of Massachusetts Amherst Keith Manville MITRE Audra McMillan Apple Jaron Mink Arizona State University Esfandiar Mohammadi Universität zu Lübeck Meisam Mohammady Iowa State University Tamalika Mukherjee Columbia University Ozan Özdenizci Montanuniversität Leoben Iyiola Emmanuel Olatunji CISPA Helmholtz Center for Information Security Rasmus Pagh University of Copenhagen Dario Pasquini George Mason University Kexin Pei University of Chicago Feargus Pendlebury Meta Stjepan Picek Radboud University Fabio Pierazzi King\u0026rsquo;s College London Rafael Pinot Sorbonne University Francesco Pinto University of Oxford Maura Pintor University of Cagliari Apostolos Pyrgelis RISE Research Institutes of Sweden Sazzadur Rahaman University of Arizona Nidhi Rastogi Rochester Institute of Technology Ambrish Rawat IBM Research Saeyoung Rho Columbia University Vera Rimmer DistriNet, KU Leuven Mikel Rodriguez Google DeepMind Alexander Robey University of Pennsylvania Amrita Roy Chowdhury University of Michigan, Ann Arbor Lea Schönherr CISPA Helmholtz Center for Information Security Vikash Sehwag Sony AI Avital Shafran Hebrew University of Jerusalem Mahmood Sharif Tel Aviv University Saeed Sharifi-Malvajerdi TTIC Ryan Sheatsley University of Wisconsin-Madison Xinyue Shen CISPA Helmholtz Center for Information Security Ilia Shumailov Google DeepMind Wai Man Si CISPA Helmholtz Center for Information Security Jessica Sorrell Johns Hopkins University Angelo Sotgiu University of Cagliari Ajith Suresh Technology Innovation Institute, Abu Dhabi Kunal Talwar Apple Guanhong Tao University of Utah Binghui Wang Illinois Institute of Technology Derui Wang CSIRO\u0026rsquo;s Data61 Ren Wang Illinois Institute of Technology Tianhao Wang University of Virginia Ting Wang Stony Brook University Xiao Wang Northwestern University Alexander Warnecke Databricks Christian Weinert Royal Holloway, University of London Emily Wenger Duke University Eric Wong University of Pennsylvania Christian Wressnegger Karlsruhe Institute of Technology (KIT) Hao Wu University of Copenhagen Chong Xiang Princeton University Shengmin Xu Fujian Normal University Jason Xue CSIRO’s Data61 Chhavi Yadav University of California San Diego Hossein Yalame Bosch GmbH Ziqi Yang Zhejiang University Jiayuan Ye National University of Singapore Chia-Mu Yu National Yang Ming Chiao Tung University Yaodong Yu University of Maryland, College Park Xiangyu Zhang Purdue University Xiao Zhang CISPA Helmholtz Center for Information Security Xuero Zhang Ohio State University Yang Zhang CISPA Helmholtz Center for Information Security ","permalink":"https://satml.org/2025/organization/","tags":null,"title":"Organization"},{"categories":null,"contents":"Venue IEEE SaTML 2025 will be held on the campus of the University of Copenhagen in Denmark from April 9 to 11, 2025. The conference venue for all three days will be the Lundbeckfond Auditorium.\nRegistration The registration for the conference is open and can be accessed via the registration website.\nAccommodation We have negotiated special room rates with hotels in Copenhagen. There may be more to come but we recommend booking them early as April can be a busy time for hotels leading to fast emptying vacancies.\nBudget Hotels Wakeup Copenhagen: Click here to book at a reduced room rate.\nCabinn Copenhagen: Click here to book a room rate and use the booking code BLKSATMLD.\nMid-Range Hotels Below is a list of mid-range hotels recommended for conference attendees:\nHotel Arthur Link\nHotel Ibsen Link\nHotel Christian IV Link\nScandic Copenhagen Link\nVisa Support Should you require a visa support letter to attend the conference, please remember to indicate this during the registration process. It can take a long time to get a visa for Denmark, so please consider registering and applying as soon as possible and at least 3 months before the conference begins. We will review and process visa support letters every two weeks, so please take into consideration this time when planning your visa application.\nTravel Scholarships The conference will be using the generous support we received from our sponsors to support students and postdoctoral researchers traveling to IEEE SaTML 2025 who do not have sufficient funding. For more information about the scholarship application, important deadlines, and scholarship criteria and conditions, please visit our travel scholarship page.\nCode of Conduct IEEE SaTML is dedicated to providing a harassment-free conference experience for everyone, regardless of gender identity or expression, sexual orientation, disability, physical appearance, body size, race, age, or religion. We do not tolerate harassment of participants in any form. Harassment includes, but is not limited to:\nSexual images in public spaces. Deliberate intimidation, stalking, or following. Harassing photography or recording. Sustained disruption of talks or other events. Inappropriate physical contact. Unwelcome sexual attention or offensive comments. Advocating for, or encouraging, any of the above behavior. Enforcement The event organizers may take action to redress anything designed to, or with the clear impact of, disrupting the event or making the environment hostile for any participants. If a participant engages in harassing behavior, event organizers retain the right to take any actions to keep the event a welcoming environment for all participants. This includes warning the offender or expulsion from the conference with no refund.\nParticipants asked to stop any harassing behavior are expected to comply immediately. We expect participants to follow these rules at all event spaces—i.e., virtual conference platform, hotel venue, and event-related social spaces. We think people should follow these rules outside event activities too!\nReporting If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible. Harassment and code of conduct violations reduce the value of our event for everyone. We want to provide a safe, inclusive environment at our event. And people who report unacceptable behavior help us cultivate a safe and inclusive environment.\nTo make a report: Email the general chair.\nWhen taking a personal report, we will ensure you are in a safe environment and cannot be overheard. We may involve other event staff to ensure your report is managed properly. Once in a safe environment, we will ask you to tell us about what happened. This can be upsetting, but we will handle it as respectfully as possible, and you can bring someone to support you. You won’t be asked to confront anyone and we won’t tell anyone who you are.\nIf you are being harassed by a member of the SaTML community outside event spaces, we still want to know about it. We will take all good-faith reports of harassment by community members seriously. This includes harassment outside event spaces and harassment that took place at any point in time. Consequences\nIf a participant engages in harassing behavior, they may be sanctioned or expelled from the event without a refund at the discretion of the organizers. Further action may be taken by IEEE in accordance with its policies covering code of conduct (9.25 Civility Policy, 9.26 Policy Against Discrimination and Harassment), whose consequences may include expulsion from membership in the IEEE; see the IEEE policies, IEEE Technical Activities procedures and the IEEE Member Code of Conduct for more information on these further IEEE actions.\nOpen Conference Statement Equity, Diversity, and Inclusion are central to the goals of the IEEE Computer Society and all of its conferences.\nEquity at its heart is about removing barriers, biases, and obstacles that impede equal access and opportunity to succeed. Diversity is fundamentally about valuing human differences and recognizing diverse talents. Inclusion is the active engagement of Diversity and Equity. A goal of the IEEE Computer Society is to foster an environment in which all individuals are entitled to participate in any IEEE Computer Society activity free of discrimination. For this reason, the IEEE Computer Society is firmly committed to team compositions in all sponsored activities, including but not limited to, technical committees, steering committees, conference organizations, standards committees, and ad hoc committees that display Equity, Diversity, and Inclusion.\nIEEE Computer Society meetings, conferences and workshops must provide a welcoming, open and safe environment, that embraces the value of every person, regardless of race, color, sex, sexual orientation, gender identity or expression, age, marital status, religion, national origin, ancestry, or disability. All individuals are entitled to participate in any IEEE Computer Society activity free of discrimination, including harassment based on any of the above factors.\nEvent Conduct and Safety Statement IEEE believes that science, technology, and engineering are fundamental human activities, for which openness, international collaboration, and the free flow of talent and ideas are essential. Its meetings, conferences, and other events seek to enable engaging, thought provoking conversations that support IEEE’s core mission of advancing technology for humanity. Accordingly, IEEE is committed to providing a safe, productive, and welcoming environment to all participants, including staff and vendors, at IEEE-related events.\nIEEE has no tolerance for discrimination, harassment, or bullying in any form at IEEE-related events. All participants have the right to pursue shared interests without harassment or discrimination in an environment that supports diversity and inclusion. Participants are expected to adhere to these principles and respect the rights of others.\nIEEE seeks to provide a secure environment at its events. Participants should report any behavior inconsistent with the principles outlined here, to on site staff, security or venue personnel, or to eventconduct@ieee.org\n","permalink":"https://satml.org/2025/attend/","tags":null,"title":"Register \u0026 Attend"},{"categories":null,"contents":"About the Scholarships The organizers of the IEEE Conference on Secure and Trustworthy Machine Learning 2025 have arranged for scholarships for students and postdoctoral researchers participating in the conference. The conference aims to offer as many scholarships as possible according to the generous sponsorships recieved from the sponsors.\nThe scholarship covers:\nRegistration fee for the conference. Travel expenses to and from Denmark (see guide for detailed costs). Accommodation during your stay (see guide for detailed costs). If your application is successful, you will receive a scholarship guide from us. Please note, that the scholarship does not cover local transportation costs or food expenses during your stay.\nYou must book and pay for all the above-mentioned services yourself. The scholarship works as a reimbursement of your expenses after participating in the conference. You can indicate whether you wish to apply for the scholarship during the registration process for the conference.\nThe scholarships will be granted based on the following criteria:\nBrief justification of your need for support to travel to SaTML in Copenhagen, Denmark If you are a graduate student currently doing research within a laboratory: a short paragraph from your advisor justifying the lack of funding available in your group to fund your travel (max 150 words). Scholarship Conditions The following requirements must be met for you to receive the scholarship:\nYou must participate in the conference. Failure to show up and participate will result in the cancellation of the scholarship. You must strictly comply with the deadlines in the scholarship process. If you do not comply with them, we reserve the right to cancel the scholarship. You must provide the required information at all stages of the process. If you cannot provide it, then we will assume that you forfeit your scholarship. Application To apply for the scholarship, you must register no later than 8th January 2025. The registration page is available here. The committee will consider all applications and give their final decisions on 24th January 2025.\nWhen you apply for the scholarship, remember to fill in all the fields in the application form or we may not be able to consider your application.\nIf Succesful If you have been selected for the scholarship, you will then receive an e-mail with a guide of instructions on how to proceed with the scholarship registration. This includes the following:\nYou must book your covered travel arrangement and hotel accommodation, and save the invoices for both and boarding pass (only if you fly). You must then fill out a pre-reimbursement form that collects all the relevant data. You will have to upload the invoices for your travel arrangements and accommodation when filling it our. Please read the attached guide to fill out this form, to make sure you have all relevant information and documentation at hand when you begin the process. The deadline for completing the scholarship registration is 31st January 2025.\nOnce you have completed this step, and received final confirmation of the registration, you will then receive the conference registration fee waiver, as part of the scholarship. The reimbursment of travel and accommodation costs will take place after the conference.\nAfter the conference When you return home after the conference, you will be required to hand in the the boarding passes from your flights to and from Denmark (if relevant) along with all the other necessary invoices. You can find precise information about this in the guide you will receive when you have been selected for the scholarship.\nOnce we have all the necessary information, we will process the reimbursements regarding your travel and accommodation costs related to the conference.\n","permalink":"https://satml.org/2025/scholarships/","tags":null,"title":"Travel Scholarships"}]