<div class="cd-schedule-modal__event-info fw-bold">
	<div> 

		<h1 style="font-weight: bold;"><a href="http://mlmac.io">Model
		Attribution Challenge:</a></h1> The task of model attribution for large
		language models (LLMs) is to identify which model was used as the basis
		for some fine-tuned model. Model attribution is especially important for
		identifying abuse, misuse, and misinformation. Especially for LLMs,
		fine-tuned models may derive from base models without obvious
		attribution characteristics. The ML Model Attribution Challenge (<a
		href="http://mlmac.io">https://mlmac.io</a>) was organized to
		incentivize contestants to develop creative techniques to discover the
		underlying (non-watermarked) base models from modest evidence and
		limited queries of the fine-tuned model. In this session, techniques
		from winning solutions of the ML Model Attribution Challenge will be
		presented.


		<br>
		<br>

		<h1 style="font-weight: bold;"><a
		href="https://github.com/google-research/lm-extraction-benchmark">Improving
		training data extraction attacks on large language models:</a></h1>
		organized by Nicholas Carlini, Christopher Choquette-Choo, Daphne
		Ippolito, Matthew Jagielski, Katherine Lee, Milad Nasr, Florian Tramer,
		Chiyuan Zhang.

		<br>
		<br>

		<h1 style="font-weight: bold;"><a
		href="https://github.com/microsoft/MICO">MICO: A Membership Inference
		Competition:</a></h1> organized by Giovanni Cherubin, Ana-Maria Cretu,
		Andrew Paverd,  Ahmed Salem, Santiago Zanella-Beguelin.

	</div>

	

</div>