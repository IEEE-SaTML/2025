<div class="cd-schedule-modal__event-info fw-bold">
	<div> 

		<h1 style="font-weight: bold;">Title:</h1> Aligning ML Systems with Human Intent
		<br>
		<br>

		<h1 style="font-weight: bold;">Abstract:</h1> ML systems are "aligned"
		if their behavior matches the intended goals of the system designer,
		many of which are implicit and informal. Alignment is difficult to
		achieve, due to misspecified reward functions (Goodhart's law),
		unexpected behavior that appears emergently at scale, and feedback loops
		arising from multi-agent interactions. For example, language models
		trained to predict tokens might give untruthful answers if truth and
		likelihood diverge, and recommender systems might optimize short-term
		engagement at the expense of long-term well-being. In this tutorial, I
		will discuss empirically observed alignment issues in large-scale
		systems, as well as several techniques for addressing them, based on (1)
		improving human feedback to reduce reward misspecification, (2)
		extracting latent knowledge from models' hidden states using
		unsupervised learning, and (3) large-scale evaluation to detect novel
		failures. I will also briefly touch on the challenges of emergence and
		multi-agent interactions and current approaches to addressing them.

		<br>
		<br>

		<h1 style="font-weight: bold;">Biography:</h1> Jacob is an Assistant
		Professor of Statistics at UC Berkeley since 2019, where he is also a
		member of the Berkeley Artificial Intelligence Lab and of the EECS
		department. He completed his PhD in machine learning at Stanford
		University working with Percy Liang. His work focuses on making
		machine learning reliable and aligned with human values, touching on
		topics such as robustness, interpretability, and value learning. He also
		collaborates with policy researchers and is a technical advisor for Open
		Philanthropy.

		<br>
		<br>
	
		<img src="/images/people/Jacob-Steinhardt.png" class="portfolio-image
		img-fluid" alt="">

	</div>

	

</div>